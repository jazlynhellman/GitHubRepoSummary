Repository,Description,ReadMe,Purpose,Languages,Lang Size by Bits
AgilTec/cadenero,Rails.API Authentication Engine for multitenant RESTful APIs,"![Cadenero Logo](https://raw.github.com/AgilTec/cadenero/master/cadenero.logo.png)
By [![Agiltec Logo](https://launchrock-assets.s3.amazonaws.com/logo-files/GpujzvLXPPqzAcz.png)](http://agiltec.github.io/).

[![Gem Version](https://fury-badge.herokuapp.com/rb/cadenero.png)](http://badge.fury.io/rb/cadenero)
[![Build Status](https://travis-ci.org/AgilTec/cadenero.png?branch=master)](https://travis-ci.org/AgilTec/cadenero)
[![Code Climate](https://codeclimate.com/github/AgilTec/cadenero.png)](https://codeclimate.com/github/AgilTec/cadenero)
[![Coverage Status](https://coveralls.io/repos/AgilTec/cadenero/badge.png?branch=master)](https://coveralls.io/r/AgilTec/cadenero?branch=master)
[![Dependency Status](https://gemnasium.com/AgilTec/cadenero.png)](https://gemnasium.com/AgilTec/cadenero)

THIS README IS FOR THE MASTER BRANCH OF **CADENERO** AND REFLECTS THE WORK CURRENTLY EXISTING ON THE MASTER BRANCH. IF YOU ARE WISHING TO USE A NON-MASTER BRANCH OF **CADENERO**, PLEASE CONSULT THAT BRANCH'S README AND NOT THIS ONE.

Authentication Engine for Rails.API multitenant RESTful APIs based on Warden. It:
* Is Racked based
* Use token authentication as strategy for the API
* Is RESTful API
* Allows you to have multiple roles (or models/scopes) signed in at the same time

# Information

## About Cadenero

### Why Cadenero?
**""Cadenero""** is the spanish word for [""Bouncer (doorman)""](http://en.wikipedia.org/wiki/Bouncer_(doorman\)). The main function of **Cadenero** is to be a resource for authenticating consumers of the services that the API provides. As the real bouncers, **Cadenero** aims to provide security, check authorized access, to refuse entry for intoxication, aggressive behavior or non-compliance with statutory or establishment rules.

You can use [Warden](https://github.com/hassox/warden) or [Devise](https://github.com/plataformatec/devise) directly but for API apps the rewritting and monkey patching can be messy.

### Installing **Cadenero**

#### Preconditions

##### PostgreSQL
You should have a PostgreSQL server (for downloading see: http://www.postgresql.org/download/). If you are using OSX, you can install using [Homebrew](http://mxcl.github.io/homebrew/) for that you can follow the following this [instructions](http://www.moncefbelyamani.com/how-to-install-postgresql-on-a-mac-with-homebrew-and-lunchy/)

##### Ruby 1.9.x or 2.x
For that we recommend that you use [rbenv](https://github.com/sstephenson/rbenv) with [ruby-build](https://github.com/sstephenson/ruby-build) or [rvm](https://rvm.io/)

We use the standard `rake`, `bundler` and `gem`

##### Git/Github
You are here. Then you know what to do ;-)

#### Setup

Rails 3.2.13 is the master version used now by **Cadenero**, if you want to use Rails 4 goodness please use the branch ""rails4""

Generate first your Rails app as usual using:

```
    $ rails _3.2.13_ new your_app --skip-test-unit  -d postgresql
```

In the `Gemfile` add the following lines:
```ruby
    gem 'cadenero', '~> 0.0.2.b10'
    gem ""strong_parameters"", ""~> 0.2.1""

    group :development, :test do
      gem 'rspec-rails', '~> 2.14.0'
      gem 'capybara', '~> 2.1.0'
      gem 'rack-test', '~> 0.6.2'
    end
```

In the `config/database.yml` replace the `sqlite3` adapter for `postgresql` as follow:

```YAML
    development:
      adapter: postgresql
      encoding: unicode
      database: your_app_development 
      pool: 5
      min_messages: warning

    test:
      adapter: postgresql
      encoding: unicode
      database: your_app_test
      pool: 5
      min_messages: warning
```

Then run bundle, create the databases and run the generator:

```Shell
    $ bundle install; rake db:create; rails-api g cadenero:install
```

Finally run the server:

```Shell
    $ rails-api s
```

Or much better for checking the multitenancy you can use [Pow](http://pow.cx/). To install or upgrade Pow, open a terminal and run this command:

```Shell
    $ curl get.pow.cx | sh (View Source)
```

To set up a Rack app, just symlink it into ~/.pow:

```Shell
    $ cd ~/.pow
    $ ln -s /path/to/myapp
```

Check that you can access the API using the default account `www` and user `testy@example.com` with password `changemeËœ or those defined for you when the generator was run. Ror the client you can use [cURL](http://curl.haxx.se/) or [RESTClient](http://restclient.net/)

You can create a new account as follows:

```Shell
    $ curl -v -X POST http://www.cadenero.dev/v1/accounts -H 'Content-Type: application/json' -d '{""account"": { ""name"": ""Testy"", ""subdomain"": ""tested1"", ""owner_attributes"": {""email"": ""testy2@example.com"", ""password"": ""changeme"", ""password_confirmation"": ""changeme""}}}'
```
Or

```
    Request

    POST http://www.cadenero.dev/v1/accounts

        Content-Type: application/json

    Body
    {""account"": { ""name"": ""Testy"", ""subdomain"": ""test2"", ""owner_attributes"": {""email"": ""testy2@example.com"", ""password"": ""changeme"", ""password_confirmation"": ""changeme""}}}
```

Have fun!

### Access Points
**Cadenero** creates the following versioned routes for exposing the authentication RESTful API

```
      v1_root        /v1(.:format)           cadenero/v1/account/dashboard#index {:default=>:json}
  v1_sessions POST   /v1/sessions(.:format)  cadenero/v1/account/sessions#create {:default=>:json}
              DELETE /v1/sessions(.:format)  cadenero/v1/account/sessions#delete {:default=>:json}
     v1_users POST   /v1/users(.:format)     cadenero/v1/account/users#create {:default=>:json}
              GET    /v1/users(.:format)     cadenero/v1/account/users#index {:default=>:json}
      v1_user GET    /v1/users/:id(.:format) cadenero/v1/account/users#show {:default=>:json}
  v1_accounts POST   /v1/accounts(.:format)  cadenero/v1/accounts#create {:default=>:json}
         root        /                       cadenero/v1/account/dashboard#index {:default=>:json}
```

You can check them running:

```Shell
    $ rake routes
```
### Strategies
For authentication **Cadenero** has two default Warden Strategies:
  * **Password**. That expect that the client to keep a session cookie and using for authentication the user `email` and `password`.
  * **Token Authentication**. That is stateless and expects that for each request the user include the `auth_token` as a key-value of the request params.

In any case when you signed up **Cadenero** creates an auth_token for the membership to the account that you signed up.

If you want to know more about Warden Strategies see: https://github.com/hassox/warden/wiki/Strategies

### Documentation
You can review the YARD docs in: http://rubydoc.info/github/AgilTec/cadenero/frames

### The Cadenero Task List
- [x] Specs for the code 100% Coverage using BDD with [Rspec](https://github.com/rspec/rspec) and [Capybara](https://github.com/jnicklas/capybara)
- [x] Documentation for all the code
- [ ] Examples of use and demo

### Versions
**Cadenero** aims to adhere to [Semantic Versioning 2.0.0](http://semver.org/) the current version is: 0.0.2-b10 meaning MAJOR.MINOR.PATCH format. Violations of this scheme should be reported as bugs. Specifically, if a minor or patch version is released that breaks backward compatibility, that version should be immediately yanked and/or a new version should be immediately released that restores compatibility. Breaking changes to the public API will only be introduced with new major versions. As a result of this policy, you can (and should) specify a dependency on this gem using the [Pessimistic Version Constraint](http://docs.rubygems.org/read/chapter/16#page74) with two digits of precision. For example:

```
    spec.add_dependency 'cadenero', '~> 1.0'
```

### Bug reports

If you discover a problem with **Cadenero**, we would like to know about it. However, we ask that you please review these guidelines before submitting a bug report:

https://github.com/AgilTec/cadenero/wiki/Bug-reports

To submit the bug or issue go to: https://github.com/AgilTec/cadenero/issues

If you found a security bug, do *NOT* use the GitHub issue tracker. Send an email to the maintainers listed at the bottom of the README please.

### Contributing

We hope that you will consider contributing to **Cadenero**. You're encouraged to submit pull requests, propose features and discuss issues.

  * Fork the project
  * Write test for your new feature or a test that reproduces a bug
  * Implement your feature or make a bug fix
  * Commit, push and make a pull request. Bonus points for topic branches.

You will usually want to write tests for your changes using BDD tools as RSpec, Rack::Test and Capybara.

To run the test suite, go into **Cadenero**'s top-level directory and run `bundle install` and `rspec spec`.  For the tests to pass, you will need to have a Postgresql server running on your system.

If you have not contribute before in a Github repo please review first:

  * [Fork A Repo](https://help.github.com/articles/fork-a-repo)
  * [Using Pull Requests](https://help.github.com/articles/using-pull-requests)

#### Running the Specs
**Cadenero** use [RSpec](https://github.com/rspec/rspec) and [Capybara](https://github.com/jnicklas/capybara). To run the specs you only need to do:

```Shell
    $ RAILS_ENV=test bundle exec rake db:create
    $ RAILS_ENV=test bundle exec rake db:migrate
    $ bundle exec rspec spec
```

You can `binstub` the command bins to avoid writing `bundle exec`. You only need to write:

```Shell
    $ bundle binstubs rspec-core
    $ bundle binstubs rake
```

## About Dependencies and Inspirations

### Warden

**Cadenero** is based on [Warden](https://github.com/hassox/warden), which is a general Rack authentication framework created by Daniel Neighman. We encourage you to read more about Warden here: https://github.com/hassox/warden/wiki

#### Devise
Some code and architectural decisions in **Cadenero** have been inspired for the excellent gem [Devise](https://github.com/plataformatec/devise).

### Rails::API

**Cadenero** is a Rails::API Engine, Rails::API is a subset of a normal Rails application, created for applications that don't require all functionality that a complete Rails application provides. It is a bit more lightweight, and consequently a bit faster than a normal Rails application. The main example for its usage is in API applications only, where you usually don't need the entire Rails middleware stack nor template generation. Rails::API was created by Santiago Pastorino. We encourage you to read more about Rails::API here: https://github.com/rails-api/rails-api

### Multitenancy
**Cadenero** use [Apartment](https://github.com/influitive/apartment) for Database multi-tenancy for Rack. **Cadenero** creates a new PostgreSQL Schema (like a NameSpace) for each account with subdomain, this means that each account has access only to its own information in that Schema. If you want to persist models that will have information that should be namespaced by the account Schemas rather than use the usual `rake db:migrate` for creating the tables you should use `rake apartment:migrate`. **Cadenero** creates for you the required `config.database_names` required for Apartment. We encourage you to review the [Apartment README](https://github.com/influitive/apartment/blob/development/README.md) to have more details

#### Multitenancy with Rails And subscriptions too!
Parts of the code of **Cadenero** have been based on the excellent work of [Ryan Bigg](https://github.com/radar) in his book [""Multitenancy with Rails And subscriptions too!""](https://leanpub.com/multi-tenancy-rails) but modified to be use in a RESTful API

### Maintainers

* [Manuel Vidaurre](https://github.com/mvidaurre)

## License

MIT License. Copyright 2013 AgilTec. http://agiltec.com.mx

You are not granted rights or licenses to the trademarks of the AgilTec, including without limitation the **Cadenero** name or logo.


This project rocks and uses MIT-LICENSE.",Authentication engine for consumers of multitenant RESTful APIs,"['Ruby', 'JavaScript']","[76700, 13073]"
brenoc/opentracks,A Flask/Python client for Open-Transactions.,"
![](http://i.imgur.com/p8bnuiZ.png) Open Tracks ![project unmaintained](https://img.shields.io/badge/project-unmaintained-red.svg)
==========

Open Tracks is an [Open-Transactions](http://opentransactions.org/) client built with Python.

## Deprecated
This project is deprecated.

I don't know any alternatives to suggest. Please, feel free to open an issue if you do find out.

![Open Tracks](https://i.imgur.com/dmVkN64.png)

Check out some [screenshots](https://imgur.com/a/BYQ24)

Interested? Please take 10 minutes to read our [Wiki](https://github.com/brenoc/opentracks/wiki).
",open-source Open-Transactions project,"['Python', 'HTML', 'JavaScript', 'CSS']","[301396, 103391, 23153, 1292]"
onaio/onadata,"Collect, Analyze and Share","Ona Platform
=================

Collect, Analyze and Share Data!

.. image:: https://travis-ci.org/onaio/onadata.svg?branch=master
  :target: https://travis-ci.org/onaio/onadata

About
-----

Ona is derived from the excellent `formhub `_ platform developed by the Sustainable Engineering Lab at Columbia University.

Installation
------------

See the `installation documentation `_.

Docker
------

Install `Docker `_ and `Docker Compose `_.

.. code-block:: sh

    docker-compose up

    # create super user
    # -----------------
    docker exec -it onadata_web_1 bash

    # activate virtual envirenment
    source /srv/.virtualenv/bin/activate

    python manage.py createsuperuser

It should be accessible via http://localhost:8000. The settings are in
`onadata/settings/docker.py `_.

On registration check the console for the activation links, the default email
backend is ``django.core.mail.backends.console.EmailBackend``. See
`Django Docs `_ for details.

Contributing
------------

If you would like to contribute code please read
`Contributing Code to Ona Data `_.

Edit top level requirements in the file `requirements/base.in `_. Use
 `pip-compile `_ to update `requirements/base.pip `_.
 You will need to update `requirements.pip` and set `lxml==3.6.0`, for some unknown reason `pip-compile` seems to
 pick a lower version of lxml when `openpyxl` requires `lxml>=3.3.4`.

.. code-block:: sh

    pip-compile --output-file requirements/base.pip requirements/base.in

Copy `pre-commit.sh `_ into `.git/hooks/pre-commit`, it ensures staged python flake8 are in acceptable code style and conventions.

.. code-block:: sh

    cp pre-commit.sh .git/hooks/pre-commit
    chmod +x .git/hooks/pre-commit

**Security Acknowledgments**

We would like to thank the following security researchers for responsibly disclosing security issues:

============= ================  ==========  ==============
 Name          Date              Severity    Contribution
============= ================  ==========  ==============
Danish Tariq   1st April 2018     Medium     `Users able to create projects in other user accounts `_
============= ================  ==========  ==============

Code Structure
--------------

* **api** - This app provides the API functionality mostly made up of viewsets

* **logger** - This app serves XForms to and receives submissions from
  ODK Collect and Enketo.

* **viewer** - This app provides a csv and xls export of the data stored in
  logger. This app uses a data dictionary as produced by pyxform. It also
  provides a map and single survey view.

* **main** - This app is the glue that brings logger and viewer
  together.

Localization
------------

To generate a locale from scratch (ex. Spanish)

.. code-block:: sh

    django-admin.py makemessages -l es -e py,html,email,txt ;
    for app in {main,viewer} ; do cd onadata/apps/${app} && django-admin.py makemessages -d djangojs -l es && cd - ; done

To update PO files

.. code-block:: sh

    django-admin.py makemessages -a ;
    for app in {main,viewer} ; do cd onadata/apps/${app} && django-admin.py makemessages -d djangojs -a && cd - ; done

To compile MO files and update live translations

.. code-block:: sh

    django-admin.py compilemessages ;
    for app in {main,viewer} ; do cd onadata/apps/${app} && django-admin.py compilemessages && cd - ; done

Api Documentation
-----------------

Generate the API documentation and serve via Django using:

.. code-block:: sh

    cd docs
    make html
    python manage.py collectstatic

Generate sphinx docs for new code using
`autodoc `_.

Run sphinx in autobuild mode using:

.. code-block:: sh

    sphinx-autobuild docs docs/_build/html

Requires sphinx-autobuild, install with ``pip install sphinx-autobuild``.


Django Debug Toolbar
--------------------

* `$ pip install django-debug-toolbar`
* Use/see `onadata/settings/debug_toolbar_settings/py`
* Access api endpoint on the browser and use `.debug` as the format extension e.g `/api/v1/projects.debug`

Upgrading existing installation to django 1.9+
----------------------------------------------

**Requirements**

* Postgres 9.4 or higher
* xcode-select version 2343 or higher

**Upgrading from a pervious Ona setup**
Ensure you upgrade all your pip requirements using the following command:

.. code-block:: sh

    pip install -r requirements/base.pip

Fake initial migration of `guardian`, `django_digest`, `registration`. Migrate `contenttypes` app first.

.. code-block:: sh

    python manage.py migrate contenttypes
    python manage.py migrate --fake-initial django_digest
    python manage.py migrate --fake-initial guardian
    python manage.py migrate --fake-initial registration
    python manage.py migrate


**Major django changes affecting Ona**
* The DATABASES settings key depricates the use of the *autocommit* setting in the *OPTIONS* dictionary.
",data collection/analyzation/sharing,"['Python', 'JavaScript', 'HTML', 'CSS', 'Shell', 'Makefile', 'Dockerfile']","[3206425, 674825, 249406, 70153, 30222, 2611, 1864]"
alu0100536829/prct11,,"# MatrixExpansion

## Introducción
    
Gema que permite el uso de matrices densas y dispersas a través de la herencia de una matriz simple. Todas las operaciones con números y fracciones incluidas.
Métodos realizados con programación funcional.

## Instalación

Add this line to your application's Gemfile:

    gem 'matrix_expansion'

And then execute:

    $ bundle

Or install it yourself as:

    $ gem install matrix_expansion

## Diseño

A la hora de realizar las matrices densas se ha realizado de forma natural (como para cualquier matriz simple). El planteamiento importante es para la representación de las matrices dispersas. Hemos decidido finalmente, tras haber pensado en implementarlo con varios vectores, en que era más sencillo, tanto para el acceso como para las operaciones, implementarlo con un array de hash. Cada fila es un array que contiene hashes, donde la clave es la columna del elemento no nulo y el valor es el propio valor del elemento no nulo. De esta  forma, solamente se almacenan los valores nulos. Si se accediera a otra posición se devolvería 0 (aunque este no este almacenado en la matriz dispersa para ahorrar memoria).
",,['Ruby'],[32791]
clementine-player/Android-Remote,Control Clementine from your Android device,"Android-Remote [![Build Status](https://travis-ci.org/clementine-player/Android-Remote.svg)](https://travis-ci.org/clementine-player/Android-Remote)
==============

Clementine Remote lets you remotely control the music player ""Clementine"".

__IMPORTANT:__ 
You need at least Clementine 1.3 to use this remote!

With this application you can control the music player ""Clementine"" while you are sitting on your couch eating potato chips.
You have access to your library, playlists, read the lyrics while listening to your favourite song, enjoy the cover art, ...

If you receive a call or want to make one, you don't have pause the current track, Clementine Remote lowers the volume for you!

You want to hear the current track or album while you are on the go? No problem, download them with one click to your phone! No need to plug in a USB cable, it works via wifi! You can even download whole playlists!

__All Features:__
* Control player
* Download songs from Clementine to your phone
* Browse your library
* Search for songs
* Displays the cover art
* Read the lyrics
* Rate, Love and ban tracks
* Change the volume
* Volume lowers when you receive a call
* Shuffle / Repeat playback
* Playlist selection
* Lockscreen Controls
* Clementine Network Discovery: You don't have to enter the ip, Clementine Remote finds Clementine Players itself in the network!

__INSTALLATION DETAILS:__
Download Clementine 1.3 from here: http://www.clementine-player.org/downloads
* The remote control is disabled by default. You have to activate it in the settings. *
* Downloads are disabled by default. You have to activate it in the setting, too *

Get Clementine Android-Remote:






This application is licensed under the GNU GPLv3.

If you have questions, suggestions etc. please write an e-mail.

Help: https://github.com/clementine-player/Android-Remote/wiki

__PERMISSIONS:__
* android.permission.ACCESS_NETWORK_STATE: Check if you are connected to a wifi network.
* android.permission.ACCESS_WIFI_STATE: Get your current ip address.
* android.permission.CHANGE_WIFI_MULTICAST_STATE: Is needed for Clementine Network Discovery.
* android.permission.INTERNET: To connect to Clementine.
* android.permission.WAKE_LOCK: The device is in partial wake mode when connect to increase stability.
* android.permission.READ_PHONE_STATE: Is needed to detect calls and lower Clementine volume.
* android.permission.WRITE_EXTERNAL_STORAGE: For downloading songs.
* com.android.vending.BILLING: Is needed for doing donations.
",remote control of Clementine,"['Java', 'HTML']","[611522, 2710]"
rafallo/p2c,New way to watch video!,"P2C
===

### New way to watch video!

## Introduction

P2C is a desktop application to stream video media through Bittorrent protocol.

Literally, ""streaming"" is not streaming.
Application can download movie and play retrieved parts. Some people call it ""progressive download"".

## Requirements

- Python 3.2
- QT5
- PyQt
- libtorrent by rastebar with python binding (http://www.libtorrent.org/)
- a few python packages

## Getting started

1. Install requirements
1. ```git clone git+ssh://git@github.com/rafallo/p2c.git```
1. ```python3.2 gui/desktop/main.py```",streaming pre-downloade content ,['Python'],[241891]
gfx/Android-HankeiN,Location Memos on the Map,"# Hankei-N [![Build Status](https://secure.travis-ci.org/gfx/Android-HankeiN.png)](http://travis-ci.org/gfx/Android-HankeiN)




*(Android, Google Play and the Google Play logo are trademarks of Google Inc.)*

# DESCRIPTION

地図上のある地点から半径Nキロメートルの地域を調べるAndroidアプリです。また、「住所メモ」により、任意の地点を地図上に記録できます。

引っ越しの際に参考としてお使いください。

# PRIVACY POLICY

See [PRIVACY-POLICY.md](PRIVACY-POLICY.md) for details.

# DEVELOPMENT INFORMATION

## RUN

* Obtain **Google API Key** from [Google Developer Console](https://cloud.google.com/console?redirected=true#/project)
* Open the project with [Android Studio](http://developer.android.com/sdk/installing/studio.html)

## Open Source Software Licenses

This application depends on a lot of Open Source Software listed in [app/build.gradle](app/build.gradle).

## SEE ALSO

* [Google Map API for Android](https://developers.google.com/maps/documentation/android/start)
* [Android Assets Studio](http://android-ui-utils.googlecode.com/hg/asset-studio/dist/index.html)

# AUTHOR

Fuji, Goro (gfx) 

```
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```
",location memos,['Java'],[100254]
Ydle/RoomBundle,Manage Rooms for the YDLE Project - http://www.ydle.fr,"RoomBundle
==========

Manage Rooms for the YDLE Project - http://www.ydle.fr
",manage rooms component of YDLE Project,['PHP'],[45817]
zeronullity/SDRwatchdog,SDR multiple signal monitoring with P25 & SmartNet trunking support.,"***UNDER CONSTRUCTION***
- Currently only supports SMARTNET/P25 trunk recording with multiple SDR devices on a single trunking system.

SDR watchdog
=================

###Requirements
 - GNURadio >= 3.7
 - DSD
 - OP-25

##Compile
- cd SDRwatchdog
- cmake .
- make -j (number of threads)


##Configure

config file:  **config.json**

Here are the different arguments:
 - **sources** - an array of JSON objects that define the different SDRs available and how to configure them
   - **center** - the center frequency in Hz to tune the SDR to
   - **rate** - the sampling rate to set the SDR to, in samples / second
   - **error** - the tuning error for the SDR in Hz. This is the difference between the target value and the actual value. So if you wanted to recv 856MHz but you had to tune your SDR to 855MHz to actually recieve it, you would set this to -1000000. You should also probably get a new SDR.
   - **gain** - the RF gain to set the SDR to. Use a program like GQRX to find a good value.
   - **ifGain** - [hackrf only] sets the ifgain.
   - **bbGain** - [hackrf only] sets the bbgain.
   - **antenna** - [usrp] lets you select which antenna jack to user on devices that support it
   - **digitalRecorders** - the number of Digital Recorders to have attached to this source. This is essentaully the number of simultanious call you can record at the same time in the frequency range that this SDR will be tuned to. It is limited by the CPU power of the machine. Some experimentation might be needed to find the appropriate number. It will use DSD or OP25 to decode the P25 CAI voice.
   - **analogRecorders** - the number of Analog Recorder to have attached to this source. This is the same as Digital Recorders except for Analog Voice channels.
   - **driver** - the GNURadio block you wish to use for the SDR. The options are *usrp* & *osmosdr*.
   - **device** - For RTL devices or other gr-osmosdr supported devices. For RTL use rtl=""index number"" or rtl=""device serial number"" (you can use rtl_eeprom in the rtl-sdr package to program RTL serial numbers)
 
*Note you can also add other gr-osmosdr settings such as buflen= which may be needed for multiple usb devices. (getting a good signal, lots of cpu power, but still gettings garbled voice only when using multiple usb devices)  config.json example:  device=""rtl=0,buflen=2048""  or device=""rtl=0,buffers=32,buflen=2048"" buflen must be in multiples of 512. If you get any usb errors try unplugging your usb rtl devices and then retry after you plug them back in.  Your not limited to RTL settings you can use other settings that osmosdr supports for other devices I just use RTL as an example because it's the cheapest most common option. 

Check out http://sdr.osmocom.org/trac/wiki/GrOsmoSDR for more information on supported devices and settings.

- **system** - This object defines the trunking system that will be recorded
   - **control_channels** - an array of the control channel frequencies for the system, in Hz. Right now, only the first value is used.
   - **type** - the type of trunking system. The options are *smartnet* & *p25*.
 - **talkgroupsFile** - this is a CSV file that provides information about the talkgroups. It determines whether a talkgroup is analog or digital, and what priority it should have. 

**ChanList.csv**

This file provides info on the different talkgroups in a trunking system. A lot of this info can be found on the Radio Reference website. You need to be a site member to download the table for your system. If you are not, try clicking on the ""List All in one table"" link, selecting everything in the table and copying it into Excel or a spreadsheet.

You will have to add an additional column that adds a priority for each talkgroup. You need that number of recorders available to record a call at that priority. So, 1 is the highest, you would need 2 recorders available to record a priority 2, 3 record for a priority 3 and so on.

The Trunk Record program really only uses the priority information and the Dec Talkgroup ID. The Website uses the same file though to help display information about each talkgroup.

Here are the column headers and some sample data:

| DEC |	HEX |	Mode |	Alpha Tag	| Description	| Tag |	Group | Priority |
|-----|-----|------|-----------|-------------|-----|-------|----------|
|101	| 065	| D	| DCFD 01 Disp	| 01 Dispatch |	Fire Dispatch |	Fire | 1 |
|2227 |	8b3	| D	| DC StcarYard	| Streetcar Yard |	Transportation |	Services | 3 | 


",multiple signal monitoring,"['C++', 'CMake']","[156374, 71228]"
linchproject/linch-servlet,,Dispatcher servlet for a Linch application,servlet for a linch application,['Java'],[20524]
tuanhiep/mqtt-jmeter,This is the plugin for Jmeter to Test MQTT protocol,"mqtt-jmeter
===========

This is the plugin for Jmeter to Test MQTT protocol

Tuan Hiep
ERODS Team
LIG- Grenoble-France


# Introduction

The MQTT Plugin in Jmeter is used for the injection testing of MQTT server. It permits the complete
test correspond many scenarios, which depend on type of messages, type of connections. Thanks to it's
interface graphic, the fact of testing mqtt protocol is taken easily.


# How to install MQTT plugin in Jmeter

From the repository: https://github.com/tuanhiep/mqtt-jmeter  
Get the source code, go to mqtt-jemeter folder and and use the command maven in terminal (Ubuntu):

	mvn clean install package

to obtain the file **mqtt-jmeter.jar** in **mqtt-jemeter/target**.  
Put the **mqtt-jemeter.jar** in the folder **lib/ext** of Jmeter
(to be downloaded on http://jmeter.apache.org/download_jmeter.cgi ).

Remind that, it's necessary to update the file **ApacheJMeter_core.jar** in the repository lib/ext of Jmeter.
Update the file messages.properties in the folder :/org/apache/jmeter/resources/
in **ApacheJMeter_core.jar** by new file messages.properties from
https://github.com/tuanhiep/mqtt-jmeter/tree/master/ressource

#  How to use MQTT plugin in Jmeter

##  MQTT Publisher

The interface graphic of Jmeter:

![Alt text](images/Main_Interface_Jmeter.png)

Right-click “Thread” and choose : Add → Sampler → MQTT Publisher

![Alt text](images/MQTT_Publisher.png)

In the principal interface of MQTT Publisher we have the fields:  
*Connection Info*  

**Name:** Name of the MQTT Publisher  
**Comments:** Your comments  
**Provider URL:** the address of MQTT server example: tcp://localhost:1883  
**Client Id:** Your Id in the session with MQTT server example: Noel De Palma  
**List Topic:** The list of topic's names you want to publish to  
The topic's names are separated by a comma "",""  
For example: List Topic: GRENOBLE/LIG,GRENOBLE/UJF-LIG  
This means, you'll publish to 2 topics: GRENOBLE/LIG and GRENOBLE/UJF-LIG  
You can choose the option **One connection per topic** : It means that for each topic in the list above, the plugin will create one correspondant
 connection. Note that, if the client Id is ""Noel De Palma"", for example, and you have 2 topics in the list, so the plugin will create 2 connections with 2 Client 
Id : ""Noel De Palma 0"" and ""Noel De Palma 1""  
The plugin provide two strategies to publish:  
 **1: Round Robin** : You'll publish to the topics in equal portions and in circular order  
 **2: Random** : You'll publish to a random topic in the list above  
If the list of topic has only one topic, so regardless the strategies, you'll publish to the topic for sure.  
 **Use Authorization check box:** Necessary in the case the connection needs the username and
password  
**User:** Your username  
**Password:** Your password  
**Number of samples to aggregate:** In other way, the number of messages you want to publish to
the MQTT sever in this MQTT Publisher thread, with the value like the configuration below.  
**Message Type:** You can choose : Text, Generated Value, Fixed Value, Random Byte Array (more detail below)  

![Alt text](images/Publisher_Text.png)  

*Encoding*  
 
**Message Format** : The type of encoding that you'll encode your data before publish .You can choose Binary Codec, Base64, BinHex or Plain Text  
If you choose Plain Text, you can choose 6 types of charsets : UTF-8, UTF-16, US-ASCII,UTF-16LE, UTF-16BE, ISO-8859-1.  
Of course, you can choose to no encoding too.  
  
*Option*  

**Add TimeStamp check box:** Add the timestamps to the message. The timestamps is 8 bytes  
**Add Number Sequence check box:** Add the number sequence to the message. Example: if you
publish 100 messages in your session, the message is numbered from 0 to 99. The number sequence 
field in the message is 4 bytes.  
**Retained check box:** You publish the messages as retained messages or not. The retain flag for an
MQTT message is set to false by default. This means that a broker will not hold onto the message 
so that any subscribers arriving after the message was sent will not see the message. By setting 
the retain flag, the message is held onto by the broker, so when the late arrivers connect to the 
broker or clients create a new subscription they get all the relevant retained messages”  
**Quality of service:** Three levels:  
0 : At most once  
1 : At least once  
2 : Exactly once  
Each message in MQTT can have its quality of service and retain flag set. The quality of service
advises the code if and how it should ensure the message arrives. There are three options, 0 (At Most Once),
1 (At Least Once) and 2 (Exactly Once). By default, a new message instance is set to ""At Least Once"",a Quality 
of Service (QoS) of 1, which means the sender will deliver the message at least once and, if there's no acknowledgement
 of it, it will keep sending it with a duplicate flag set until an acknowledgement turns up, at which point the
client removes the message from its persisted set of messages.  
A QoS of 0, ""At Most Once"", is the fastest mode, where the client doesn't wait for an
acknowledgement. This means, of course, that if there’s a disconnection or server failure, a message
may be lost. At the other end of the scale is a QoS of 2, ""Exactly Once"", which uses two pairs of
exchanges, first to transfer the message and then to ensure only one copy has been received and is
being processed. This does make Exactly Once the slower but most reliable QoS setting.

With MQTT Publisher in Jmeter, three type of messages can be sent (Message Type):  
**Text:** The text message, without flag header and the server MQTT can deliver it like a normal
text if you choose *No Encoding* or *Plain Text*.  

![Alt text](images/Publisher_Text.png)  
 
![Alt text](images/Receive.png)  

1 byte “flag header” for the messages of type: Generated value, Fixed value  

![Alt text](images/Flag_Header.png)  
In the flag header, if one field is set to 1, it means, we use the header in the message.
For example: With this flag header  

![Alt text](images/Flag_Header_Example.png)  
It means that, in the message, we have :  
![Alt text](images/Message.png)  

**Generated Value:**  
The generated value can be of type: Integer, Long, Float, Double within the range [Min,Max] .
The type of random can be: Pseudo random or Secure random. In the two cases, we can set the Seed
for the generator.  

![Alt text](images/Publisher_generated_value.png)  
  
**Fixed Value:**  
The fixed value can be of type: Integer, Long, Float, Double, String within the range [Min,Max].  
  
![Alt text](images/Publisher_fixed_value.png)  
**Random Byte Array:**  

The data in form of random byte array with the size array as an input.  
For example, if you type 9 in the field size array, so without encoding, time header, number sequence, the message has 9 bytes of content(random data) and 1 byte of flag header  
and so, 10 bytes to publish.
The images below show when you publish with option : One Connection Per Topic and the data is type of Random Byte Array.
In the terminal,you see that there are two connections of 2 client Id : ""Didier Donsez 0"" and ""Didier Donsez 1"" 
     
![Alt text](images/One_connection_per_topic.png)  

![Alt text](images/Random_Byte_Array.png)  


For mesuring, thanks to Jmeter, we can add some listeners:  
  
![Alt text](images/Publisher_result.png)  

## MQTT Subscriber  
 
 
![Alt text](images/MQTT_Subscriber.png)  
 
 
 
*Name:* Name of the MQTT Subscriber  
*Comments:* Your comments  
*Provider URL:* The address of MQTT server  
*Client Id:* Your Id in the session  
*Topic:* The topic you want to subscribe.  
*Use Authorization :* Necessary in the case the connection need username and password  
*User:* your username  
*Password:* your password  
*Number of samples to aggregate:* In other way, the number of message you want to receive from
the topic in one session  
*Time out (milliseconds):* Timeout for the connection to receive message from the topic  

![Alt text](images/Subscriber_result.png)  
 
 
![Alt text](images/Publisher_Subscriber.png)  
  
  
  
  Grenoble, France 14/03/2014,
  
  ERODS Team
  
  http://www.liglab.fr/erods?lang=fr&var_mode=calcul 
  
    
",test MQTT protocal,"['Java', 'Shell']","[98619, 4633]"
zeisler/active_mocker,Generate mocks from ActiveRecord models for unit tests that run fast because they don’t need to load Rails or a database.,"# ActiveMocker
[![Gem Version](https://badge.fury.io/rb/active_mocker.svg)](http://badge.fury.io/rb/active_mocker)
[![Build Status](https://travis-ci.org/zeisler/active_mocker.svg?branch=master)](https://travis-ci.org/zeisler/active_mocker)
[![Gitter chat](https://badges.gitter.im/zeisler/active_mocker.svg)](https://gitter.im/zeisler/active_mocker)

## Description
Creates stub classes from any ActiveRecord model. 

By using stubs in your tests you don't need to load Rails or the database, sometimes resulting in a 10x speed improvement. 

ActiveMocker analyzes the methods and database columns to generate a Ruby class file. 

The stub file can be run standalone and comes included with many useful parts of ActiveRecord.

Stubbed out methods contain their original argument signatures or ActiveMocker's friendly code can be brought over in its entirety.

Mocks are regenerated when the schema is modified so your mocks won't go stale, preventing the case where your unit tests pass but production code fails.

*Examples from a real app*

		Finished in 1 seconds
		374 examples, 0 failures

## Around the web
[""Mocking ActiveRecord with ActiveMocker"" by Envy](https://web.archive.org/web/20150511052653/http://madewithenvy.com/ecosystem/articles/2015/mocking-activerecord-with-activemocker/)

------------------------------------------

* [Documentation](#documentation)
* [Contact](#contact)
* [Installation](#installation)
* [Setup](#setup)
  * [Generate](#generate_mocks)
* [Dependencies](#dependencies)
* [Usage](#usage)
* [Optional Features](#optional-features)
* [Mocking Methods](#mocking-methods)
* [Managing Mocks](#managing-mocks)
* [ActiveRecord supported methods](#activerecord-supported-methods)
* [Known Limitations](#known-limitations)
* [Inspiration](#inspiration)
* [Contributing](#contributing)


---------------------------

## Documentation [![Inline docs](http://inch-ci.org/github/zeisler/active_mocker.png?branch=master)](http://inch-ci.org/github/zeisler/active_mocker)

[rdoc](http://rdoc.info/github/zeisler/active_mocker/master/ActiveMocker)

------------------------------------------

## Contact

Ask a question in the [chat room](https://gitter.im/zeisler/active_mocker).

------------------------


## Installation

Add this line to your application's Gemfile:
```ruby
group :development, :test do
  gem 'active_mocker'
end
```
It needs to be in development as well as test groups, as the development environment is where mocks will be generated.
Then execute:

    $ bundle

Or install it yourself as:

    $ gem install active_mocker

## Dependencies
* Tested with Rails 4.2, 5.1, 5.2
* Requires Ruby MRI >= 2.3.x

## Setup


  See [example_rails_app](https://github.com/zeisler/active_mocker/tree/master/example_rails_app) for complete setup.


### Generate Mocks

Running this rake task builds/rebuilds the mocks. It will be ran automatically after every schema modification. If the model changes, this rake task needs to be called manually. You could add a file watcher for when your models change and have it run the rake task.

    rake active_mocker:build

## Usage
```ruby
#db/schema.rb

ActiveRecord::Schema.define(version: 20140327205359) do

  create_table ""people"", force: true do |t|
    t.integer  ""account_id""
    t.string   ""first_name"",        limit: 128
    t.string   ""last_name"",         limit: 128
    t.string   ""address"",           limit: 200
    t.string   ""city"",              limit: 100
  end

end
```
--------------
```ruby
#app/models/person.rb

class Person < ActiveRecord::Base
  belongs_to :account

  def self.bar(name, type=nil)
	puts name
  end

end
 ```
----------------- 
  
### Using With Rspec, --tag active_mocker:true

```ruby
require 'rspec'
require 'active_mocker/rspec_helper'
require 'spec/mocks/person_mock'
require 'spec/mocks/account_mock'

describe 'Example', active_mocker:true do

  before do
	Person.create # stubbed for PersonMock.create
  end

end
```       
----------

* Assigning the tag `active_mocker:true` will stub any ActiveRecord model Constants for Mock classes in an `it` or a `before/after(:each)`. This removes any need for dependency injection. Write tests and code like you would normally.
* To stub any Constants in `before(:all)`, `after(:all)` use `active_mocker.find('ClassName')`.
* Mock state will be cleaned up for you in an `after(:all)`. To clean state by yourself, use `active_mocker.delete_all`.

---------
    
```ruby
Person.column_names
  => [""id"", ""account_id"", ""first_name"", ""last_name"", ""address"", ""city""]

person = Person.new( first_name:  ""Dustin"", 
    				 last_name:   ""Zeisler"", 
    				 account:      Account.new )
  => ""#""

person.first_name
  => ""Dustin""
```

### When schema.rb changes, the mock fails
(After `rake db:migrate` is called the mocks will be regenerated.)

```ruby
#db/schema.rb

ActiveRecord::Schema.define(version: 20140327205359) do

  create_table ""people"", force: true do |t|
    t.integer  ""account_id""
    t.string   ""f_name"",        limit: 128
    t.string   ""l_name"",        limit: 128
    t.string   ""address"",       limit: 200
    t.string   ""city"",          limit: 100
  end

end
```
--------------

```ruby
Person.new(first_name: ""Dustin"", last_name: ""Zeisler"")
  =>#
```

### Creating Custom collections

If you want to create a custom set of records that is not part of the global collection for model. (ie. for stubbing in a test)

```ruby
User::ScopeRelation.new([User.new, User.new])
```

This gives the full query API (ie. `find_by`, `where`, etc). 

This is not a feature available in ActiveRecord, so do not include this where you intend to swap for ActiveRecord.


## Optional Features

Use theses defaults if you are starting fresh

```ruby
ActiveMocker::LoadedMocks.features.enable(:timestamps)
ActiveMocker::LoadedMocks.features.enable(:delete_all_before_example)
ActiveMocker::LoadedMocks.features.enable(:stub_active_record_exceptions)
```

### timestamps

  Enables `created_at` and `updated_at` to be updated on save and create

### delete_all_before_example

  When using ""active_mocker/rspec_helper"", it deletes all records from all mocks before each example.

### stub_active_record_exceptions

  When requiring ""active_mocker/rspec_helper"", and adding `active_mocker: true` to the `describe` metadata, these errors will be auto stubbed:
  
  * ActiveRecord::RecordNotFound
  * ActiveRecord::RecordNotUnique
  * ActiveRecord::UnknownAttributeError
    
### Copy over Mock safe methods into the generated mock
  
  Adding the comment `ActiveMocker.safe_methods` at the top of a class marks it as safe to copy to the mock.
  Be careful. It should not contain anything that ActiveMocker cannot run.
  
  ```ruby
  # ActiveMocker.safe_methods(scopes: [], instance_methods: [:full_name], class_methods: [])
  class User
    def full_name
      ""#{first_name} + #{last_name}""
    end
  end
  ```

## Mocking Methods

#### Rspec 3 Mocks - verify double
Verifying doubles is a stricter alternative to normal doubles that provides guarantees about
what is being verified. When using verifying doubles, RSpec will check if the methods
being stubbed are actually present on the underlying object if it is available.
[rspec-mocks/docs/verifying-doubles](https://relishapp.com/rspec/rspec-mocks/docs/verifying-doubles)
```ruby
RSpec.configure do |config|
  config.mock_framework = :rspec
  config.mock_with :rspec do |mocks|
    mocks.verify_doubled_constant_names = true
    mocks.verify_partial_doubles = true
  end
end
```

```ruby
Person.bar('baz')
  => NotImplementedError: ::bar is not Implemented for Class :PersonMock. To continue stub the method.

allow(Person).to receive(:bar) do |name, type=nil|
  ""Now implemented with #{name} and #{type}""
end

Person.bar('foo', 'type')
=> ""Now implemented with foo and type""
```

#### When the model changes, the mock fails
(Requires a regeneration of the mocks files.)

```ruby
#app/models/person.rb

class Person < ActiveRecord::Base
  belongs_to :account

  def self.bar(name)
    puts name
  end

end
```

--------------
```ruby
Person.bar('foo', 'type')
  => ArgumentError: wrong number of arguments (2 for 1)
```
----------------
```ruby
#app/models/person.rb

class Person < ActiveRecord::Base
  belongs_to :account

  def self.foo(name, type=nil)
    puts name
  end

end
 ```   
--------------
```ruby
allow(Person).to receive(:bar) do |name, type=nil|
  ""Now implemented with #{name} and #{type}""
end
=> RSpec::Mocks::MockExpectationError: PersonMock does not implement: bar
 ```     
### Constants and Modules

* Any locally defined modules will not be included or extended. It can be disabled by `ActiveMocker::Config.disable_modules_and_constants = true`

---------------
```ruby
class Person < ActiveRecord::Base
  CONSTANT_VALUE = 13
end
```
-----------------------
```ruby
PersonMock::CONSTANT_VALUE
  => 13
```

### Scoped Methods 
* Any chained scoped methods will be available when the mock file that defines it is required. When called, it raises a `NotImplementedError`. Stub the method with a value to continue.

### Managing Mocks  

```ruby    
require ""active_mocker/rspec_helper""

active_mocker.delete_all # Delete all records from loaded mocks

active_mocker.find(""User"") # Find a mock by model name. Useful in before(:all)/after(:all) where automatic constant stubbing is unavailable.

active_mocker.mocks.except(""User"").delete_all # Delete all loaded mock expect the User mock.

```
### ActiveRecord supported methods

See [Documentation](http://rdoc.info/github/zeisler/active_mocker/master/ActiveMocker) for a complete list of methods and usage.

**Class Methods** - [docs](http://rdoc.info/github/zeisler/active_mocker/master/ActiveMocker/Mock/Base)

  * new
  * create/create!
  * column_names/attribute_names
  * delete_all/destroy_all
  * table_name
  * slice
  * alias_attributes
  
**Query Methods** - [docs](http://rdoc.info/github/zeisler/active_mocker/master/ActiveMocker/Mock/Queries)

  * all
  * find
  * find_by/find_by!
  * find_or_create_by
  * find_or_initialize_by
  * where(conditions_hash)
  * where(key: array_of_values)
  * where.not(conditions_hash)
  * delete_all/destroy_all
  * delete_all(conditions_hash)
  * destroy(id)/delete(id)
  * update_all
  * update(id, attributes)
  * count
  * uniq
  * first/last
  * average(:field_name)
  * minimum(:field_name)
  * maximum(:field_name)
  * sum(:field_name)
  * order(:field_name)
  * reverse_order
  * limit
  * none
  
**Relation Methods** - [docs](http://rdoc.info/github/zeisler/active_mocker/master/ActiveMocker/Mock/Collection)
  * concat
  * include
  * push
  * clear
  * take
  * empty?
  * replace
  * any?
  * many?

**instance methods** - [docs](http://rdoc.info/github/zeisler/active_mocker/master/ActiveMocker/Mock/Queries)
  
  * attributes
  * update
  * save/save!
  * write_attribute/read_attribute
  * delete
  * new_record?
  * persisted?
  * reload
  * attribute_names
  * attribute_present?
  * has_attribute?
  * slice
  * attribute_alias?
  * alias_attributes
  * touch

**has_one/belongs_to/has_many**

  * build_< association >
  * create_< association >
  * create_< association >!
  * < association >.create
  * < association >.build

### Schema/Migration Option Support
* A db/schema.rb is not required.
* All schema types are supported and coerced by [Virtus](https://github.com/solnic/virtus). If coercion fails, the passed value will be retained.
* Default value is supported.
* Scale and Precision are not supported.

### Known Limitations
* Namespaced modules are not currently supported.
* When an association is set in one object it may not always be reflective in other objects, especially when it is a non standard/custom association. See [test_rails_4_app/spec/active_record_compatible_api.rb](https://github.com/zeisler/active_mocker/blob/master/test_rails_4_app/spec/active_record_compatible_api.rb) for a complete list of supported associations. 
* Validation/Callbacks are not supported.
* Sql queries, joins, etc will never be supported.
* A record that has been created and then is modified will persist changes without calling `#save`. Beware of this difference.
* This is not a full replacement for ActiveRecord.
* Primary key will always default to `id`. If this is causing a problem, feel free to open an issue (or even better, a PR =)). 

## Inspiration
Thanks to Jeff Olfert for being my original inspiration for this project.

## Contributing
Your contributions are welcome!

1. Fork it ( http://github.com/zeisler/active_mocker/fork )
2. Create your feature branch (`git checkout -b my-new-feature`)
3. Commit your changes (`git commit -am 'Add some feature'`)
4. Push to the branch (`git push origin my-new-feature`)
5. Create new Pull Request
",create fast stub classes from ActiveRecord Model ,"['Ruby', 'HTML', 'CSS', 'JavaScript']","[255310, 11304, 683, 664]"
NSLS-II/pyRafters,Image processing tools for computed tomography,"pyRafters
==============

Interface layer for CT image processing tools.

[![Build Status](https://travis-ci.org/NSLS-II/pyRafters.png?branch=master)](https://travis-ci.org/NSLS-II/pyRafters)

Base Classes
------------

This project contains base classes to provide a common self-describing
interface for calling image analysis `Tools`.

class family | purpose | module
------------ | ------- | ------
data handlers | to abstract away the backing method of data storage | `pyRafters.handler_base`
`ArgSpec` | container for all the meta-data about arguments | `pyRafters.args_base`
`ToolBase` | container for all of the meta-data + code to run a given computation | `pyRafters.tools_base`

Handlers
--------

The idea of handlers is to provide a layer between the `Tool` code and
the details of how the data is to be stored.  There will be a
sub-class of `BaseSource`/`BaseSink` for each semantic type of data.
Tools validate their data I/O by requiring that arguments be of a
specific sub-class.  For example the class `DistributionSource`
provides the functions:

 - `read_values()`
 - `read_edges()`

which return, respectively, the value and each bin and the left edges
of the bins.  A `Tool` that needs a distribution as input can thus
take a `DistributionSource` instance and work correctly, independent
of the details of exactly how the data is stored.


Eventually, the `handlers` module should include reference
implementations for all of the data types that can be stored in hdf files.

`ArgSpec`
---------

These classes provide enough meta-data about the arguments
(parameters, input data, output data) to auto-generate user interfaces
from the `Tool` class.


Example Tools
-------------



The tools in `pyRafters.tools` are intended to provide a template for adding new tools.
",computation tomograophy image processing tools,"['Python', 'Shell']","[125428, 6712]"
liquidise/Quickbase-Gem,Ruby Gem for rapid interfacing with Intuit Quickbase,"#AIS QuickBase Ruby Gem

[![Gem Version](https://badge.fury.io/rb/advantage_quickbase.svg)](http://badge.fury.io/rb/advantage_quickbase)

This gem is designed to be a concise, clear and maintainable collection of common Quickbase API calls used in ruby development. It implements a subset of the total Quickbase API.

##Example
```ruby
require 'quickbase'

# Create a new API connection

qb_api = AdvantageQuickbase::API.new( domain, username, password, app_token, ticket, user_token )

# Load all of the Books in our table
query_options = { query: ""{6.EX.'Book'}"", clist: [7] }
books = qb_api.do_query( 'books_db_id', query_options )

puts books.inspect
# => [ {""7"" => ""Lord of the Flies""}, {""7"" => ""The Giver""} ]
```

##API Documentation
###New Connection

```ruby
qb_api = Advantage::QuickbaseAPI.new( domain, username, password, app_token, ticket, user_token )
```
###Find
**find(db\_id, record\_id, query\_options)** => **[json] record**

`query_options` expects a hash containing any (or none) of the following options:

* `clist` - a list (Array or period-separated string) of fields to return
* `fmt` - defaults to ""structured""; use `fmt: ''` to set api responses to unstructured


```ruby
#Load the record that has a Record ID 8 from the books table
book = qb_api.find( 'books_db_id', 8, clist: [3, 7] )

puts book.inspect
# => {""3"" => ""8"", ""7"" =>""The Giver""}
```

###Get DB Var
**get\_db\_var( app_id, variable_name )** => **[string] Variable Value**
```ruby
value = qb_api.get_db_var( 'abcd1234', 'test' )
````

###Set DB Var
**set\_db\_var( app_id, variable_name, value=nil )** => **[bool] Success?**
```ruby
qb_api.set_db_var( 'abcd1234', 'test', 'value' )
````

###Do Query Count
**do\_query\_count( db_id, query=nil )** => **[int] Record Count**

```ruby
today = Date.today.strftime( '%Y-%m-%d' )
num_records = qb_api.do_query_count( 'abcd1234', ""{1.EX.'#{today}'}"" )
````

###Do Query
**do\_query( db\_id, query\_options )** => **[json] records**

`query_options` expects a hash containing any (or none) of the following options:

* `query` - typical Quickbase query string. ex: `""{3.EX.'123'}""`
* `qid` - report or query id to load (should not be used with `query` or `qname`)
* `qname` - report or query name to load (should not be used with `query` or `qid`)
* `clist` - a list (Array or period-separated string) of fields to return
* `slist` - a list (Array or period-separated string) of fields to sort by
* `fmt` - defaults to ""structured""; use `fmt: ''` to set api responses to unstructured
* `options` - string of additional options. ex: `""num-200.skp-#{records_processed}""`


```ruby
records = qb_api.do_query( 'bdjwmnj33', query: ""{3.EX.'123'}"", clist: [3, 6, 10] )
```

###Add Record
**add\_record( db\_id, new\_data )** => **[int] New Record Id**

```ruby
new_data = { 6 => 'Book', 7 => 'My New Title', 8 => 'John Smith'}
new_record_id = qb_api.add_record( 'abcd1234', new_data )
````

###Edit Record
**edit\_record( db\_id, record\_id, new\_data )** => **[bool] Success?**

```ruby
new_data = { 7 => 'My Second Title', 8 => 'John Smith'}
call_successful = qb_api.edit_record( 'abcd1234', 136, new_data )
````

###Delete Record
**delete\_record( db\_id, record\_id )** => **[bool] Success?**

```ruby
call_successful = qb_api.delete_record( 'abcd1234', 136 )
````

###Purge Records
**purge\_records( db\_id, options )** => **[int] Records Deleted**

`options` expects a hash containing any of the following options:

* `query` - typical Quickbase query string. ex: `""{3.EX.'123'}""`
* `qid` - report or query id to load (should not be used with `query` or `qname`)
* `qname` - report or query name to load (should not be used with `query` or `qid`)


```ruby
records_deleted = qb_api.purge_records( 'abcd1234', {qid: 6} )
````

###Get Schema
Get the complete schema of the whole quickbase app


**get_schema( db_id )**

```ruby
app_schema = qb_api.get_schema( 'abcd1234' )
````

###Import From CSV
**import\_from\_csv( db\_id, data, column\_field\_ids )** => **[json] New Record Ids**

```ruby
new_data = [
  ['Book', 'Lord of the Flies', 'William Golding'],
  ['Book', 'A Tale of Two Cities', 'Charles Dickens'],
  ['Book', 'Animal Farm', 'George Orwell']
]
record_ids = qb_api.import_from_csv( 'abcd1234', new_data, [6, 7, 8] )
````


###Create App Token
Create an app token that gives you access to that Quickbase app


**create\_app\_token(db\_id, description, page\_token)**

* `db_id` - database id
* `description` - description of what the token is for
* `page_token` - token hidden in the page DOM


```ruby
app_token = qb_api.create_app_token( 'abcd1234', 'Access all the books in the database', 'TugHxxkil9t6Kdebac' )
````
",rapid interfacing with Quickbase API calls,['Ruby'],[24958]
franksl/ncsvlib,,"# NCsvLib

## Introduction

NCsvLib is a .NET library for writing 'structured' text files/streams.
An example of structured file is CSV (comma separated values), but the library
can handle more complex structures.
Basically you define a 'schema' that describes size and format of various
fields that the library then reads from a data source (ie. database) and
writes their values to an output stream (ie. text file or http stream).
Its architecture makes possible to use various methods for 
storing data (ie. on database) and define file schema (default is XML 
file).

## Installation

You can download the latest compiled version from:

http://sourceforge.net/projects/ncsvlib/

Simply copy NCsvLib.dll in the same directory of your project executables.
The file LICENSE should be deployed with NCsvLib.dll.
",writing structured text files/streams,['C#'],[148268]
sangotaro/my-boxen,,"# Our Boxen

This is a template Boxen project designed for your organization to fork and
modify appropriately.
The Boxen rubygem and the Boxen puppet modules are only a framework for getting
things done.
This repository template is just a basic example of _how_ to do things with them.

## Getting Started

To give you a brief overview, we're going to:

* Install dependencies (basically Xcode)
* Bootstrap a boxen for your self/team/org/company
* Then convert your local copy of that boxen to the post-bootstrapped version

There are a few potential conflicts to keep in mind.
Boxen does its best not to get in the way of a dirty system,
but you should check into the following before attempting to install your
boxen on any machine (we do some checks before every Boxen run to try
and detect most of these and tell you anyway):

* Boxen __requires__ at least the Xcode Command Line Tools installed.
* Boxen __will not__ work with an existing rvm install.
* Boxen __may not__ play nice with a GitHub username that includes dash(-)
* Boxen __may not__ play nice with an existing rbenv install.
* Boxen __may not__ play nice with an existing chruby install.
* Boxen __may not__ play nice with an existing homebrew install.
* Boxen __may not__ play nice with an existing nvm install.
* Boxen __recommends__ installing the full Xcode.

### Dependencies

**Install the Xcode Command Lines Tools and/or full Xcode.**
This will grant you the most predictable behavior in building apps like
MacVim.

How do you do it?

#### OS X 10.9 (Mavericks)

If you are using [`b26abd0` of boxen-web](https://github.com/boxen/boxen-web/commit/b26abd0d681129eba0b5f46ed43110d873d8fdc2)
or newer, it will be automatically installed as part of Boxen.
Otherwise, follow instructions below.

#### OS X < 10.9

1. Install Xcode from the Mac App Store.
1. Open Xcode.
1. Open the Preferences window (`Cmd-,`).
1. Go to the Downloads tab.
1. Install the Command Line Tools.

### Bootstrapping

Create a **new** git repository somewhere on the internet.
It can be private or public -- it really doesn't matter.
If you're making a repository on GitHub, you _may not_ want to fork this repo
to get started.
The reason for that is that you can't really make private forks of public
repositories easily.

Once you've done that, you can run the following to bootstrap
your boxen:

```
sudo mkdir -p /opt/boxen
sudo chown ${USER}:staff /opt/boxen
git clone https://github.com/boxen/our-boxen /opt/boxen/repo
cd /opt/boxen/repo
git remote rm origin
git remote add origin 
git push -u origin master
```

Now that your boxen is bootstrapped, you can run the following to
install the default configuration from this repo:

```
cd /opt/boxen/repo
./script/boxen
```

### Distributing

That's enough to get your boxen into a usable state on other machines,
usually.
From there, we recommend setting up
[boxen-web](https://github.com/boxen/boxen-web)
as an easy way to automate letting other folks install your boxen.

If you _don't_ want to use boxen-web, folks can get using your boxen like so:

```
sudo mkdir -p /opt/boxen
sudo chown ${USER}:staff /opt/boxen
git clone  /opt/boxen/repo
cd /opt/boxen/repo
./script/boxen
```

Keep in mind this requires you to encrypt your hard drive by default.
If you do not want to do encrypt your hard drive, you can use the `--no-fde`.

```
./script/boxen --no-fde
```

It should run successfully, and should tell you to source a shell script
in your environment.
For users without a bash or zsh config or a `~/.profile` file,
Boxen will create a shim for you that will work correctly.
If you do have a `~/.bashrc` or `~/.zshrc`, your shell will not use
`~/.profile` so you'll need to add a line like so at _the end of your config_:

``` sh
[ -f /opt/boxen/env.sh ] && source /opt/boxen/env.sh
```

Once your shell is ready, open a new tab/window in your Terminal
and you should be able to successfully run `boxen --env`.
If that runs cleanly, you're in good shape.

## What You Get

This template project provides the following by default:

* Homebrew
* Git
* Hub
* dnsmasq w/ .dev resolver for localhost
* rbenv
* Full Disk Encryption requirement
* Node.js 0.6
* Node.js 0.8
* Node.js 0.10
* Ruby 1.9.3
* Ruby 2.0.0
* Ruby 2.1.0
* Ruby 2.1.1
* ack
* Findutils
* GNU tar

## Customizing

You can always check out the number of existing modules we already
provide as optional installs under the
[boxen organization](https://github.com/boxen). These modules are all
tested to be compatible with Boxen. Use the `Puppetfile` to pull them
in dependencies automatically whenever `boxen` is run.

### Including boxen modules from github (boxen/puppet-)

You must add the github information for your added Puppet module into your Puppetfile at the root of your
boxen repo (ex. /path/to/your-boxen/Puppetfile):

    # Core modules for a basic development environment. You can replace
    # some/most of these if you want, but it's not recommended.

    github ""repository"", ""2.0.2""
    github ""dnsmasq"",    ""1.0.0""
    github ""gcc"",        ""1.0.0""
    github ""git"",        ""1.2.2""
    github ""homebrew"",   ""1.1.2""
    github ""hub"",        ""1.0.0""
    github ""inifile"",    ""0.9.0"", :repo => ""cprice404/puppetlabs-inifile""
    github ""nginx"",      ""1.4.0""
    github ""nodejs"",     ""2.2.0""
    github ""ruby"",       ""4.1.0""
    github ""stdlib"",     ""4.0.2"", :repo => ""puppetlabs/puppetlabs-stdlib""
    github ""sudo"",       ""1.0.0""

    # Optional/custom modules. There are tons available at
    # https://github.com/boxen.

    github ""java"",     ""1.1.0""

In the above snippet of a customized Puppetfile, the bottom line
includes the Java module from Github using the tag ""1.1.0"" from the github repository
""boxen/puppet-java"".  The function ""github"" is defined at the top of the Puppetfile
and takes the name of the module, the version, and optional repo location:

    def github(name, version, options = nil)
      options ||= {}
      options[:repo] ||= ""boxen/puppet-#{name}""
      mod name, version, :github_tarball => options[:repo]
    end

Now Puppet knows where to download the module from when you include it in your site.pp or mypersonal.pp file:

    # include the java module referenced in my Puppetfile with the line
    # github ""java"",     ""1.1.0""
    include java

### Hiera

Hiera is preferred mechanism to make changes to module defaults (e.g. default
global ruby version, service ports, etc). This repository supplies a
starting point for your Hiera configuration at `config/hiera.yml`, and an
example data file at `hiera/common.yaml`. See those files for more details.

The default `config/hiera.yml` is configured with a hierarchy that allows
individuals to have their own hiera data file in
`hiera/users/{github_login}.yaml` which augments and overrides
site-wide values in `hiera/common.yaml`. This default is, as with most of the
configuration in the example repo, a great starting point for many
organisations, but is totally up to you. You might want to, for
example, have a set of values that can't be overridden by adding a file to
the top of the hierarchy, or to have values set on specific OS
versions:

```yaml
# ...
:hierarchy:
  - ""global-overrides.yaml""
  - ""users/%{::github_login}""
  - ""osx-%{::macosx_productversion_major}""
  - common
```

### Node definitions

Puppet has the concept of a
['node'](http://docs.puppetlabs.com/references/glossary.html#agent),
which is essentially the machine on which Puppet is running. Puppet looks for
[node definitions](http://docs.puppetlabs.com/learning/agent_master_basic.html#node-definitions)
in the `manifests/site.pp` file in the Boxen repo. You'll see a default node
declaration that looks like the following:

``` puppet
node default {
  # core modules, needed for most things
  include dnsmasq

  # more...
}
```

### How Boxen interacts with Puppet

Boxen runs everything declared in `manifests/site.pp` by default.
But just like any other source code, throwing all your work into one massive
file is going to be difficult to work with. Instead, we recommend you
use modules in the `Puppetfile` when you can and make new modules
in the `modules/` directory when you can't. Then add `include $modulename`
for each new module in `manifests/site.pp` to include them.
One pattern that's very common is to create a module for your organization
(e.g., `modules/github`) and put an environment class in that module
to include all of the modules your organization wants to install for
everyone by default. An example of this might look like so:

``` puppet
# modules/github/manifests/environment.pp

 class github::environment {
   include github::apps::mac

   include ruby::1-8-7

   include projects::super-top-secret-project
 }
```

 If you'd like to read more about how Puppet works, we recommend
 checking out [the official documentation](http://docs.puppetlabs.com/)
 for:

 * [Modules](http://docs.puppetlabs.com/learning/modules1.html#modules)
 * [Classes](http://docs.puppetlabs.com/learning/modules1.html#classes)
 * [Defined Types](http://docs.puppetlabs.com/learning/definedtypes.html)
 * [Facts](http://docs.puppetlabs.com/guides/custom_facts.html)

### Creating a personal module

See [the documentation in the
`modules/people`](modules/people/README.md)
directory for creating per-user modules that don't need to be applied
globally to everyone.

### Creating a project module

See [the documentation in the
`modules/projects`](modules/projects/README.md)
directory for creating organization projects (i.e., repositories that people
will be working in).

## Binary packages

We support binary packaging for everything in Homebrew, rbenv, and nvm.
See `config/boxen.rb` for the environment variables to define.

## Sharing Boxen Modules

If you've got a Boxen module you'd like to be grouped under the Boxen org,
(so it can easily be found by others), please file an issue on this
repository with a link to your module.
We'll review the code briefly, and if things look pretty all right,
we'll fork it under the Boxen org and give you read+write access to our
fork.
You'll still be the maintainer, you'll still own the issues and PRs.
It'll just be listed under the boxen org so folks can find it more easily.

##upgrading boxen
See [FAQ-Upgrading](https://github.com/boxen/our-boxen/blob/master/docs/faq.md#q-how-do-you-upgrade-your-boxen-from-the-public-our-boxen).

## Integrating with Github Enterprise

If you're using a Github Enterprise instance rather than github.com,
you will need to set the `BOXEN_GITHUB_ENTERPRISE_URL` and
`BOXEN_REPO_URL_TEMPLATE` variables in your
[Boxen config](config/boxen.rb).

## Halp!

See [FAQ](https://github.com/boxen/our-boxen/blob/master/docs/faq.md).

Use Issues or #boxen on irc.freenode.net.
",manage mac development environments,"['Ruby', 'Puppet', 'Shell']","[13005, 3394, 1280]"
marsender/atoll-digital-library,Atoll Digital Library is a document publishing system that allows users to create their own digital library.,"
## Atoll Digital Library ##

Atoll Digital Library is a document publishing system that allows users to create online collections of ebooks.

It provides a cross platform search engine that integrate ePub eBooks or any XML document (XHTML, DocBook, TEI, etc.), specifying the items to index, the page breaks and the stylesheets to apply for rendering.

This program is distributed under the GNU General Public License Version 3.
Read the LICENSE file for more about the license.

Read the INSTALL file for prerequisites and installation instructions.

Project home page: [Atoll Digital Library](https://atoll-digital-library.org/en).
",create personal ebook library collections,"['C++', 'HTML', 'CMake', 'Makefile', 'XSLT', 'NSIS', 'Batchfile', 'Shell', 'C', 'CSS']","[1695768, 73172, 54943, 32364, 20484, 9380, 4507, 1341, 407, 282]"
mikesname/blueprints-sql-graph,Noddy JDBC Blueprints implementation. Use at your own risk.,"Quick and dirty SQL/JDBC implementation for ""Blueprints"":https://github.com/tinkerpop/blueprints.

Written solely for the sake of it.

_Warning_: Current caveats:

* Does no optimisations, like property caching, whatsoever
* Leaks resources unless you cast the _Iterable_s to _CloseableIterable_s and close them explicitly.
* Untested with databases other than H2 and Postgres
* Only supports primitive types and strings as property values
* Mmmn, probably lots more I haven't thought of...

To try this out in the Gremlin repl you need to put some jars on the Gremlin-Groovy standalone classpath:

* blueprints-sql-graph-1.0.jar (this thing)
* Your database driver, i.e. postgresql-9.1-901.jdbc4.jar, or h2.jar

If you're using a Gremlin distribution compiled from Github, these extra jars need to be copied to:

bc. $GREMLIN_HOME/gremlin-groovy/target/gremlin-groovy-2.4.0-SNAPSHOT-standalone/lib/

h3. Loading a graph in the Gremlin repl:

h4. H2 in-memory database:

bc. gremlin> import com.tinkerpop.blueprints.impls.sql.SqlGraph
gremlin> g = new SqlGraph([""sql.datasource.class"": ""org.h2.jdbcx.JdbcDataSource"",
""sql.datasource.url"": ""jdbc:h2:mem:test""])
gremlin> g.createSchemaIfNeeded()
gremlin> v1 = g.addVertex()
gremlin> v1.addEdge(""knows"", g.addVertex())
gremlin> g.commit()

h4. Postgres:

For server DBs you *first* have to create an empty database, with a username/password. The graph can be loaded thusly:

bc. gremlin> import com.tinkerpop.blueprints.impls.sql.SqlGraph
==> ...
gremlin> g = new SqlGraph([""sql.datasource.class"": ""org.postgresql.ds.PGSimpleDataSource"",
""sql.datasource.serverName"": ""example.com"", ""sql.datasource.databaseName"": ""blueprints"", ""sql.datasource.user"": ""..."",
""sql.datasource.password"": ""...""])
gremlin> g.createSchemaIfNeeded()
==>null

The parameter for the constructor is a map specifying the class of the datasource + any additional properties of the
datasource and optionally the names of the tables to be used for storage:

* _sql.datasource.class_ - the name of the datasource class from some JDBC driver on the classpath
* _sql.datasource.*_ - any properties of the datasource can be passed using this prefix. E.g. sql.datasource.portNumber, sql.datasource.serverName, etc. See the documentation of the datasource for the list of available properties.
* _sql.verticesTable_ - the name of the vertices table. Defaults to ""vertices"".
* _sql.edgesTable_ - the name of the edges table. Defaults to ""edges"".
* _sql.vertexPropertiesTable_ - the name of the table for vertex properties. Defaults to ""vertex_properties"".
* _sql.edgePropertiesTable_ - the name of the table for edge properties. Defaults to ""edge_properties"".

h3. Potential optimisations.

This is more of a toy than an serious attempt at performant graph database backed by an RDBMS. As such, there are
no optimizations done either at the schema level or in the codebase above declaring indexes on frequently accessed
columns.
","implement ""Blueprints"" project's generic Graph API",['Java'],[69472]
SomethingExplosive/android_frameworks_av,SoMeX android_frameworks_av,,,"['C++', 'C', 'Assembly', 'Objective-C', 'Perl', 'Shell']","[13539789, 9915253, 1218838, 196955, 7262, 640]"
wardrobecms/locales,Wardrobe Language Files,"Wardrobe Language Files
=======================

To create a new language pack please clone this repo and add new files into the lang/ folder. Please put yours in a sub folder for the region.

To use these with your install download the files and move the lang/:yourlang: into your Wardrobe app/lang folder.
",provide extra language files for Wardrobe blog,['PHP'],[140849]
lbitonti/liquibase-hana,Liquibase Hana extension,"Liquibase extension that provides support for  SAP Hana DB.

Initial Hana DB support is currently implemented in the 2.0.x and 3.0.x branches. The master branch at the moment follows 3.4.x and it's built against version 3.4.2 of Liquibase.

The original Liquibase distribution is available at: https://github.com/liquibase/liquibase/

The official liquibase site is: http://www.liquibase.org

Released under the Apache 2.0 License with no guarantee whatsoever.

This is currently used in some projects I have been involved with. Branch 3.0.x has definitely undergone more testing at this point, but 2.0.x is still used in production with good results.
If you give it a go, I'd like to know how it worked for you.

It's available from Maven Central. If you are using maven, add the following dependency:


com.github.lbitonti
liquibase-hana
3.1.1



The following generators are currently implemented (please keep in mind Hana limitations for some operations while using them):

AddAutoIncrementGenerator
AddColumnGenerator
AddDefaultValueGenerator
AddForeignKeyConstraintGenerator
AddPrimaryKeyGenerator
AddUniqueConstraintGenerator
AlterSequenceGenerator
CreateIndexGenerator
CreateSequenceGenerator
CreateTableGenerator
CreateViewGenerator
DropColumnGenerator
DropForeignKeyConstraintGenerator
DropIndexGenerator
DropPrimaryKeyGenerator
DropSequenceGenerator
DropTableGenerator
DropUniqueConstraintGenerator
DropViewGenerator
GetViewDefinitionGenerator
RenameColumnGenerator
RenameTableGenerator
RenameViewGenerator
SetNullableGenerator

",track database changes on SAP HANA database management system,['Java'],[252087]
Querela/ekIRC,A Java IRC Chat implementation,"ekIRC
=====

A Java IRC Chat implementation

 - Java 1.7 !

--

 - [TODO List](https://raw.github.com/Querela/ekIRC/master/TODO.txt)

## License
 - This software is licensed under the [GNU General Public License, version 3 (GNU GPL v3)](http://www.gnu.org/licenses/gpl.html).
 - Contact me if there are conflicts/right violations/etc. with existing projects.


Some parts of the source code are from/were inspired by:
 - Bukkit: (event system)
  - http://jd.bukkit.org/rb/doxygen/d5/dff/RegisteredListener_8java_source.html
  - http://jd.bukkit.org/rb/doxygen/d9/d7a/EventExecutor_8java_source.html
  - (http://jd.bukkit.org/rb/doxygen/da/dd8/Listener_8java_source.html)
  - http://jd.bukkit.org/rb/doxygen/da/d93/EventException_8java_source.html
  - http://jd.bukkit.org/rb/doxygen/d6/dc9/EventHandler_8java_source.html
  - http://jd.bukkit.org/rb/doxygen/d9/d11/HandlerList_8java_source.html
  - http://jd.bukkit.org/rb/doxygen/d1/ddb/JavaPluginLoader_8java_source.html#l00359
  - http://jd.bukkit.org/rb/doxygen/df/db4/SimplePluginManager_8java_source.html#l00497

 - pircbotx
  - http://code.google.com/p/pircbotx/ (project home on google code)
",implment Internet Relay Chat (IRC),['Java'],[325793]
tbruyelle/HappyContacts,Birthday and name-day reminder for Android,"Dependencies
==========

* libs directory
* facebook for android : you need to clone the repo https://github.com/facebook/facebook-android-sdk/tree/master/facebook
and add as a library dependency to HappyContacts
",remind users of birthdays and name-days,"['Java', 'Shell']","[409437, 830]"
videolan/x265,https://bitbucket.org/multicoreware/x265/ git mirror,"=================
x265 HEVC Encoder
=================

| **Read:** | Online `documentation `_ | Developer `wiki `_
| **Download:** | `releases `_ 
| **Interact:** | #x265 on freenode.irc.net | `x265-devel@videolan.org `_ | `Report an issue `_

`x265 `_ is an open
source HEVC encoder. See the developer wiki for instructions for
downloading and building the source.

x265 is free to use under the `GNU GPL `_ 
and is also available under a commercial `license `_ 
",high efficiency video coding,"['Assembly', 'C++', 'C', 'CMake', 'Batchfile', 'Shell']","[6608380, 5265652, 262055, 64334, 13662, 3846]"
xenserver/xsconsole,,"What is this repository ?
==========================

This is the repository of XenServer-specific commits to the Xen project
xsconsole.

xsconsole is python based and provides and a ncurses GUI to get/set 
XenServer information from within a Domain0 console.

To contribute bug fixes, email them to the XenServer development mailing

list (xs-devel@lists.xenserver.org).


What other documentation is available ?
========================================

To install xsconsole for XenServer please read INSTALL.
",get and set XenServer information                                                                                                          ,"['Python', 'Makefile', 'Shell', 'Ruby']","[745241, 8517, 5716, 5338]"
higanworks-cookbooks/mruby,Install mruby,"mruby Cookbook
==============

This cookbook installs mruby. 

- /usr/local/bin/mruby
- /usr/local/bin/mirb
- /usr/local/bin/mrbc

Includes recipe ngx_mruby helper.

[http://community.opscode.com/cookbooks/mruby](http://community.opscode.com/cookbooks/mruby)

Platform
-------

The following platforms are supported and tested under test kitchen:

* Ubuntu 12.04
* CentOS 6.4


TODO
----

- create LWRP

Requirements
------------

- gcc  (package or source)
- make (package or source)

### Recommends

- 'build-essential' (Communty)


### Depends

- 'nginx' (Communty)
- 'apache2' (Communty)


Attributes
----------

### default.rb

- `node[:mruby][:build_dir] - directory to build`
  -   default: `'/opt/chef_mruby'`
- `node[:mruby][:use_chef_ruby]`           - Use ruby chef runtime.
  -   default: `true`
- `node[:mruby][:add_path]`  - create symlink to. If you don't need link, set nil.
  -   default: `'/usr/local/bin'`
-   `node[:mruby][:git_refernce]` - branch or tag of mruby repository
  - default: `'master'`

- `node[:mruby][:build_options][:bins]`     - symlink target binaries
  - default:`%w(mruby mrbc mirb)`
- `node[:mruby][:build_options][:user_gems]` - user mgem to install
  - default:`[]`
  - Add user Gem example: Arrays of methd(Stting like a symbol) and url(Stting).

```
node[:mruby][:build_options][:user_gems] = [
  [':git', 'https://github.com/iij/mruby-io.git']
]
```

### depends.rb

- `node[:mruby][:depend_pkgs]` - packcage dependencies(`action :upgrade`)
  - default: `['git','rsync']`

### ngx_mruby.rb

- `node[:mruby][:ngx_mruby][:git_refernce]` - branch or tag of ngx_mruby repository
  - default: `'master'`


### mod_mruby.rb

- `node[:mruby][:mod_mruby][:git_refernce]` - branch or tag of mod_mruby repository
  - default: `'master'`

- node[:apache2][:mod_mruby][:config][:by_line] = puts lines to mruby.conf
  - default: ['AddHandler mruby-script .rb'] (Array)


Recipes
----

### default.rb

Install mruby to `/usr/local/bin`.

#### suggests cookbooks

- 'build-essential' (Opscode)

#### Usage

add `mruby::default` to run_list.


### depends.rb

Install package dependencies.

It's included by default.rb. Nothing to do.


### ruby_install(instability support)

install `ruby-2.0.0-p247` with rbenv to system global.

#### depends cookbooks

- rbenv cookbook(Community)

#### Usage

add `mruby::ruby_install` to run_list.

### ngx_mruby

Regist config option to nginx build options.

#### depends cookbooks

- nginx(Community)


Usage
-----

add `mruby::ngx_mruby,nginx::default` to run_list.

### Example

**Attributes(test-kitchen format)**

```
- name: ngx_mruby
  run_list:
    - ""recipe[build-essential::default]""
    - ""recipe[mruby::ngx_mruby]""
    - ""recipe[nginx]""
  attributes:
    nginx:
      install_method: source
      version: 1.4.2
      configure_flags: [""--with-debug""]
      source:
        modules:
          - http_ssl_module
          - http_geoip_module
          - http_realip_module
          - http_stub_status_module
          - http_gzip_static_module
    mruby:
      force_rebuild: true
      build_options:
        user_gems:
          -  [':git', 'https://github.com/iij/mruby-io.git']
```

ChefClient converges below.

```
# /opt/nginx-1.4.2/sbin/nginx -V
nginx version: nginx/1.4.2
built by gcc 4.6.3 (Ubuntu/Linaro 4.6.3-1ubuntu5) 
TLS SNI support enabled
configure arguments:
--prefix=/opt/nginx-1.4.2
--conf-path=/etc/nginx/nginx.conf
--sbin-path=/opt/nginx-1.4.2/sbin/nginx
--with-debug
--add-module=/opt/chef_mruby/ngx_mruby
--add-module=/opt/chef_mruby/ngx_mruby/dependence/ngx_devel_kit
--with-http_ssl_module
--with-http_geoip_module
--with-ld-opt='-Wl,-R,/usr/local/lib -L /usr/local/lib'
--with-http_realip_module
--with-http_stub_status_module
--with-http_gzip_static_module
```

### Example (JSON style attribute)

```
{
  ""run_list"" : [
    ""recipe[build-essential::default]"",
    ""recipe[mruby::ngx_mruby]"",
    ""recipe[nginx]""
  ],
  ""mruby"": {
    ""force_rebuild"" : true,
    ""build_options"" : {
       ""user_gems"" : [
          ["":git"", ""https://github.com/iij/mruby-io.git""]
        ]
    }
  },
  ""nginx"" : {
    ""install_method"" : ""source"",
    ""version"" : ""1.4.2"",
    ""configure_flags"" : [
      ""--with-debug""
    ],
    ""source"" : {
    ""modules"" : [
      ""http_ssl_module"",
      ""http_geoip_module"",
      ""http_realip_module"",
      ""http_stub_status_module"",
      ""http_gzip_static_module""
    ]
    }
  }
}
```

### mod_mruby

Build mod_mruby.so and regist config to apache httpd .

Usage
-----

add `mruby::mod_mruby` to run_list.


#### depends cookbooks

- apache2(Community)


### Example

**Attributes(test-kitchen format)**

```
- name: mod_mruby
  run_list:
    - ""recipe[build-essential::default]""
    - ""recipe[mruby::mod_mruby]""
  attributes:
    apache:
    mruby:
      build_type: debug
      force_rebuild: true
      git_refernce: abe6db945491105ac265884990b73af0a073d16d
      build_options:
        user_gems:
          # - [':git', 'git://github.com/iij/mruby-process.git']
          # - [':git', 'git://github.com/iij/mruby-pack.git']
          # - [':git', 'git://github.com/iij/mruby-digest.git']
          - [':git', 'git://github.com/mattn/mruby-json.git']
          # - [':git', 'git://github.com/mattn/mruby-curl.git']
          - [':git', 'git://github.com/matsumoto-r/mruby-thread.git']
          # - [':git', 'git://github.com/matsumoto-r/mruby-redis.git']
          - [':git', 'git://github.com/matsumoto-r/mruby-vedis.git']
          - [':git', 'git://github.com/matsumoto-r/mruby-sleep.git']
          - [':git', 'git://github.com/matsumoto-r/mruby-config.git']
          - [':git', 'git://github.com/masamitsu-murase/mruby-hs-regexp.git']
```


ChefClient converges below.

```
# httpd -M
Loaded Modules:
 core_module (static)
 mpm_prefork_module (static)
 http_module (static)
 so_module (static)
 mruby_module (shared)
 alias_module (shared)
 auth_basic_module (shared)
 authn_file_module (shared)
 authz_default_module (shared)
 authz_groupfile_module (shared)
 authz_host_module (shared)
 authz_user_module (shared)
 autoindex_module (shared)
 deflate_module (shared)
 dir_module (shared)
 env_module (shared)
 log_config_module (shared)
 logio_module (shared)
 mime_module (shared)
 negotiation_module (shared)
 setenvif_module (shared)
 status_module (shared)
Syntax OK
```


Test
---

### Install test dependencies

`bundle`

### test

`kitchen test`

#### Platforms for test-kitchen

```
 $ kitchen list
Instance               Driver   Provisioner  Last Action
default-ubuntu-1204    Vagrant  ChefSolo     
default-centos-64      Vagrant  ChefSolo     
rbenv-ubuntu-1204      Vagrant  ChefSolo     
rbenv-centos-64        Vagrant  ChefSolo     
ngx-mruby-ubuntu-1204  Vagrant  ChefSolo     
ngx-mruby-centos-64    Vagrant  ChefSolo     
mod-mruby-ubuntu-1204  Vagrant  ChefSolo     
mod-mruby-centos-64    Vagrant  ChefSolo     
```

You can test specific recipe.

```
kitchen converge mod-mruby
```

Contributing
------------

1. Fork the repository on Github
2. Create a named feature branch (like `add_component_x`)
3. Write you change
4. Write tests for your change (if applicable)
5. Run the tests, ensuring they all pass
6. Submit a Pull Request using Github

License and Authors
-------------------
Authors:  Yukihiko Sawanobori (HiganWorks LLC)

under MIT License

",install mruby,['Ruby'],[5480]
muchomasfacil/WysiwygBundle,"This bundles makes really easy to add ckeditor4, tinymce4 to your forms inputs with per input field settings. Optional integrate the elfinder file manager. You extend it to use with your prefered editors.","# WysiwygBundle

This bundles makes really easy to add a ckeditor4, tinymce4 to your forms with per input field settings. Optional integrate the elfinder file manager. You extend it to use with your prefered editors.

## Security
Add the route you need to secure to the security.yml access-control part:
''' yaml
    access_control:
        - ...
        - { path: ^/_wysiwyg, role: ROLE_IPE_EDITOR }
        - { path: ^/_elfinder, role: ROLE_IPE_EDITOR }
'''

## ElFinder (file manager)

app/console assetic:dump

Pending:
add flash connector

todo
- integrar con ckeditor y tinymce


ckeditor with ajax requests
            (function( $ ) {
                var id = 'idOfTheEditor';

                $( '#load' ).on( 'click', function() {
                    $.ajax({
                        type: 'POST',
                        url: 'index.php',
                        data: {
                            id: id
                        },
                        error: function(){
                            console.log( 'Error?' );
                        },
                        success: function( data ) {
                            if ( window.CKEDITOR && CKEDITOR.instances[ id ] ) {
                                console.log( 'Desytroying an existing instance!' );
                                CKEDITOR.instances[ id ].destroy();
                            }
                            $( '#mydiv' ).html( data );
                        }
                    });
                });

            })( jQuery );

git clone git@github.com:Studio-42/elFinder.git ./vendor/studio-42/elfinder",provide additional WYSIWYG (what you see is what you get editing) functionality to preferred editors,['PHP'],[16964]
jgauffin/SipSharp,Partial implementation of a SIP stack (RFC3261 etc),"This is a SIP stack which I created a long time ago. 


I do currently not actively maintain it, but moved it here per request from a user.


-------------------------


A sip stack under development. It's far from done and is not usable by any means yet.

Feel free to participate.

Plan:

* Create simplest working sip implementation
 * Create transport layer (done)
 * Create fast parser (done)
 * Create transaction layer (done)
 * Create stack main class (done)
 * Create Registrar (done)
 * Create stateful proxy (in progress)
 * Create B2BUA
 * Create PBX
* Refactor all parts.
* Create SDP implementation
 * Create parser
 * Write tests
 * Add support to the sip stack
* Create RTP/RTCP implementation
 * Create parser
 * Write tests
 * Add support to the sip stack
* Refactor everything
* Write tests
* Refactor more
* Create release.

I'm doing this on my spare time. Development will take time.

The most important goal is to keep all parts modular with as little dependencies as possible. Everything should be very modular.

Steps 3 and 4 doesn't depend on the earlier steps. Feel free to do them.

The goal is to have a complete stack which I can use in our voip system.

",implement a session initiation protocl (SIP) stack,['C#'],[633711]
jessy1092/jackpad,Hackpad Java API,"#Overview

[![Build Status](https://travis-ci.org/jessy1092/jackpad.png?branch=master)](https://travis-ci.org/jessy1092/jackpad)

A client library for the Hackpad API (Version 1.0)

Make sure to check out the official [Hackpad API documentation](https://hackpad.com/Hackpad-API-v1.0-k9bpcEeOo2Q).

#Basic Usage

##JackpadClient

``` java
JackpadClient jackpadClient = new JackpadClient(HACKPAD_CLIENT_ID, HACKPAD_SECRET);
jackpadClient.build();
```

##Creat Pad

``` java
Pad pad = new Pad(""text/plain"", ""Create Pad"");
pad.setTitle(""Hello"");
jackpadClient.createPad(pad);
```

##Get Pad Content

``` java
String padText = jackpadClient.getPadContent(PADID, ""latest"", ""html"");
```
or
``` java
String padText = jackpadClient.getPadContentHTML(PADID, ""latest"");
```

##Update Pad Content

``` java
Pad pad = new Pad(""text/plain"", ""Update Pad"");
pad.setTitle(""Hello"");
pad.setPadID(PADID);
boolean test = jackpadClient.updatePadContent(pad);
```

#Maven

##Running Test

`mvn test`

##Installation

`mvn install`

#Reporting issues

GitHub issue tracker: https://github.com/jessy1092/jackpad/issues

#License

The code is available at github project under [MIT License](https://github.com/jessy1092/jackpad/blob/master/LICENSE)

",Hackpad's API,['Java'],[8330]
spring-projects/spring-net-codeconfig,Provides the ability to configure a Spring container using standard .NET code instead of or in addition to XML.   configuration,"The Spring.NET CodeConfig project, Release 2.0.0 (8/31/2012)
--------------------------------------------------------------------
https://github.com/SpringSource/spring-net-codeconfig


1. INTRODUCTION

The 2.0.0 release of Spring.NET CodeConfig provides:

     * the ability to configure a Spring container using standard .NET attributes in addition to XML configuration
     * the ability to configure a Spring container using standard .NET code instead of or in addition to XML configuration

2. SUPPORTED .NET FRAMEWORK VERSIONS

Spring Code Config for .NET version 2.0.0 supports the .NET 3.5 and later framework runtimes (REQUIRES Visual Studio 2008 or later).

3. KNOWN ISSUES/LIMITATIONS

* Features
This relase of Spring Code Config for .NET supports the basic configuration of Object Definitions Metadata related to object construction but does not (yet) support the more advanced features of the Spring.NET Dependency Injection container including the application of Aspects, the subsitution of VariablePlaceholderValues, and others.  This support will be provided in subsequent releases of this project.

4. RELEASE INFO

Release contents:

* ""src"" contains the C# source files for the framework
* ""test"" contains the C# source files for the test suite
* ""bin\net\3.5"" contains the distribution dll files for .NET 3.5
* ""bin\net\4.0"" contains the distribution dll files for .NET 4.0
* ""lib\net"" contains common libraries needed for building the project
* ""doc"" contains reference documentation, MSDN-style API help, and the Spring Code Config for .NET xsd.
* ""examples"" contains sample applications.

The VS.NET solution for the framework and examples are provided.

Latest info is available at the public website: http://www.springframework.net/

Spring Code Config for .NET is released under the terms of the Apache Software License (see license.txt).

5. DISTRIBUTION DLLs

The ""bin"" directory contains the following distinct dll files for use in applications:

* bin\net\3.5\Spring.Core.CodeConfig.dll for .NET 3.5
* bin\net\4.0\Spring.Core.CodeConfig.dll for .NET 4.0


6. WHERE TO START?

Documentation can be found in the ""docs"" directory:
* The Spring.NET CodeConfig reference documentation

Documented sample applications can be found in ""examples"":

7. How to build

VS.NET
------
The is a solution file for different version of VS.NET

* Spring.Config.2008.sln for use with VS.NET 2008
* Spring.Config.2010.sln for use with VS.NET 2010

8. Support

The user forums at http://forum.springframework.net/ are available for you to submit questions, support requests, and interact with other Spring.NET users.

Bug and issue tracking can be found at https://jira.springsource.org/browse/SPRNET/component/11081

To get the sources, check them out at the git repository at https://github.com/SpringSource/spring-net-codeconfig

We are always happy to receive your feedback on the forums. If you think you found a bug, have an improvement suggestion
or feature request, please submit a ticket in JIRA (see link above).

9. Acknowledgements

InnovaSys Document X!
---------------------
InnovSys has kindly provided a license to generate the SDK documentation and supporting utilities for

10. Contributing to Spring.NET CodeConfig

Github is for social coding: if you want to write code, we encourage contributions through pull requests from forks of this repository (see http://help.github.com/forking/). Before we accept a non-trivial patch or pull request we will need you to sign the contributor's agreement (see https://support.springsource.com/spring_committer_signup). Signing the contributor's agreement does not grant anyone commit rights to the main repository, but it does mean that we can accept your contributions, and you will get an author credit if we do. Active contributors might be asked to join the core team, and given the ability to merge pull requests.",enable configuration of the Spring Framework's container,"['XSLT', 'C#', 'CSS', 'ASP', 'Shell']","[2904336, 287494, 9268, 8520, 4613]"
Skobayashi/Weather,引数に地名を与えて明日の天気を取得するコマンドラインツール,"TripleI.Weather  [![Build Status](https://travis-ci.org/Skobayashi/Weather.svg?branch=master)](https://travis-ci.org/Skobayashi/Weather) [![Coverage Status](https://coveralls.io/repos/Skobayashi/Weather/badge.png?branch=master)](https://coveralls.io/r/Skobayashi/Weather?branch=master) 
=======
引数に地名を与えて明日の天気を返すコマンドラインツールです。  
livedoor の天気情報を利用しています。  
下記アドレスに記載のない地名では天気を取得することが出来ません。  
http://weather.livedoor.com/weather_hacks/webservice


動作環境
------------
 * PHP 5.3+

使い方
---------------

### bin/weather を実行します
```
 $ bin/weather 豊橋
 $ -- 豊橋の明日の天気 --
 $ 晴のち曇
 $ 最高気温 16 度
 $ 最高気温 8 度
```",,['PHP'],[4913]
gpac/gpac,GPAC main code repository,"[![Build Status](https://tests.gpac.io/testres/badge/build/ubuntu64)](https://buildbot.gpac.io/#/grid?branch=master)
[![Tests](https://tests.gpac.io/testres/badge/tests/linux64)](https://tests.gpac.io/)

[![Build Status](https://tests.gpac.io/testres/badge/build/ubuntu32)](https://buildbot.gpac.io/#/grid?branch=master)
[![Tests](https://tests.gpac.io/testres/badge/tests/linux32)](https://tests.gpac.io/)

[![Build Status](https://tests.gpac.io/testres/badge/build/windows64)](https://buildbot.gpac.io/#/grid?branch=master)
[![Tests](https://tests.gpac.io/testres/badge/tests/win64)](https://tests.gpac.io/)

[![Build Status](https://tests.gpac.io/testres/badge/build/windows32)](https://buildbot.gpac.io/#/grid?branch=master)
[![Tests](https://tests.gpac.io/testres/badge/tests/win32)](https://tests.gpac.io/)

[![Build Status](https://tests.gpac.io/testres/badge/build/macos)](https://buildbot.gpac.io/#/grid?branch=master)
[![Tests](https://tests.gpac.io/testres/badge/tests/macos)](https://tests.gpac.io/)

[![Build Status](https://tests.gpac.io/testres/badge/build/ios)](https://buildbot.gpac.io/#/grid?branch=master)

[![Build Status](https://tests.gpac.io/testres/badge/build/android)](https://buildbot.gpac.io/#/grid?branch=master)

[![Coverage](https://tests.gpac.io/testres/badge/cov/linux64)](https://tests.gpac.io/)

![License](https://img.shields.io/badge/license-LGPL-blue.svg)

README for GPAC version 0.8.0

GPAC is a multimedia framework oriented towards rich media and distributed under the LGPL license (see COPYING).

GPAC supports many multimedia formats, from simple audiovisual containers (avi, mov, mpg) to complex
presentation formats (MPEG-4 Systems, SVG Tiny 1.2, VRML/X3D) and 360 videos. GPAC supports presentation scripting for MPEG4/VRML/X3D through mozilla SpiderMonkey javascript engine.

GPAC currently supports local playback, http progressive download, Adaptive HTTP Streaming (MPEG-DASH, HLS), RTP/RTSP streaming over UDP (unicast or multicast) or TCP and TS demuxing (from file, IP or DVB4Linux).

GPAC also features MP4Box, a multimedia swiss-army knife for the prompt, and MP42TS, a fast TS multiplexer from MP4 and RTP sources.

For compilation and installation instruction, check INSTALLME file

For GPAC configuration instruction, check gpac/doc/configuration.html or gpac/doc/man/gpac.1 (man gpac when installed)

For more information, visit the GPAC website:
	http://gpac.io

",multimedia framework,"['C', 'C++', 'JavaScript', 'Java', 'Shell', 'Makefile', 'XSLT', 'Objective-C', 'NSIS', 'Rebol', 'GLSL', 'Assembly', 'Batchfile', 'HTML', 'CSS']","[22468761, 4281484, 640571, 192846, 151865, 141231, 34677, 34513, 24436, 13726, 13036, 12073, 8094, 4771, 1796]"
rhq-project/wildfly-cassandra,ARCHIVED - Cassandra integration with Wildfly ,"

# Cassandra Subsystem

## Prerequisites

### Wildfly 8.1.0

Get and install Wildfly 8.1.0: http://download.jboss.org/wildfly/8.1.0.Final/wildfly-8.1.0.Final.zip

It's currently been tested against WF 8.1.0 and the default server configuration (standalone-cassdandra.xml) is configured for WF 8.
But apart from that there should be no reason to not use it on WF 9.

### Cassandra Trunk (> 31.10.2014)
Checkout and build cassandra trunk from the apache repo. This will put it to your local maven repo:

`git clone http://git-wip-us.apache.org/repos/asf/cassandra.git cassandra-trunk`

 `cd cassandra-trunk; ant  mvn-install`

(For further details on how to build cassandra please see http://wiki.apache.org/cassandra/HowToContribute)

## Build & Install

Build the top level project:

`mvn clean install`

This will also create a wildfly-cassandra-overlay.zip, that can be installed on Wildfly:

`unzip target/wildfly-cassandra-overlay.zip -d $WILDFLY_HOME`

This will add an additional module that contains the cassandra extension and subsystem:

`modules/system/layers/base/org/wildfly/extension/cassandra/main/`

### Package Contents

The following contents will be installed when you unpack the wildfly-cassandra-overlay.zip:


 bin/nodetool (1)
 modules/system/layers/base/org/wildfly/extension/cassandra/main/module.xml (2)
 modules/system/layers/base/org/wildfly/extension/cassandra/main/*.jar (3)
 standalone/configuration/standalone-cassandra.xml (4)
 domain/configuration/cassandra-domain.xml (5)
 domain/configuration/cassandra-host.xml (6)


 A patched nodetool that allows full JMX urls (i.e service:jmx:http-remoting-jmx://127.0.0.1:9990)
     The module descriptor
     Required libraries to run cassandra on Wildfly
     An example configuration for standalone servers
     An example configuration for managed domains
     An example host configuration (seed nodes)


## Server Configuration Profiles

The wildfly-cassandra-overlay.zip installs server profiles for both standalone and domain mode that can be used to start a pre-configured Wildfly instance:

### Standalone Mode

`./bin/standalone.sh -c standalone-cassandra.xml -b 127.0.0.1 -Dcassandra.boot_without_jna=true`

(yes, the jna switch matters)

### Domain Mode

`./bin/domain.sh --domain-config=cassandra-domain.xml --host-config=cassandra-host.xml -b 127.0.0.1`

## Cassandra Configuration

The service configuration can be accessed like any other wildfly resource:


`[standalone@localhost:9990 /] /subsystem=cassandra/cluster=WildflyCluster:read-resource
 {
     ""outcome"" => ""success"",
     ""result"" => {
         ""authenticator"" => ""AllowAllAuthenticator"",
         ""authorizer"" => ""AllowAllAuthorizer"",
         ""broadcast-address"" => expression ""${jboss.default.multicast.address:230.0.0.4}"",
         ""client-encryption-enabled"" => false,
         ""commitlog-sync"" => ""periodic"",
         ""commitlog-sync-period-in-ms"" => 10000,
         ""debug"" => false,
         ""endpoint-snitch"" => ""SimpleSnitch"",
         ""hinted-handoff-enabled"" => true,
         ""internode-authenticator"" => ""org.apache.cassandra.auth.AllowAllInternodeAuthenticator"",
         ""listen-address"" => expression ""${jboss.bind.address:127.0.0.1}"",
         ""native-transport-port"" => 9042,
         ""num-tokens"" => 256,
         ""partitioner"" => ""org.apache.cassandra.dht.Murmur3Partitioner"",
         ""request-scheduler"" => ""org.apache.cassandra.scheduler.NoScheduler"",
         ""rpc-port"" => 9160,
         ""seed-provider-class-name"" => ""org.apache.cassandra.locator.SimpleSeedProvider"",
         ""seed-provider-seeds"" => expression ""${jboss.bind.address:127.0.0.1}"",
         ""server-encryption-enabled"" => false,
         ""start-native-transport"" => true,
         ""start-rpc"" => true
     }
 }`


## Issues

If you discover any problems or see room for improvement, feel free to file an issue and we'll discuss it:
https://github.com/rhq-project/wildfly-cassandra/issues

## Get In touch

The best way to reach out and discuss the cassandra subsystem is currently the RHQ mailing list and/or the Chat Room:

- Mailing List: https://lists.fedorahosted.org/mailman/listinfo/rhq-devel
- IRC: irc://freenode.org/#rhq

## License

- http://www.apache.org/licenses/LICENSE-2.0.html

## Resources
- https://docs.jboss.org/author/display/WFLY8/Documentation
- http://www.datastax.com/documentation/cassandra/2.0/cassandra/gettingStartedCassandraIntro.html

",integrate Apache Cassandra with Wildfly,"['Java', 'Shell']","[55043, 261]"
NathanSweet/dnsmadeeasy,Dynamic DNS updater for DnsMadeEasy,"# DnsMadeEasy DNS Update

This is a simple tool that periodically updates your IP when it changes for [DnsMadeEasy](http://www.dnsmadeeasy.com/)'s dynamic DNS. It works on Windows (EXE file provided), Mac, and Linux.

## Download

Download the latest version here:

[Download DnsMadeEasy](https://github.com/EsotericSoftware/dnsmadeeasy/releases)

There is no installation, only an EXE and JAR file. Windows users only need the EXE file and run it in the usual ways. Other OS users only need the JAR file and run it using `java -jar dnsmadeeasy.jar`.

## Setup

When run the first time, the tool creates a `~/.dnsmadeeasy/config.txt` file containing:

```
User: 
Password: 
Record ID: 
Minutes: 30
Last IP: 
Exit: false
```

`User` and `Password` are your DnsMadeEasy credentials. You may optionally configure DnsMadeEasy to have a password per record, so you don't need to use your account password. `Record ID` identifies the record to update. `Minutes` is the number of minutes between IP checks. `Last IP` does not need to be set manually. If `Exit` is true, the program does not stay running. It will check the IP and update if needed, then exit.

After saving the `config.txt` file, the DnsMadeEasy tool must be restarted. On Windows the tool runs in the background and only a single instance is allowed, so use Task Manager to end the task before starting it again.

Configure your OS to start the tool when the OS starts.

## Troubleshooting

The tool creates a `~/.dnsmadeeasy/dnsmadeeasy.log` file that contains timestamps for when the IP has changed and any error messages.

## License

The tool is released as OSS under the [New BSD license](https://github.com/EsotericSoftware/dnsmadeeasy/blob/master/LICENSE).
",periodically updates dynamic DNS IP changes for DNSMadeEasy,['Java'],[5107]
Stibbons/pyyaml,,"PyYAML - The next generation YAML parser and emitter for Python.

To install, type 'python setup.py install'.

By default, the setup.py script checks whether LibYAML is installed
and if so, builds and installs LibYAML bindings.  To skip the check
and force installation of LibYAML bindings, use the option '--with-libyaml':
'python setup.py --with-libyaml install'.  To disable the check and
skip building and installing LibYAML bindings, use '--without-libyaml':
'python setup.py --without-libyaml install'.

When LibYAML bindings are installed, you may use fast LibYAML-based
parser and emitter as follows:

    >>> yaml.load(stream, Loader=yaml.CLoader)
    >>> yaml.dump(data, Dumper=yaml.CDumper)

PyYAML includes a comprehensive test suite.  To run the tests,
type 'python setup.py test'.

For more information, check the PyYAML homepage:
'http://pyyaml.org/wiki/PyYAML'.

For PyYAML tutorial and reference, see:
'http://pyyaml.org/wiki/PyYAMLDocumentation'.

Post your questions and opinions to the YAML-Core mailing list:
'http://lists.sourceforge.net/lists/listinfo/yaml-core'.

Submit bug reports and feature requests to the PyYAML bug tracker:
'http://pyyaml.org/newticket?component=pyyaml'.

PyYAML is written by Kirill Simonov .  It is released
under the MIT license. See the file LICENSE for more details.



============================================================

Imported from http://svn.pyyaml.org/pyyaml/

Cloned by:

    git svn clone --prefix=origin/ --stdlayout http://svn.pyyaml.org/pyyaml/ .

3.11 Imported by downloading the tar file:

    http://pyyaml.org/download/pyyaml/PyYAML-3.11.tar.gz

Package built with

    python setup.py sdist
",,"['C', 'Python']","[981189, 670316]"
emersion/bups,"Simple GUI for Bup, a very efficient backup system.","Bups
====

Simple GUI for [Bup](https://github.com/bup/bup), a very efficient backup system.

![Main window](https://cloud.githubusercontent.com/assets/506932/8412281/2b916e6e-1e89-11e5-9b78-dbb6c55367de.png)

# Purposes

I personaly use it to backup my files to a hard disk drive plugged into my ISP box (it's a Livebox).

Features:
* Multiple directories support
* Backup, with a nice progressbar
* Show current backups in your favorite file manager
* Backups on local filesystem or over Samba, SSH and [Google Drive](https://github.com/astrada/google-drive-ocamlfuse)
* Backup scheduling (using `systemd` or `anacron`)
* Exclude paths/patterns
* Restore backups

Changelog: https://github.com/emersion/bups/releases

# How to use

Requires Python 2, GTK 3 and Bup. Tested on Archlinux and elementary OS (so it should run on Ubuntu and Debian).

Installation:
* If you are running on a Debian-based system, you can install the `.deb` package from here: https://github.com/emersion/bups/releases
* If you are using Archlinux, install it from the AUR: https://aur.archlinux.org/packages/bups/
* For development, you can download the source:
  
  ```shell
  git clone https://github.com/emersion/bups.git
  cd bups
  bin/bups
  ```

When installed, you can start Bups by running `bups`. A launcher will also be added to your desktop's app menu.

# Configuration

You can edit config with the GUI. You can also manually edit `~/.config/bups/config.json`.

# Translating

The project is on Transifex: https://www.transifex.com/emersion/bups/

You can also email me `.po` files if you don't want to register on Transifex. Download the source and run `tools/makepot.sh` to generate `lang/messages.pot`.
",GUI for BUP backup system,"['Python', 'Shell']","[65097, 671]"
aelarabawy/glib,"This is a mirror to the glib library source code, with my own updates. The code follows the same original license of the original code","glib
====

This is a mirror to the glib library source code, with my own updates.
The code follows the same original license of the original code

I first cloned the original project on my machine
$ git clone git://git.gnome.org/glib

Then Created this repo on github

After that I added this repo as a remote for my project
$ git remote add own https://github.com/aelarabawy/glib.git

Then I pushed the project to github, this required to pull from it first to be on the tip of the branch, then performed the push
$ git pull own master
$ git push own master

I will use the original repo to pull any new updates or bug fixes they add
$ git pull origin master

While I will use this repo (on Github) to push my own updates 
$ git push own master

If I find something real useful, I will suggest to add to the original repo
",mirror to Glib low-level system libraries,"['C', 'Python', 'C++', 'Objective-C', 'Perl', 'Shell', 'Makefile', 'D', 'Emacs Lisp']","[20635125, 314882, 113012, 59301, 51602, 29589, 14821, 1133, 41]"
sphaero/uae4all-rpi, UAE4All for Raspberry Pi,,emulator for raspberry Pi single-board computers,"['C', 'C++', 'Assembly', 'Makefile']","[1424903, 1226524, 38304, 3363]"
DragonSpawn/Json2Class,"Generates class objects from Json strings, along with methods to read/write these json strings with the classes","Json2Class
=============
This is a little tool written in Python 2.7 which generates class definitions from json files that are supplied to it.
It also generates functions which can load and save these objects from json strings.

I like to have static typing since it makes it a lot easier to keep track of what is going on and having classes
generated from the same source makes it easier to sync changes between frontend/backend if they use different languages.

If you make changes to the code I would love it if you submit a pull request so I can take in any improvements or fixes.

## Requirements
View [Getting Started](GettingStarted.md) for requirements and more details into how the tool is used

## Example
### Commandline
    Json2Class.py Person.json --cs-out cs/Generated --py-out py/Generated --java-out java/Generated --namespace Generated
### Person.json
    {
      ""name"": ""Incognito"",
      ""age"": 32,
      ""country"": ""Sweden"",
      ""family"": [{
                    ""@name"": ""Person"",
                    ""@skip_generate"": """",
                    ""name"": ""Mom"",
                    ""age"": 57,
                    ""country"": ""Sweden"",
                    ""family"": []
                },
                {
                    ""@name"": ""Person"",
                    ""@skip_generate"": """",
                    ""name"": ""Dad"",
                    ""age"": 65,
                    ""country"": ""Sweden"",
                    ""family"": []
                }]
    }
### Results in
#### [cs/Generated/Person.cs](test/SampleProjects/CsSample/Generated/Person.cs)
#### [py/Generated/person.py](test/SampleProjects/PySample/Generated/person.py)
#### [java/Generated/Person.java](test/SampleProjects/javaSample/src/main/java/Generated/Person.java)
#### See [Getting Started](GettingStarted.md) for more information
",generate class objects from json strings,"['Python', 'C#', 'Java']","[104260, 84311, 37872]"
wangduoxiong/Egg,一个通用的爬虫,"Egg 简介
=====
## [Egg](https://github.com/wangduoxiong/Egg)
>它一个通用高效的爬虫,希望它能够替大家实现一些需求，更希望能为开源做出自己的贡献。目前，还在成长，在我的构想下，它还需要添加很多功能，我会继续完善。有任何疑问以及需求请以与作者交流:630841816@qq.com

- Egg是一个通用，多线程的Java爬虫框架。
- Egg简单小巧，api非常简单，容易上手。
- Egg性能不错，并实现多种请求方式。
- 能够比较快的响应使用者的需求

###速度说明
####实测数据，在20M无线网下（隔了堵墙，所以会不稳定）
#####1. 爬取1000网页，重复爬取十次
- 8线程，耗时平均在15秒左右
- 16线程，平均耗时12秒左右
- 32线程，平均耗时12秒左右

>速度大概在1.7-2.5m/s左右

###功能介绍
    1.可以通过各种方式爬取网页，当然有些并不完善，在目前主要实现get,post，其余会继续开发

    2.可以从网页中提取出主体内容

    3.其余在仍在开发....

###开发说明
作者非常希望能通过自己的努力，可以推动一点开源事业的发展，很希望可以为开源做出自己的一份贡献，更希望能够在一个深入发掘。Egg虽然还有很多不如人意的地方，但是作为在校大学生，会为大家及时解决并更新代码。遇到有需要的需求，可以及时添加功能，和大家广泛交流。作为开发者，在开发出简单易懂，容易使用的软件做着努力，所以，初学者我认为也应该很容易可以从它里面获取自己想要的。大家可以阅读源码，熟悉代码流程，和相关包的使用。

###希望
 ***作为开发者当然希望越来越多的人使用它。并且可以为它提出你宝贵的意见和建议。***

### 协议
遵循[Apache 2.0协议](http://opensource.org/licenses/Apache-2.0)

### 贡献者
向下面提交过代码的同学表示感谢(哈哈，就我啦)

* ***[Andrew](http://www.github.com/wangduoxiong)***

### 邮件交流
王捉熊630841816@qq.com

### 0.3.00版本更新说明
相对于以前的api，这个版本更加易用，功能也愈加完善，配置更加简单

- 与之前的版本添加dataprocesspor包，用来处理Page 中的result 

- 新添model包，系统中数据传输的model
    * BaseModel任何model都可以继承它，它封装了model的共有操作
    * 新添Site，用于配置爬虫爬取的站点信息，有了更加丰富的配置，类似于headers,cookie,等等
- net包修改
    * 添加FactoryMonitor 虚拟类，用来监控factory长生多少request
    * 新添page 用来保存抓取后的数据
    * RequestFactory用来产生request
- 新添parse包，用来解析抓取数据
    * Parser是一个解析器，用来方便用户提取抓取数据中自己需要的东西
    * Handler是一个处理器，用来处理每次抓取之后的操作
    * Selector是一个选择器，用来判断是否是我们所需要的内容
",,"['JavaScript', 'HTML', 'CSS', 'Java', 'PHP', 'Go', 'Python', 'CoffeeScript', 'Shell', 'Makefile', 'ApacheConf']","[3475959, 1217609, 817918, 121072, 49503, 6808, 5596, 4704, 2025, 1420, 36]"
chef/knife-ec2,Chef knife plug-in for AWS EC2,"# Knife EC2
[![Gem Version](https://badge.fury.io/rb/knife-ec2.svg)](https://rubygems.org/gems/knife-ec2)
[![Build status](https://badge.buildkite.com/6dce7349d1291de0f8a8c68a78cb6829067d5e44bec7c4c552.svg?branch=master)](https://buildkite.com/chef-oss/chef-knife-ec2-master-verify)

This is the official Chef Knife plugin for Amazon EC2. This plugin gives knife the ability to create, bootstrap, and manage EC2 instances.
- Documentation: [https://github.com/chef/knife-ec2/blob/master/README.md](https://github.com/chef/knife-ec2/blob/master/README.md)
- Source: [https://github.com/chef/knife-ec2/tree/master](https://github.com/chef/knife-ec2/tree/master)
- Issues: [https://github.com/chef/knife-ec2/issues](https://github.com/chef/knife-ec2/issues)
- Mailing list: [https://discourse.chef.io/](https://discourse.chef.io/)

## Installation

We highly recommend using [Chef Workstation](https://downloads.chef.io/chef-workstation/), which includes knife-ec2 out of the box. If for some reason you can't use Chef Workstation you can manually install the gem.

If you're using bundler, simply add Chef and Knife EC2 to your `Gemfile`:

```ruby
gem 'knife-ec2'
```

If you are not using bundler, you can install the gem manually from Rubygems:

```bash
$ gem install knife-ec2
```

Depending on your system's configuration, you may need to run this command with root privileges.

## Configuration

In order to communicate with the Amazon's EC2 API you will need to pass Knife your AWS Access Key, Secret Access Key, and if using STS your session token. The knife-ec2 plugin supports multiple methods for configuring these credentials including:
  - AWS configuration / credential files (preferred method)
  - knife.rb / config.rb configuration files
  - environmental variables
  - command line arguments

### AWS Configuration / Credential Files

The preferred method of storing credentials for AWS is to use Amazon's own credential and configuration files. The files allow for multiple ""profiles"", each with their own set of credentials. Also since these credentials aren't stored in your knife.rb/config.rb files you don't have to worry about accidentally checking credentials into a git repository. The configs can be created by hand or generated automatically by running `aws configure` if the AWS Command Line Interface (awscli) is installed.


See Amazon's [Configuration and Credentials Files](https://docs.aws.amazon.com/cli/latest/userguide/cli-config-files.html) documentation for additional information on the file format and default locations for Linux/Mac & Windows hosts.

#### Alternative Config Files Location

If you're not storing the files in their default directory you'll need to specify the location in your `knife.rb`/`config.rb` files:

```ruby
knife[:aws_credential_file] = ""/path/to/credentials/file""
knife[:aws_config_file] = ""/path/to/configuration/file""
```
Since the Knife config file is just Ruby you can also avoid hardcoding your home directory, which creates a configuration that can be used for any user:

```ruby
knife[:aws_credential_file] = File.join(ENV['HOME'], ""/.aws/credentials"")
knife[:aws_config_file] = File.join(ENV['HOME'], ""/path/to/configuration/file"")
```

#### Specifying the AWS Profile

If you have multiple profiles in your credentials file you can define which profile to use. The `default` profile will be used if not supplied,

```ruby
knife[:aws_profile] = ""personal""
```

### Config.rb / Knife.rb Configuration

If you prefer to keep all of your configuration in a single location with Chef you can store your Amazon EC2 credentials in Chef's `knife.rb` or `config.rb` files:

```ruby
knife[:aws_access_key_id] = ""Your AWS Access Key ID""
knife[:aws_secret_access_key] = ""Your AWS Secret Access Key""
```

Additionally if using AWS STS:

```ruby
knife[:aws_session_token] = ""Your AWS Session Token""
```

Note: If your `knife.rb` or `config.rb` files will be checked into a source control management system, or are otherwise accessible by others, you may want to use one of the other configuration methods to avoid exposing your credentials.

### Environmental Variables

Knife-ec2 can also read your credentials from shell environmental variables. Export `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_SESSION_TOKEN` variables in your shell then add the following configuration to your `knife.rb` file:

```ruby
knife[:aws_access_key_id] = ENV['AWS_ACCESS_KEY_ID']
knife[:aws_secret_access_key] = ENV['AWS_SECRET_ACCESS_KEY']
```

Additionally if using AWS STS:

```ruby
knife[:aws_session_token] = ENV['AWS_SESSION_TOKEN']
```

### CLI Arguments

You also have the option of passing your AWS API Key/Secret into the individual knife subcommands using the `--aws-access-key-id` and `--aws-secret-access-key` command options

Example of provisioning a new t2.micro Ubuntu 16.04 webserver:

```bash
$ knife ec2 server create -r 'role[webserver]' -I ami-5e8bb23b -f t2.micro --aws-access-key-id 'Your AWS Access Key ID' --aws-secret-access-key ""Your AWS Secret Access Key"" -ssh-key my_key_name --region us-west-2
```

Note: Passing credentials via the command line exposes the credentials in your shell's history and should be avoided unless absolutely necessary.

## Additional config.rb & knife.rb Configuration Options

The following configuration options may be set in your configuration file:
- flavor
- image
- availability_zone
- ssh_key_name
- aws_session_token
- region

## Using Cloud-Based Secret Data

knife-ec2 now includes the ability to retrieve the encrypted data bag secret and validation keys directly from a cloud-based assets store (currently only S3 is supported). To enable this functionality, you must first upload keys to S3 and give them appropriate permissions. The following is a suggested set of IAM permissions required to make this work:

```json
{
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:Get*"",
        ""s3:List*""
      ],
      ""Resource"": [
        ""arn:aws:s3:::example.com/chef/*""
      ]
    }
  ]
}
```

### Supported URL format

- `http` or `https` based: 'http://example.com/chef/my-validator.pem'
- `s3` based:  's3://chef/my-validator.pem'

### Use the following configuration options in `knife.rb` or `config.rb` to set the source URLs:

```ruby
knife[:validation_key_url] = 'http://example.com/chef/my-validator.pem'
knife[:s3_secret] = 'http://example.com/chef/encrypted_data_bag_secret'
```

### Alternatively, URLs can be passed directly on the command line:

- Validation Key: `--validation-key-url s3://chef/my-validator.pem`
- Encrypted Data Bag Secret: `--s3-secret s3://chef/encrypted_data_bag_secret`

## knife-ec2 Subcommands

This plugin provides the following Knife subcommands. Specific command options can be found by invoking the subcommand with a `--help` flag

### `knife ec2 server create`

Provisions a new server in the Amazon EC2 and then perform a Chef bootstrap (using the SSH or WinRM protocols). The goal of the bootstrap is to get Chef installed on the target system so it can run Chef Client with a Chef Server. The main assumption is a baseline OS installation exists (provided by the provisioning). It is primarily intended for Chef Client systems that talk to a Chef server.  The examples below create Linux and Windows instances:

```bash
# Create some instances -- knife configuration contains the AWS credentials

# A Linux instance via ssh
knife ec2 server create -I ami-d0f89fb9 --ssh-key your-public-key-id -f m1.medium --ssh-user ubuntu --identity-file ~/.ssh/your-private-key

# A Windows instance via the WinRM protocol -- --ssh-key is still required due to EC2 API operations that need it to grant access to the Windows instance
# `--spot-price` option lets you specify the spot pricing
knife ec2 server create -I ami-173d747e -G windows -f m1.medium --user-data ~/your-user-data-file -x '.\a_local_user' -P 'yourpassword' --ssh-key your-public-key-id --spot-price price-in-USD

# Pass --server-connect-attribute to specify the instance attribute that we will try to connect to via ssh/winrm
# Possible values of --server-connect-attribute: private_dns_name, private_ip_address, dns_name, public_ip_address
# If --server-connect-attribute is not specified, knife attempts to determine if connecting to the instance's public or private IP is most appropriate based on other settings
knife ec2 server create -I ami-173d747e -x ubuntu --server-connect-attribute public_ip_address
```

View additional information on configuring Windows images for bootstrap in the documentation for [knife-windows](https://docs.chef.io/plugin_knife_windows.html).

#### Adding server_id to the node name

Users can also include the ec2 server id in the node name by placing `%s` in the string passed to the `--chef-node-name` option. The %s is replaced by the ec2 server id dynamically.
e.g. `-N ""www-server-%s"" or  --chef-node-name ""www-server-%s""`

#### Tagging node in Chef

Users can use option `--tag-node-in-chef` for tagging node in EC2 and chef both with `knife ec2 server create` command. If user does not pass this option, then the node will be tagged only in EC2.

#### Tagging EBS Volumes

Users can attach ebs volumes to a new instance being created with knife-ec2 using `--volume-tags Tag=Value[,Tag=Value...]`.


#### Bootstrap Windows (2012 R2 and above platform) instance without user-data through winrm ssl transport

Users can bootstrap the Windows instance without the need to provide the user-data. `knife-ec2` has the ability to bootstrap the Windows instance through `winrm protocol` using the `ssl` transport. This requires users to set `--winrm-ssl` option and `--winrm-no-verify-cert`. This will do the necessary winrm ssl transport configurations on the target node and the bootstrap will just work.

***Note***: Users also need to pass the `--security-group-ids` option with IDs of the security group(s) having the required ports opened like `5986` for winrm ssl transport. In case if `--security-group-ids` option is not passed then make sure that the default security group in your account has the required ports opened.

Below is the sample command to create a Windows instance and bootstrap it through `ssl` transport without passing any user-data:

```
knife ec2 server create -N chef-node-name -I your-windows-image -f flavor-of-server -x '.\a_local_user' -P 'yourpassword' --ssh-key your-public-key-id --winrm-ssl --winrm-no-verify-cert --security-group-ids your-security-groups -VV
```

#### Bootstrap Windows (2012 R2 and above platform) instance with user-data through winrm with negotiate transport

Users can bootstrap the Windows instance with the user-data. `knife-ec2` has the ability to bootstrap the Windows instance through `winrm protocol` using the `negotiate` transport. This requires users to set `--winrm-auth-method` option as `negotiate` and `--connection-protocol` option as `winrm` and `--user-data` file. USER DATA file contains winrm configurations which needs to be set for successful winrm communication. This will do the necessary winrm configurations on the target node and the bootstrap will just work.

***Note***: Users also need to pass the `--security-group-ids` option with IDs of the security group(s) having the required ports opened like `5985` for winrm with negotiate transport. In case if `--security-group-ids` option is not passed then make sure that the default security group in your account has the required ports opened.

Below is the sample command to create a Windows instance and bootstrap it through `negotiate` transport with passing user-data:

```
knife ec2 server create -N chef-node-name -I your-windows-image -f flavor-of-server -U '.\a_local_user' -P 'yourpassword' --ssh-key your-public-key-id --connection-protocol winrm --winrm-auth-method negotiate --user-data '\path\to\user-data-file' --security-group-ids your-security-groups -VV
```
Below is the content of user data which is required to set winrm configurations and important ports to get open for successful winrm communication to node.

```

# Allow script execution
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Force
# PS Remoting and & winrm.cmd basic config
Enable-PSRemoting -Force -SkipNetworkProfileCheck
winrm quickconfig -q
$user = ""username""
$password = ""password""
net user /add $user $password
net localgroup administrators $user /add
winrm create winrm/config/Listener?Address=*+Transport=HTTP
# winrm set winrm/config/winrs '@{MaxMemoryPerShellMB=""300""}'
winrm set winrm/config/winrs '@{MaxMemoryPerShellMB=""1024""}'
winrm set winrm/config/winrs '@{MaxShellsPerUser=""50""}'
winrm set winrm/config '@{MaxTimeoutms=""1800000""}'
winrm set winrm/config/service '@{AllowUnencrypted=""true""}'
winrm set winrm/config/service/auth '@{Basic=""true""}'
netsh advfirewall firewall add rule name=""WinRM 5985"" protocol=TCP dir=in localport=5985 action=allow
netsh advfirewall firewall add rule name=""WinRM 5986"" protocol=TCP dir=in localport=5986 action=allow
NetSh Advfirewall set allprofiles state off
net stop winrm
sc.exe config winrm start=auto
net start winrm

```
#### Options for bootstrapping Windows

The `knife ec2 server create` command also supports the following options for bootstrapping a Windows node after the VM is created:

```
:connection_password           The WinRM password
:winrm_auth_method             Defaults to negotiate, supports kerberos, can be set to basic for debugging
:winrm_ssl                     SSL in the WinRM connection
:connection_port               Defaults to 5985 plaintext transport, or 5986 for SSL
:ca_trust_file                 The CA certificate file to use to verify the server when using SSL
:winrm_no_verify_cert          When flag is present, SSL cert will not be verified. Same as original mode of 'verify_none'
:kerberos_keytab_file          The Kerberos keytab file used for authentication
:kerberos_realm                The Kerberos realm used for authentication
:kerberos_service              The Kerberos service used for authentication
```
### `knife ec2 ami list`

This command provides the feature to list all EC2 AMIs. It also provides the feature to filter the AMIs based on owner and platform.

```
knife ec2 ami list
```

#### Options for AMIs list

- **Owner:**
  By default owner is aws-marketplace but you can specify following owner with the help of -o or --owner:

  **command:** knife ec2 ami list -o (options)

  ```
  :self                         Displays the list of AMIs created by the user.
  :aws-marketplace              Displays all AMIs form trusted vendors like Ubuntu, Microsoft, SAP, Zend as well as many open source offering.
  :micosoft                     Displays only Microsoft vendor AMIs.
  ```
- **Platform:**
  By default all platform AMIs are displayed, but you can filter your response by specifying the platform using -p or --platform:

  **command:** knife ec2 ami list -p (options)

  ```
  :Allowed platform             windows, ubuntu, debian, centos, fedora, rhel, nginx, turnkey, jumpbox, coreos, cisco, amazon, nessus
  ```
- **Search:**
  User can search any string into the description column by using -s or --search:

  **command:** knife ec2 ami list -s (search_keyword)

  ```
  :search_keyword             Any String or number
  ```

### `knife ec2 server list`

Outputs a list of all servers in the currently configured AWS account. **Note, this shows all instances associated with the account, some of which may not be currently managed by the Chef server.**

### `knife ec2 server delete`

Deletes an existing server in the currently configured AWS account. **By default, this does not delete the associated node and client objects from the Chef server. To do so, add the `--purge` flag**

## Development Documentation

All documentation is written using YARD. You can generate a by running:

```
rake docs
```

## Contributing

For information on contributing to this project please see our [Contributing Documentation](https://github.com/chef/chef/blob/master/CONTRIBUTING.md)

## License & Copyright

- Copyright:: Copyright (c) 2009-2019 Chef Software, Inc.
- License:: Apache License, Version 2.0

```text
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```
",manage EC2 instances on Chef workstation,"['Ruby', 'Shell']","[250279, 451]"
crbanman/AstroidEscape,An arcade space survival game.,"Now Deprecated. Game has been redone in Unity3D.

AstroidEscape - Android
=============

An arcade space survival game.


TODO
====
- [x] Finish implementing ads
- [x] Implement Google Analytics
- [x] Implement Google Leaderboards 
- [x] Implement Facebook install analytics
- [x] Implement Facebook score posting
- [x] Resize buttons to be all the same size
- [ ] Implement Twitter score posting
- [ ] Fix collision boxes on LargeAsteroid
- [ ] Implement Google Acheivments

Ideas
=====
- Option to change laser color
- Unlock new ships with different weapons
- add an indestructable object
",arcade space survival game for Android,"['Java', 'CSS', 'JavaScript']","[71271, 3341, 24]"
android-ia/platform_external_libsepol,,,,,
Serneum/jousting-core,A Python implementation of the To Cry a Joust rule set.,"[![Build Status](https://travis-ci.org/Serneum/jousting-core.svg?branch=master)](https://travis-ci.org/Serneum/jousting-core) [![Coverage Status](https://coveralls.io/repos/Serneum/jousting-core/badge.svg?branch=master)](https://coveralls.io/r/Serneum/jousting-core?branch=master)

A Python implementation of the [To Cry a Joust](http://boardgamegeek.com/boardgame/124129/cry-joust) rule set.


### Installing jousting-core
`pip install https://github.com/Serneum/jousting-core/releases/download/v1.0/jousting-core.tar.gz`
",implmentation of the To Cry a Joust Board game rule set,['Python'],[33635]
davidkempers/django-tasks,Automatically exported from code.google.com/p/django-tasks,"# django-tasks
Automatically exported from code.google.com/p/django-tasks
",automatic exportation,['Python'],[87815]
Taapeli/ProtoLoader,Prototype for gedcom file loader,"Taapelin prototyyppi
====================

Prototyyppi sisältää nyt seuraavat ohjelmat:

* gedLoader:		Gedcom-tiedoston lukeminen kantaan
* gedLoaderWithLabel:	Gedcom-tiedoston lukeminen kantaan käyttäjä-label'illa

* listToDoData		Tulostus tarkistusajossa havaituista korjausta vaativista seikoista

* addBirthRepo		Kytketään henkilölle syntymätapahtuman repo-tiedot tietokannasta
* addRepoData		Syötetään henkilölle syntymätapahtuman repo-tiedot
* chooseRepo		Repon ja repon sourcen valinta
* disconnectRepo	Poistetaan haluttu repo-tieto
* updateRepoData	Tulostus henkilön repo-datasta

* changeBirthData: 	Henkilön syntymäajan muutos
* listBirths:		Tulostus syntymäajan perusteella poimittuna
* listNotSetBirthdays: 	Tulostus henkilöistä, joilla ei ole annettu syntymäaikaa
* readBirths:		Haku syntymäajalla
* updateBirthData: 	Tulostus henkilöstä, jolle halutaan muuttaa syntymäaikaa

* addHiskiLink: 	Hiski-linkin lisäys henkilölle
* deleteHiskiLink: 	Hiski-linkin poisto henkilöltä
* listNoHiskiLinks: 	Tulostus henkilöistä, joilla ei ole annettu Hiski-linkkiä

* readNames:		Haku nimellä
* listNames:		Tulostus nimen mukaan poimittuna

* readIndividData:	Tulosta henkilön tiedot ja hänen esi- ja jälkipolvet
",GEDCOM file loader,"['PHP', 'HTML', 'CSS']","[378877, 23114, 2247]"
vzvu3k6k/mcg_source_list,Markov Chain GeneratorのソースをTwitterから収集するWebサービス,"# MCG Source List

[Markov Chain Generator](http://mcg.herokuapp.com/)のソースをTwitterから収集するサイトです。

- [http://mcg-source-list.herokuapp.com/](http://mcg-source-list.herokuapp.com/)
- [https://github.com/vzvu3k6k/mcg_source_list/](https://github.com/vzvu3k6k/mcg_source_list/)

## インストール

以下の手順を実行すればHerokuで動かせます。

1. [https://apps.twitter.com/](https://apps.twitter.com/)でTwitterのApplicationを登録する。
1. [https://heroku.com/deploy?template=https://github.com/vzvu3k6k/mcg_source_list/tree/master](https://heroku.com/deploy?template=https://github.com/vzvu3k6k/mcg_source_list/tree/master)を開き、先ほど登録したApplicationの各種キーを設定してデプロイする。
1. Schedulerに`bundle exec ruby scripts/crawl.rb`を追加する。

## ライセンス

[Creative Commons — CC0 1.0 Universal](http://creativecommons.org/publicdomain/zero/1.0/)
",markov chain generator,"['Ruby', 'JavaScript', 'CSS']","[4214, 2035, 777]"
mthli/Tweetin,Yet another Twitter unofficial client for Lollipop.,"Tweetin
=======

[![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-Tweetin-brightgreen.svg?style=flat)](https://android-arsenal.com/details/3/1154)

Yet another Twitter unofficial client.

Just design for __Lollipop__ now!!!

## Screenshot:

![All1.png](/Art/All1.png ""All1.png"")

![All2.png](/Art/All2.png ""All2.png"")

## How to use the source code?

Just import the `Tweetin` folder with your __IntelliJ IDEA__.

## Thanks:

 - [android-ago](https://github.com/curioustechizen/android-ago ""android-ago"")

 - [Android-ProgressFragment](https://github.com/johnkil/Android-ProgressFragment ""Android-ProgressFragment"")

 - [AndroidSlidingUpPanel](https://github.com/umano/AndroidSlidingUpPanel ""AndroidSlidingUpPanel"")

 - [android-volley](https://github.com/mcxiaoke/android-volley ""android-volley"")

 - [colorpicker](https://github.com/flavienlaurent/colorpicker ""colorpicker"")

 - [glide](https://github.com/bumptech/glide ""glide"")

 - [materialish-progress](https://github.com/pnikosis/materialish-progress ""materialish-progress"")

 - [NineOldAndroids](https://github.com/JakeWharton/NineOldAndroids ""NineOldAndroids"")

 - [snakeyaml](http://code.google.com/p/snakeyaml/ ""snakeyaml"")

 - [twitter4j](https://github.com/yusuke/twitter4j ""twitter4j"")

 - [twitter-text](https://github.com/twitter/twitter-text ""twitter-text"")

 - Some others :)

## License:
_[Apache License, Version 2.0](https://github.com/mthli/Tweetin/blob/master/LICENSE ""Apache License, Version 2.0"")_
",unofficial Twitter client,"['Java', 'CSS']","[484005, 12409]"
uProxy/obfuscation,,"uTransformers
=============

[![Build Status](https://travis-ci.org/uProxy/uTransformers.svg?branch=master)](https://travis-ci.org/uProxy/uTransformers) [![devDependency Status](https://david-dm.org/uProxy/uTransformers/dev-status.svg)](https://david-dm.org/uProxy/uTransformers#info=devDependencies)

uTransformers is a transport-layer obfuscation library for uProxy.

This library currently builds two uTransformers modules:

* **uTransformers.rabbit**: based on http://en.wikipedia.org/wiki/Rabbit_(cipher)
* **uTransformers.fte**: based on https://github.com/uproxy/libfte

See ""Example Usage"" below for more details.

Installation
------------

```bash
npm install uTransformers
```

Example Usage
-------------

### FTE

```javascript
var fte = require('uTransformers/src/transformers/uTransformers.fte.js');
var regex2dfa = require('regex2dfa/regex2dfa.js');

var transformer = new fte.Transformer();

var key = ""FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF"";
var ab_key = str2ab(key);
transformer.setKey(ab_key);

var json_obj = {
  'plaintext_dfa': regex2dfa.regex2dfa(""^.+$""),
  'plaintext_max_len': 128,
  'ciphertext_dfa': regex2dfa.regex2dfa(""^.+$""),
  'ciphertext_max_len': 128
};
        
var json_str = JSON.stringify(json_obj);
transformer.configure(json_str);

var input_plaintext = ""Hello, World!"";
var ab_plaintext = str2ab(input_plaintext);
var ciphertext = transformer.transform(ab_plaintext);
var ab_output_plaintext = transformer.restore(ciphertext);
var output_plaintext = ab2str(ab_output_plaintext);
```

### Rabbit

```javascript
var rabbit = require('uTransformers/src/transformers/uTransformers.rabbit.js');
var transformer = new rabbit.Transformer();

var key = ""FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF"";
var ab_key = str2ab(key);
transformer.setKey(ab_key);

var input_plaintext = ""Hello, World!"";
var ab_plaintext = str2ab(input_plaintext);
var ciphertext = transformer.transform(ab_plaintext);
var ab_output_plaintext = transformer.restore(ciphertext);
var output_plaintext = ab2str(ab_output_plaintext);
```

Building
--------

There are two stages to the build process:

* The first stage is the process of building ```uTransformers.fte.js``` and ```uTransformers.rabbit.js``` using emscripten. This is done in a vagrant virutal machine. See ```vagrant/README.md``` for details. To ease the build process, these artifacts are checked into git when the their cc files change.
* The second step of the build process runs jasmine tests with grunt. This produces artifacts in ```build/```. 
",Transport layer obfuscation library,"['C++', 'JavaScript', 'Shell', 'Makefile', 'Python', 'M4', 'Ruby']","[27890, 13843, 3885, 1975, 1004, 566, 493]"
hadleyrich/GerbLook,"A web based gerber renderer based on Python, gerbv and imagemagick held together with a little glue and string.","GerbLook
=======
Copyright 2013 Hadley Rich, nice technology Ltd.  
Website: 

A web based gerber renderer based on Python, gerbv and imagemagick held together with a little glue and string.

Running your own instance
-------------------------

- Install [Redis](http://redis.io/) and run redis-server (default port is 6379 which will work)
- Install `libpq-dev`,`imagemagick` and `gerbv`
- `pip install -r requirements.txt`
- `./renderer.py &`
- `./manage.py` and direct your browser to `0.0.0.0:5000`

LICENSE
-------
BSD - See LICENSE file
",formatting 2D binary images,"['Python', 'HTML', 'JavaScript', 'CSS']","[53358, 19627, 12421, 238]"
usgs/icoast,,"# iCoast Â©USGS 2013

**Lead Scientist:** Karen Morgan - 
[kmorgan@usgs.gov](mailto:kmorgan@usgs.gov) - (727) 502-8037  
**Application Developer:** Richard Snell - 
[rsnell@usgs.gov](mailto:rsnell@usgs.gov) - (727) 502-8025

iCoast is a PHP/HTML/JS/CSS/MySQL based web application designed to 
harness the power of ""crowdsourcing"" to compare coastal imagery from 
both before and after extreme weather events. Questions are asked about 
visible changes and answers are recorded using admin configurable tags. 
The data is then used to help ground truth and improve existing 
predictive change models.

iCoast is live and can be found at: 
[https://coastal.er.usgs.gov/icoast/](https://coastal.er.usgs.gov/icoast/)

## Frameworks, Plug-ins, & Libraries Used
* [GeoNames](geonames.org)  
  Location names derived from GPS co-ordinates using the GeoNames 
  Gazetteer.
  * License(s): [Creative Commons Attribution 3.0]
  (http://creativecommons.org/licenses/by/3.0/)
* [JQuery](jquery.com)  
  To ease cross platform development and allow the use of other 
  dependent plug-ins.
* [TipTip](https://github.com/drewwilson/TipTip)  
  Provides css formatted and screen responsive tool tips.
  * License(s): [MIT](http://opensource.org/licenses/mit-license.php), 
    [GPL](http://www.gnu.org/licenses/gpl.html)
  * Dependent Upon: [JQuery](jquery.com)
* [Leaflet](leafletjs.com)  
  Provides the interactive map.
* [Geo Search](https://github.com/smeijer/L.GeoSearch)  
  Map based location searching.
  * License(s): [MIT]
  (https://github.com/smeijer/L.GeoSearch/blob/master/LICENSE)
* [MarkerCluster](https://github.com/Leaflet/Leaflet.markercluster)  
  Groups map markers together dynamically.
  * License(s): [MIT](http://opensource.org/licenses/mit-license.php)
  * Dependent Upon: [Leaflet](leafletjs.com)
* [Form Validation](http://jqueryvalidation.org/)
  * License(s): [MIT](http://opensource.org/licenses/mit-license.php)
  * Dependent Upon: [JQuery](jquery.com)

## Attributions
  
Page elements, graphics, icons, and images without attribution on the 
page or above are either the property of USGS or were freely available 
for private or commercial use without restrictions or the need for 
attribution.
",compare crowdsourced coastal imagery from extreme weather events,"['PHP', 'JavaScript', 'CSS', 'HTML']","[2727536, 86191, 75049, 186]"
structured-commons/tools,"SC-related reference tools, libraries and utilities","Example Structured Commons utilities
====================================

This repository provides an example implementation of the `Structured
Commons`__ [#]_ [#]_ [#]_, an alternative publication and dissemination
model for scientific works.

This directory contains a **Python library** ``sc`` for manipulating
Structured Commons objects and fingerprints; and two front-end
**command-line utilities**:

**objtool.py**
   Convert between object representations and compute object fingerprints.

**fptool.py**
   Convert between fingerprint representations and compare fingerprints.

Installation
------------

Requirements: Python 2 or 3 (the code was tested with Python 2.7 and 3.3)

The utilities can be run directly from the source directory without
""installing"" them elsewhere.

For a separate installation::

     python setup.py build
     python setup.py install

This installs to the default ``site-packages`` directory for that
version of the Python interpreter. To change the target directory, add
the argument ``--prefix=DIR`` after ``install``.

Usage: objtool.py
-----------------

Examples
````````

To compute the fingerprint of the single file named ""``1404.7753v2.pdf``""::

     $ python objtool.py fs:1404.7753v2.pdf
     fp:FvYPWVbnhezNY5vdtqyyef0wpvj149A7SquozxdVe3jigg

To compute the ""long"" fingerprint of an entire Python source tree
starting at directory ""``sc``"", excluding compiled bytecode objects,
with verbose reporting::

     $ python objtool.py -i '*.pyc' -i __pycache__ -v fs:sc fp:long
     dictionary, entering:
     entry u'__init__.py': file, sz 33 (fp:nquSc-41kbl6K2QfhiYQxJZFgKO4YPpeS6iz3SmlY1Dkhw)
     entry u'fp.py': file, sz 13731 (fp:Uxs7Oczd4boiEoqCdFDgCKnBXDo3K4h2rY5wu9LnDLdjSw)
     entry u'fs.py': file, sz 3857 (fp:uIuqi_hOvEhd9in2LPcNrVXubrRcv13dR52FteK9fJSqqA)
     entry u'pyrepr.py': file, sz 2593 (fp:xivrx77SyVJyWvVTwm2wialKuRprZB47uuRSgn6WGoPrxg)
     leaving dictionary (fp:C49RMXE36qDzdc9r61JiwfCl9_KCOdVlrgQ-sy9DiKkaAw)
     fp::BOHV-CMLR-G7VK-B43V-Z5V6-WUTC-YHYK-L57S-QI45-KZNO-AQ7L-GL2D-RCUR-UAY

To convert an object from its filesystem representation to a representation
as Python dictionary tree::

     $ python objtool.py -i '*.pyc' -i __pycache__ -v fs:sc py:-
     {u'fs.py': u'#! /usr/ .... '}


Command-line syntax
```````````````````

The general syntax for ``objtool.py`` is the following::

     objtool.py [OPTIONS] [SOURCE] [DESTINATION]

Where ``SOURCE`` is any of the following:

``fs:PATH``
   Filesystem representation starting from ``PATH``.

``raw:FILE`` or ``raw:-``
   Read a single file object as byte stream from ``FILE`` or stdin.

``utf8:FILE`` or ``utf8:-``
   Read a single file object as an UTF-8 encoded byte stream from ``FILE`` or stdin.

``pickle:FILE`` or ``pickle:-``
   Read a pickled Python object from ``FILE`` or stdin.

``json:FILE`` or ``json:-``
   JSON syntax read as associative arrays / strings / numbers from ``FILE`` or stdin.

and ``DESTINATION`` is any of the following:

``fp:FORMAT``
   Compute and print the input object's fingerprint using ``FORMAT``. See
   the description of ``fptool.py`` below for possible formats.

``fs:PATH``
   Write the filesystem representation starting from ``PATH`` (which must not exist yet).

``json:FILE`` or ``json:-``
   Emit the JSON syntax as associative arrays / strings to ``FILE`` or stdout.

``raw:FILE`` or ``raw:-``
   Write a single file object as byte stream to ``FILE`` or stdout.

``utf8:FILE`` or ``utf8:-``
   Write a single file object as UTF-8 encoded byte stream to ``FILE`` or stdout.

``py:FILE`` or ``py:-``
   Write an quivalent Python syntax  to  ``FILE`` or stdout.


The defaults for ``SOURCE`` and ``DESTINATION`` are ``raw:-`` and ``fp:compact``, respectively.

Command-line options:

``-h``
   Print a command-line help and exit.

``-v``
   Explore recursive structures verbosely.

``-i PAT``
   Ignore filesystem names matching the pattern ``PAT`` (fnmatch syntax).

``-a``
   Also include filesystem names starting with a dot (by default, they are ignored).

``-b``
   Use Base64 encoding when outputting JSON.

Usage: fptool.py
----------------

Examples
````````

To convert a fingerprint to long format (eg. for easier communication over the phone)::

     $ python fptool.py -f long fp:FvYPWVbnhezNY5vdtqyyef0wpvj149A7SquozxdVe3jigg
     fp::C33A-6WKW-46C6-ZTLD-TPO3-NLFS-PH6T-BJXY-6XR5-AO2K-VOUM-6F2V-PN4O-FAQ

To show all possible representations of a fingerprint::

     $ python fptool.py -a fp:FvYPWVbnhezNY5vdtqyyef0wpvj149A7SquozxdVe3jigg
     Argument: 'fp:FvYPWVbnhezNY5vdtqyyef0wpvj149A7SquozxdVe3jigg' (compact)
       compact: fp:FvYPWVbnhezNY5vdtqyyef0wpvj149A7SquozxdVe3jigg
       long:    fp::C33A-6WKW-46C6-ZTLD-TPO3-NLFS-PH6T-BJXY-6XR5-AO2K-VOUM-6F2V-PN4O-FAQ
       hex:     16f60f59-56e785ec-cd639bdd-b6acb279-fd30a6f8-f5e3d03b-4aaba8cf-17557b78
       dec:     10385632981549898505027615664606801012501301866546186765965067533389527350136

Recognized options:

``-h``
   Print a help text and exit.

``-a``
   Print all representations of a fingerprint.

``-f FMT``
   Print a particular representation.

Recognized formats:

======= ================================= ========================================
Name    Format / Encoding                 Target use
======= ================================= ========================================
binary  32 bytes (256 bits), no encoding  Binary storage, network protocols
compact 46 characters, Base64 + checksum  Print and hypertext media
long    55 characters, Base32 + checksum  Mouth-to-ear, analog phone/radio
hex     64 characters, hexadecimal        Databases w/o proper support for binary
dec     1-78 decimal digits               Academic / teaching
carray  C char array definition           Academic / teaching
======= ================================= ========================================


References
----------

.. __: http://www.structured-commons.org/

.. [#] `Academia 2.0: removing the publisher middle-man while retaining
   impact`__. Poss, R.; Altmeyer, S.; Thompson, M.; and Jelier, R.  In
   Proc 1st ACM SIGPLAN Workshop on Reproducible Research
   Methodologies and New Publication Models in Computer Engineering
   (TRUST'14), Edinburgh, UK, June 2014. ACM

.. [#] http://arxiv.org/abs/1404.7753

.. [#] http://science.raphael.poss.name/aca2-draft-spec.html

.. __: http://www.bibbase.org/network/publication/poss-altmeyer-thompson-jelier-academia20removingthepublishermiddlemanwhileretainingimpact-2014
",example implementation of structured commons,"['Python', 'Shell']","[34405, 3234]"
ntuosproj/fastalg-nfqueue,,,,"['C', 'Shell']","[59611, 8162]"
OCA/banking-addons,Odoo Electronic Payment,"[![Runbot Status](https://runbot.odoo-community.org/runbot/badge/flat/173/12.0.svg)](https://runbot.odoo-community.org/runbot/repo/github-com-oca-bank-payment-173)
[![Build Status](https://travis-ci.org/OCA/bank-payment.svg?branch=12.0)](https://travis-ci.org/OCA/bank-payment)
[![Coverage Status](https://coveralls.io/repos/OCA/bank-payment/badge.png?branch=12.0)](https://coveralls.io/r/OCA/bank-payment?branch=12.0)

OCA banking payment addons for Odoo
===================================

On version 12.0, this project focus on payment interface. The indentation below 
indicates the dependency graph of the main modules.

-  `account_payment_order` - Basic export functionality of payment orders

    - `account_banking_sepa_credit_transfer` - Export of payment orders in SEPA format

    - `account_direct_debit` - Debit order infrastructure analogous to Odoo native payment orders

        - `account_banking_sepa_direct_debit` - Export of debit orders in SEPA format
        
Other features can now be found in these repositories:

 * https://github.com/OCA/bank-statement-import
 * https://github.com/OCA/bank-statement-reconcile


Contributing
------------
Do you want to contribute? Please read our [contributing guidelines](https://github.com/OCA/maintainer-tools/blob/master/CONTRIBUTING.md).

----

OCA, or the Odoo Community Association, is a nonprofit organization whose 
mission is to support the collaborative development of Odoo features and 
promote its widespread use.

http://odoo-community.org/
",addons and interface for Odoo's electronic payment component,"['Python', 'HTML', 'CSS']","[306174, 152264, 564]"
ldrumm/good-talk,"simple chat app with AES encryption in browser zmq/gevent message transport, and stateless (anonymous) usage","good-talk
=========

An experimental chatroom server with all communication AES-encrypted in the browser via [crypto-js](https://crypto-js.googlecode.com).
The idea is to allow private and free association on the internet without the website operator being put in the regrettable position where they have to give up any useful information (i.e. content) from your conversations should they be 'compelled' to do so.
It requires no session-cookies or other persistent state, allowing greater scalability and simpler assertions about how your conversations may be recorded by the operator.
Server-side, it utilizes zero-mq in a very simple django application for realtime, database-free message delivery.

For a demo of the current master, see https://rtps.co.

To get started just enter your shared-secret (get this to your friends over the sneakernet), name your chatroom, and provide a personal alias.

What's encrypted?
================
Both your alias and your message are encrypted in-browser before your message is sent.

What's not encrypted?
=====================
The name of your chatroom.  Don't call this something that may identify you (such as ""OMGALICEANDBOBARETOtallYGONNATAKEOVERTHEWORLD"").

Is this actually secure?
========================
**TL;DR:** It's an experiment in the early stages, I'm not a cryptographer, and doing this in JavaScript opens up many attack vectors - so probably not.

Performing encryption in a language like JavaScript - while technically feasible - has many pitfalls that allow your key/plaintext to be leaked through all sorts of side channels. It is impossible to say how your particular browser/OS combination will store your secrets at runtime or whether it will be suitably amnesiac when you close the browser window.  For example, if your system is low on memory, some things (including your passphrase) may be paged to disk.
JavaScript provides no native mechanism to prevent this.  In C, on a POSIX system, we could call `mlock(2)` after we `malloc(3)` the string, and then use something like FreeBSD's `explicit_bzero(3)` when we're done, but this is impossible with the current implementation.
Therefore, you *can* do encryption with Javascript, but before you do you should take note of the possible attack surfaces.
As such if you want to ensure real privacy, you should use something more trustworthy such as GPG.

All that said, this application provides a ""better than nothing"" solution for proper OTR web chat, so you may find it useful.
",experimental chatroom with in browser AES-encryption,"['Python', 'JavaScript']","[11092, 6327]"
tanel/bugsnag-qt,Bugsnag client for QT projects,"bugsnag-qt
==========

Bugsnag client for QT projects

[![Build Status](https://travis-ci.org/tanel/bugsnag-qt.svg?branch=master)](https://travis-ci.org/tanel/bugsnag-qt)
",provide error monitoring and reporting for Qt projects ,"['C++', 'IDL', 'C']","[8732, 501, 293]"
gabepolk/double-dog,Simple POS system using Rails,"# Double Dog PoS

A Point of Sales system built for Double Dog Hot Dogs.

----
## User Stories

- As a manager, I want the first account created to be my admin account
- As a manager, I want to sign in
- As a manager, I want to create items for order
- As a manager, I want to see all orders
- As a manager, I want to see all orders created by a given employee.
- As a manager, I want to create employee accounts
- As an employee, I want to sign into Double Dog PoS
- As an employee, I want to create orders
  - Each order will have at least one item
",point of sales (POS) system for double dog hot dogs,"['Ruby', 'CSS', 'JavaScript', 'CoffeeScript']","[55883, 859, 664, 211]"
bryanjswift/simplenote-android,Android client for SimpleNote,"# This project is ancient - don't use it

This is open sourcing of code for a Simplenote client built and released years ago.
It will no longer function against the current Simplenote API. In fact, it is unlikely
to even build against current Android SDKs.
",android client for the note-taking application SimpleNote,['Java'],[252843]
russellsimpkins-nyt/varnish-mmdb-vmod,A fork. Provides GeoIP lookup for varnish using libmaxminddb,"#Varnish vmod for GeoMaxMind DB
https://www.maxmind.com/en/home

**NOTE**
This is for Varnish 3

I forked this to add a new feature that would allow me to work with our weather API provider, Accuweather.

## Installation
This module requires the following:

Varnish cache from https://github.com/varnish/Varnish-Cache

libmaxminddb from https://github.com/maxmind/libmaxminddb

http://maxmind.github.io/MaxMind-DB/

```
cd /usr/local/src
git clone https://github.com/varnish/Varnish-Cache.git
cd Varnish-Cache
git branch 3.0 -t origin/3.0
git checkout 3.0
# make sure i'm matching release versions - 3.0.5 in my case.
git checkout 1a89b1f75895bbf874e83cfc6f6123737a3fd76f
./autogen.sh
./configure --prefix=/usr/local
make
make install
cd ..
git clone --recursive https://github.com/maxmind/libmaxminddb.git
cd libmaxminddb
./bootstrap
./configure --prefix=/usr/local
make 
make install
cd ..
git clone git@github.com:russellsimpkins/varnish-mmdb-vmod.git
cd varnish-mmdb-vmod
./autogen.sh
./configure --prefix=/usr --with-maxminddbfile=/mnt/mmdb/GeoIP2-City.mmdb VARNISHSRC=/usr/local/src/Varnish-Cache VMODDIR=/usr/lib64/varnish/vmods
make
make install
```
I added --with-maxminddbfile to autoconf so that you can decide, when you build the module, where you're data file will live. If you don't specify a value the default will be used.

```
#define MAX_CITY_DB ""/mnt/mmdb/GeoLite2-City.mmdb""
```
I also modified the module to open the maxmind db file once, on Init. I found that if you open the data file with each execution you incur a significant performance impact. 


This will work with the free data or the licensed data. 


## Example

```
import geo;
import std;

sub vcl_recv{
 set req.http.X-Forwarded-For = client.ip;
 std.syslog(180, geo.city(req.http.X-Forwarded-For));
 std.syslog(180, geo.country(req.http.X-Forwarded-For));
}

```

## Testing
You can add tests in src/tests. Use src/tests/test01.vtc as an example or check out https://github.com/varnish/libvmod-example to see how their done. **NOTE** you will need to 

```
cd src
make tests/*
```
",provide GeoIP lookup functionality and integration with the Accuweather API,"['C', 'Ruby', 'C++', 'Shell']","[128605, 35423, 28065, 925]"
BobKingstone/Pedlar-Cart,e-commerce shopping cart package for laravel,"Pedlar-Cart Laravel Package
===============

[![Build Status](https://travis-ci.org/BobKingstone/Pedlar-Cart.svg?branch=master)](https://travis-ci.org/BobKingstone/Pedlar-Cart)[![Latest Stable Version](https://poser.pugx.org/bobkingstone/pedlar-cart/v/stable.svg)](https://packagist.org/packages/bobkingstone/pedlar-cart) [![Total Downloads](https://poser.pugx.org/bobkingstone/pedlar-cart/downloads.svg)](https://packagist.org/packages/bobkingstone/pedlar-cart) [![Latest Unstable Version](https://poser.pugx.org/bobkingstone/pedlar-cart/v/unstable.svg)](https://packagist.org/packages/bobkingstone/pedlar-cart) [![License](https://poser.pugx.org/bobkingstone/pedlar-cart/license.svg)](https://packagist.org/packages/bobkingstone/pedlar-cart)

Laravel e-commerce shopping cart package. It uses Laravel's Session class by default.

Installation
-----------

Add bobkingstone/pedlar-cart to your composer.json

    ""require"": {
        ""bobkingstone/pedlar-cart"": ""dev-master""
    }

Run composer update

    composer update

Once this has completed add the service provider to the array of providers in `app/config/app.php`

    'Bobkingstone\PedlarCart\PedlarCartServiceProvider'


Usage
---

The package generates a 'Cart' facade for the package automatically so there is no need to add it to the alias array.

To add an item to the cart:

    $item = array(
        'id => '1', //your cart id
        'qty' => 2,
        'price' => 200.00,
    );

    $CartItemIdentifier = Cart::insert($item);

To get the total number of all items in cart:

    Cart::countItems();

To get an array of CartItems from cart:

    Cart::all();

To access cart items values:

    foreach (Cart::all() as $item)
    {
        echo $item->id;
        echo $item->price;
        echo $item->qty;
    };

To get the total value of all items in cart:

    Cart::totalValue();

To get a count of unique items in cart:

    Cart::totalUniqueItems();

To empty the cart:

    Cart::clear();

To set tax rate, you can either add it to each item:

    $item = array(
        'id => '1',
        'qty' => 2,
        'tax' => 20,
        'price' => 200.00,
    );

    Cart::totalWithTax();

Or pass in the percentage with the cart total calculation (this will override each items predefined tax rate):

    Cart::totalWithTax(20);

To update an item

    $item = Cart::find('91936a0df88d531b5a770b614cd3c1ea');

    $item->update('qty','3');

This will replace the existing value, to add to the quantity add the same item:

    $item = array(
            'id => '1',
            'qty' => 2,
            'price' => 200.00,
        );

    Cart::insert($item);

The item quantity will automatically be added.

To remove a single item from cart:

    Cart::remove('91936a0df88d531b5a770b614cd3c1ea');


Exceptions
---

The package will throw InvalidNumberOfValuesException if one of the following required params is missing:

    $requiredParams = array (
        'id',
        'qty',
        'price'
    );

It will also throw InvalidItemKeyException if an invalid update is passed to a cart item.

Notes
---

Using dd() to view cart contents will interrupt Laravels Session storage functions and can prevent the cart from being updated.",provide a shopping cart feature for e-commerce sites utilizing Laravel web framework,['PHP'],[16869]
semantic-dependency-parsing/toolkit,Semantic Dependency Parsing Toolkit,"# Semantic Dependency Parsing Toolkit

This repository contains a Java toolkit for semantic dependency parsing. It has been developed in connection with two shared tasks:

* [SemEval-2014 Task on Broad-Coverage Semantic Dependency Parsing](http://alt.qcri.org/semeval2014/task8/)
* [SemEval-2015 Task on Broad-Coverage Semantic Dependency Parsing](http://alt.qcri.org/semeval2015/task18/)

Detailed information about the tasks can be found at the respective websites.

## Downloading

The primary form of distribution for the project is via Git. See the [Releases](https://github.com/semantic-dependency-parsing/toolkit/releases) page for precompiled jar files.

## Building

After checking out the project from the repository, you should be able to build it using [Gradle](http://www.gradle.org/).

	$ cd toolkit
	$ gradle build

This will create a file `build/libs/sdp.jar` with the compiled classes. The jar can then be added to your classpath, whereby you will be able to use the provided classes in your own project. To see what is there, build the documentation:

	$ gradle javadoc

The entry page for the documentation is `build/docs/javadoc/index.html`.

## Command-line tools

Some of the tools implemented in the project can be called from the command line. The most revelant example is the `Scorer` tool, which is run as follows:

	$ java -cp se.liu.ida.nlp.sdp.toolkit.tools.Scorer gold.sdp system.sdp representation=DM

This will evaluate the parser output in the file `system.sdp` based on the gold-standard analyses in the file `gold.sdp` based on the assumption that the data is given in the `DM` representation; other possible representations are `PAS` and `PSD`. The evaluation metrics used are defined on the [Evaluation page](http://alt.qcri.org/semeval2015/task18/index.php?id=evaluation).

The Git repository contains a convenience shell script called `run.sh` that allows you to use an abbreviated form of the above command, assuming that the jar file is in `build/libs/sdp.jar`:

	$ sh run.sh Scorer gold.sdp system.sdp representation=DM

Abbreviations:

	LP: labeled precision
	LR: labeled recall
	LF: labeled F1
	LM: labeled exact match
	
	UP: unlabeled precision
	UR: unlabeled recall
	UF: unlabeled F1
	UM: unlabeled exact match
	
	PP: precision with respect to complete predications
	PR: recall with respect to complete predications
	PF: F1 with respect to complete predications
	
	FP: precision with respect to semantic frames
	FR: recall with respect to semantic frames
	FF: F1 with respect to semantic frames
	
",semantic dependency parsing,"['Java', 'Shell']","[88844, 133]"
numat/threeflex,Python driver and command line tool for Micromeritics 3Flex surface characterization analyzers.,"3flex
=====

Raw TCP/IP driver and command line tool for [Micromeritics 3Flex surface characterization analyzers](http://www.micromeritics.com/Product-Showcase/3Flex-Surface-Characterization-Analyzer.aspx).





This is a read-only driver, as there is currently no support for
programmatically controlling the 3Flex.

Installation
============

```
pip install threeflex
```

If you don't like pip, you can also install from source:

```
git clone https://github.com/numat/threeflex.git
cd threeflex
python setup.py install
```

Usage
=====

### Command Line

For basic tasks, this driver includes a command-line interface. This command
will output either a JSON representation of the current state, or a raw stream
of tab-separated values with the `--stream` flag. Read the help for more.

```
threeflex --help
```

### Python

For more complex projects, use python to automate your workflow.

```python
from threeflex import Analyzer
analyzer = Analyzer(""192.168.77.100"")
print(analyzer.get())
```

If the analyzer is currently running, this should return an object of the form:

```python
{
  ""time"": 4816304.0,             # Time since beginning, in milliseconds
  ""manifold"": {
    ""pressure"": 6.3,             # Manifold pressure, torr
    ""temperature"": 318.1,        # Manifold temperature, K
    ""volume"": 35.0               # Manifold volume, cc
  },
  ""ports"": [
    {
      ""adsorbed"": 2.8,           # Quantity adsorbed, cc (STP)
      ""data_points_taken"": 0,    # Number of data points taken
      ""dosed"": 3.0142,           # Quantity dosed, cc (STP)
      ""freespace"": {
        ""cold"": 22.205,          # Cold freespace, cc (STP)
        ""warm"": 22.205           # Warm freespace, cc (STP)
      },
      ""has_manifold"": False,     # Don't know.
      ""last_data_point"": {       # Data saved from previous measurement
        ""adsorbed"": 0.0,
        ""dosed"": 0.0,
        ""elapsed_time"": 0.0,
        ""p0"": 0.0,
        ""pressure"": 0.0
      },
      ""pressure"": 4.2,           # Pressure, torr
      ""pressure_table_index"": 0, # Don't know.
      ""temperatures"": {          # Port temperature readings at various points
        ""ambient"": 298.0,
        ""analysis"": 298.0,
        ""overall"": 318.1
      },
      ""valve_open"": False,       # Whether or not port valve is open
      ""volume"": 15.7195          # Port volume, cc
    },
    ...                          # Two more of above for the other ports
  ]
}
```
",Micromeritics' product 3Flex surface characterization,['Python'],[6288]
slowmoVideo/slowmoVideo,Official slowmoVideo repository,"slowmoVideo
===========

Hello! This is a short introduction for you if you want to:
- compile
- develop
- translate

slowmoVideo. For everything else please go to the 
[web page](http://slowmoVideo.granjow.net) or the 
[Google+ group](https://plus.google.com/communities/116570263544012246711).

Building
--------

### Building for Linux
See [our wiki](https://github.com/slowmoVideo/slowmoVideo/wiki/Download) for build instructions

Or see (outdated) http://slowmovideo.granjow.net/download.php



### Building for Windows
Compiling slowmoVideo for Windows using MXE on Linux:

1.  Get mxe _not_ from http://mxe.cc/ BUT, as long as OpenCV is not in the official branch, from
    https://github.com/Granjow/mxe/tree/opencv (Changes by Christian Frisson)
3.  Build opencv, qt, ffmpeg
    and copy the fixed CMake file (avoids library names like liblibjasper) with:
    `$ cp replaceOnTime/OpenCVConfig.cmake usr/i686-pc-mingw32/`
4.  Run cmake for slowmoVideo, but now give a toolchain file:
    `cmake .. -DCMAKE_TOOLCHAIN_FILE=/PATH_TO_MXE/usr/i686-pc-mingw32/share/cmake/mxe-conf.cmake`
5.  Compile!

### Building for MacOS
take a look at README.osx for more detailed instruction

#### Notes
Additionally to slowmoVideo, ffmpeg.exe (32-bit build, static) is required.
Download it from http://ffmpeg.zeranoe.com/builds/ and put it into the same directory as slowmoUI.exe.


Translating
-----------

For this you should be in the slowmoVideo subdirectory which contains the tr/ directory. 
The tools (`linguist`, `lupdate`, `lrelease`) are available in the `qt4-dev-tools` package for Debian based systems.

### Adding your language
To add your language xx (like fr, it), run the following command to generate the respective .ts file:

    lupdate . -ts tr/slowmoVideo_xx.ts
    
After this you can start translating. To make slowmoVideo actually use the translation, add this entry
to `slowmoUI/resources.qrc`:

    
../tr/slowmoVideo_xx.qm


### Translation
First, run `lupdate` to get the newest strings to translate from the code. 
(Otherwise you might be translating something that does not even exist anymore.)

Then the .ts file can be translated, preferrably with qt’s Linguist, or with any other 
translation tool you like.

Finally, to see your translation “in action”, release the .ts file (this creates a .qm file).

    lupdate src/ -ts tr/slowmoVideo_xx.ts
    linguist tr/slowmoVideo_xx.ts
    lrelease tr/slowmoVideo_xx.ts

Now you can push your `.ts` file to git.

",make videos slow motion,"['C++', 'CMake', 'Makefile', 'C', 'Objective-C++', 'CSS', 'Python', 'Shell', 'Objective-C']","[877984, 59081, 23214, 22313, 14361, 8176, 6445, 2493, 632]"
TroyShaw/troykanoid,Arkanoid clone in C,"##Troykanoid - an Arkanoid Clone written in C

This is a basic Arkanoid clone written in C, using SDL and the Chipmunk physics engine.


![ScreenShot](http://i.imgur.com/NMN5qfH.png)

It features various powerups, including lots of balls, different sized paddles, powerful comet balls, and more.
It also includes a save feature which saves the best 9 scores to disc, along with the users name.

It uses the same level designs as the original arkanoid game.

![ScreenShot](http://i.imgur.com/sL0VsXs.png)

###Dependencies
Troykanoid depends on a few libraries including:
- SDL 2
- SDL_image
- SDL_ttf
- SDL_gfxPrimitves
- SDL_mixer
- Chipmunk Physics Library

To install these libraries in Ubuntu, type

- `sudo apt-get install libsdl2-dev`
- `sudo apt-get install libsdl2-image-dev`
- `sudo apt-get install libsdl2-ttf-dev`
- `sudo apt-get install libsdl2-gfx-dev`
- `sudo apt-get install libsdl2-mixer-dev`
- `sudo apt-get install chipmunk-dev`

Ensure these libraries are present, then build by typing `make clean` followed by `make`. `make clean` is necessary first because it copies various resources files to the bin directory.

To run, execute bin/game\_debug by typing `./bin/game_debug` from the root directory of the project.
",1986 arcade game Arkanoid,"['C', 'HTML', 'Objective-C', 'Makefile', 'C++', 'Shell']","[74023, 12590, 9952, 3637, 1043, 242]"
uakatt/kaikifs,"KaikiFS uses Cucumber, Capybara, Selenium, and RSpec to test KFS","KaikiFS
=======

Introduction
------------

KaikiFS contains cucumber scenarios and steps that use the `KaikiFS::WebDriver` (a thick wrapper
for `Selenium::WebDriver`) to test KFS with Selenium.

Compatibility
-------------

KaikiFS is tested on the following platforms:

* **ruby-1.9.3** on **Linux**

That is all.

Current Capabilities
--------------------

* dot
* dot
* dot

Roadmap
-------

* dot
* dot

Installation
------------

This is not a gem yet. So... just clone this repository for now.

Contributing
------------

Please do! Contributing is easy in Insinkerator. Please read the CONTRIBUTING.md document for more info. ... When that file exists.

Usage
-----

Big section.

Versioning
----------

KaikiFS follows [Semantic Versioning](http://semver.org/) (at least approximately) version 2.0.0-rc1.

License
-------

Please see [LICENSE.md](LICENSE.md)

",test the KFS file system,"['Ruby', 'Shell']","[84761, 1633]"
Twisol/anachronism,An RFC-compliant implementation of the Telnet protocol,"# Anachronism
Anachronism is a fully-compliant implementation of [the Telnet protocol][wiki-telnet]. Fallen
out of favor in this day and age, most people only know it as a command-line
tool for debugging HTTP. Today, Telnet is most commonly used in the realm of
[MUDs][wiki-muds], though there are still a few other niches filled by Telnet.

Anachronism offers a simple API for translating between streams of data and
events, and is completely network-agnostic. Anachronism also offers **channels**, an
abstraction layer which treats Telnet as a data multiplexer. Channels make it
extremely easy to build reusable modules for Telnet sub-protocols such
as MCCP (MUD Client Compression Protocol), which can be written once and plugged
into any application that wants to include support.

[wiki-telnet]: http://en.wikipedia.org/wiki/Telnet (Telnet at Wikipedia)
[wiki-muds]: http://en.wikipedia.org/wiki/MUD (MUDs at Wikipedia)

## Installation
While Anachronism has no dependencies and is theoretically cross-platform, I've
only written a Makefile for Linux. Help would be appreciated for making this
work across more platforms.

    make
    sudo make install

This will install Anachronism's shared and static libraries to /usr/local/lib,
and its header files to /usr/local/include/anachronism/. You may also need to
run `ldconfig` to make Anachronism available to your project's compiler/linker.

## Usage
The anachronism/nvt.h header can be consulted for more complete documentation.

### Basic usage
The core type exposed by Anachronism is the telnet\_nvt, which represents the
Telnet RFC's ""Network Virtual Terminal"". An NVT is created using
telnet\_nvt\_new(). When creating an NVT, you must provide it with a set of
callbacks to send events to, and an optional void\* to store as the event
handler's context. You can use telnet\_recv() to process incoming data, and
the telnet\_send\_\*() set of functions to emit outgoing data.

    #include 
    #include 
    
    void on_event(telnet_nvt* nvt, telnet_event* event)
    {
      switch (event->type)
      {
        // A data event (normal text received)
        case TELNET_EV_DATA:
        {
          telnet_data_event* ev = (telnet_data_event*)event;
          printf(""[IN]: %.*s\n"", ev->length, ev->data);
          break;
        }
        
        // Outgoing data emitted by the NVT
        case TELNET_EV_SEND:
        {
          telnet_send_event* ev = (telnet_send_event*)event;
          printf(""[OUT]: %.*s\n"", ev->length, ev->data);
          break;
        }
      }
    }
    
    int main()
    {
      // Create an NVT
      telnet_nvt* nvt = telnet_nvt_new(NULL, &on_event, NULL, NULL);
      
      // Process some incoming data
      const char* data = ""foo bar baz"";
      telnet_receive(nvt, (const telnet_byte*)data, strlen(data), NULL);
      
      // Free the NVT
      telnet_nvt_free(nvt);
      return 0;
    }

### Telopts
Anachronism provides an easy-to-use interface to Telnet's ""telopt"" functionality
via the telnet\_telopt\_*() set of functions. As telopts are negotiated and
utilized, events are sent to the telopt callback provided to telnet_nvt_new().

    #include 
    #include 
    
    void on_event(telnet_nvt* nvt, telnet_event* event)
    {
      switch (event->type)
      {
        // Outgoing data emitted by the NVT
        case TELNET_EV_SEND:
        {
          telnet_send_event* ev = (telnet_send_event*)event;
          printf(""[OUT]: %.*s\n"", ev->length, ev->data);
          break;
        }
      }
    }
    
    void on_telopt_event(telnet_nvt* nvt, telnet_byte telopt, telnet_telopt_event* event)
    {
      // telopt is the telopt this event was triggered for
      
      switch (event->type)
      {
        case TELNET_EV_TELOPT_TOGGLE:
          telnet_telopt_toggle_event* ev = (telnet_telopt_toggle_event*)event;
          // ev->where is TELNET_TELOPT_LOCAL or TELNET_TELOPT_REMOTE,
          //     corresponding to Telnet's WILL/WONT and DO/DONT commands.
          // ev->status is TELNET_TELOPT_ON or TELNET_TELOPT_OFF.
          break;
        case TELNET_EV_TELOPT_FOCUS:
          telnet_telopt_focus_event* ev = (telnet_telopt_focus_event*)event;
          // ev->focus is 1 or 0 depending on if a subnegotiation packet has
          //     begun or ended.
          break;
        case TELNET_EV_TELOPT_DATA:
          telnet_telopt_data_event* ev = (telnet_telopt_data_event*)event;
          // ev->data is a pointer to the received data.
          // ev->length is the length of the data buffer.
          break;
      }
    }
    
    int main()
    {
      // Create an NVT
      telnet_nvt* nvt = telnet_nvt_new(NULL, &on_event, &on_telopt_event, NULL);
      
      // Ask to enable a telopt locally (a WILL command)
      telnet_request_enable(nvt, 230, TELNET_LOCAL);
      
      // Process some incoming data
      const char* data = ""\xFF\xFD\xE6"" // IAC DO 230  (turn channel on)
                         ""\xFF\xFA\xE6"" // IAC SB 230  (switch to channel)
                         ""foo bar baz""                 (send data)
                         ""\xFF\xF0"";    // IAC SE      (switch to main)
      telnet_receive(nvt, (const telnet_byte*)data, strlen(data), NULL);
      
      // Free the NVT
      telnet_nvt_free(nvt);
      return 0;
    }

### Interrupting
    TODO: Explain how to interrupt the parser.

## Alternatives
* [libtelnet][github-libtelnet], by Elanthis
  It incorporates a number of (rather MUD-specific) protocols by default,
  though its API is quite different.

[github-libtelnet]: https://github.com/elanthis/libtelnet (libtelnet on GitHub)

## Credits
Someone from #startups on Freenode IRC suggested the name (I'm sure as a joke).
If you read this, remind me who you are so I can credit you properly!
",an RFC-compliant Telnet network protocol,['C'],[35206]
jruby/jruby-ldap,JRuby/LDAP is a native LDAP implementation that uses JNDI to implement a Ruby/LDAP compatible API.,"JRuby/LDAP
-----------

This is an interface compatible port of Ruby/LDAP - to allow binary LDAP usage through JRuby. This implementation is pure Ruby, but uses the Java Integration features of JRuby to access the JNDI libraries and through these implement the correct functionality.

Many classes are missing right now, but the base functionality should be there.


Usage
-----
require 'rubygems'
require 'ldap'

And then use it like all tutorials of Ruby/LDAP show you.
",binary-LDAP usage through Jruby,['Ruby'],[44600]
collegedesis/bidwars,A little app for competitions to bid on DJs for The Best Brown Party. Maybe we'll expand this later to use for real events too! ,"Bidding application to get competitions to bid on DJs for [The Best Brown Party, Round 2](https://www.facebook.com/events/346163575480157/) on Feb 10th. 

Competition sponsors are:

* 2PM: Midwest Dhamaka
* 3PM: Gateway to India
* 4PM: Tufaan Hour
* 5PM: Virsa Punjab Da Hour
* 6PM: Michigan Mazaa Hour
* 7PM: Boiler Bhangra Hour
* 8PM: Buckeye Mela
* 9PM: Nachte Raho
","bid for the best DJ at the Facebook Event ""The Best Brown Party""","['Ruby', 'CoffeeScript', 'JavaScript']","[38508, 1352, 663]"
TI-OpenLink/ti-utils_soldel_maintenance,calibrator and FW reposetories,"
The calibrator and other useful utilities for TI wireless solution,
based on wl12xx driver.

Calibration is a process in which specific radio configuration parameters are
generated and saved into the NVS file, later to be used by the wl12xx driver
upon initialization. 
These configuration parameters are specific to the chip on the specific design
and therefore are sent back to the driver to store in non-volatile memory for
later use. Upon initialization, the wl12xx driver will load an NVS file where
it expects to read those parameters and send them to the chip. 

The NVS file contains 2 main parts - one stores the calibration parameters and
the other one stores initialization information required for the wl12xx driver. 

--- Build procedure

Kernel configuration.
Make sure your kernel is configured to support nl80211 testmode commands
(NL80211_TESTMODE=y). Also enable following configurations:
CRC7=m
FW_LOADER=m

In userspace there is dependent on libnl v2.x package. It can be downloaded
from http://www.infradead.org/~tgr/libnl/files/libnl-2.0.tar.gz

Set follow environment variables:
export NFSROOT=
export CROSS_COMPILE=arm-none-linux-gnueabi-
make
make install

--- How to calibrate

Automatic calibration procedure.

* Make sure all modules up to wl12xx.ko are loaded. wl12xx_sdio.ko should NOT be loaded.
* The firmware directory must be writeable.
* A ""plt firmware"" must be present in the firmware directory.

calibrator plt autocalibrate     
	dev		Device name. Probably wlan0
	kernel mod	Full path to wl12xx_sdio.ko kernel module
	ini 		Full path to Radio param ini file
	nvs		Full path of nvs file. Must be the real path as wl12xx will load it

This command will perform the following steps:

* Parse the ini and create an nvs without calibration data.
* Load kernel module.
* Perform TxBip and update nvs with calibration data.
* Unload kernel module.

Android example:
./calibrator plt autocalibrate wlan0 /system/lib/modules/wl12xx_sdio.ko TQS_D_1.7.ini /etc/firmware/ti-connectivity/wl1271-nvs.bin 00:01:02:03:04:05

Native Linux example:
./calibrator plt autocalibrate wlan0 /lib/modules/wl12xx_sdio.ko TQS_D_1.7.ini /lib/firmware/ti-connectivity/wl1271-nvs.bin 00:01:02:03:04:05

--- How to choose INI file

For Beagle board and Panda board use ini_files/127x/TQS_S_2.6.ini
For Blaze board with ES2.1 or ES2.1 use ini_files/128x/TQS_D_1.7.ini

--- How to change MAC address in calibrated NVS

./calibrator set nvs_mac  []

If the MAC address missing, the random valid value will be added.


--- How to dump NVS file

calibrator get dump_nvs []


--- Detailed instructions for calibrator procedures

Normally you should use the autocalibrate command but it's also possible to
run each step manually.

	TxBip procedure (calibration)

It is important to set MAC address to an interface before the procedure.
For example, `ifconfig wlan0 hw ether xx:xx:xx:xx:xx:xx'

calibrator wlan0 plt power_mode on
calibrator wlan0 plt tx_bip <0|1> <0|1> <0|1> <0|1> <0|1> <0|1> <0|1> <0|1>
calibrator wlan0 plt power_mode off

Result of this procedure is new NVS file created locally ./new-nvs.bin
In order to use it, copy the file to /lib/firmware/wl1271-nvs.bin and reload.

	TxCont procedure

calibrator wlan0 plt power_mode on
calibrator wlan0 plt tune_channel  
calibrator wlan0 plt tx_cont               
calibrator wlan0 plt tx_stop
calibrator wlan0 plt power_mode off

Description: This test sends packets of data directly to air. It receives 
several parameters as described bellow, to enable diversity of
operational modes.
It is mostly used to see Energy and radio impact on Air.
Content of Packet can be Random, or Zero, One, Zero, one...

Packets send are configurable with following parameters:
  Delay between packets in microseconds
  Rate
    1 Mbps -   0x00000001       MCS_0 - 0x00002000
    2 Mbps -   0x00000002       MCS_1 - 0x00004000
    5.5 Mbps - 0x00000004       MCS_2 - 0x00008000
    6 Mbps -   0x00000008       MCS_3 - 0x00010000
    9 Mbps -   0x00000010       MCS_4 - 0x00020000
    11 Mbps -  0x00000020       MCS_5 - 0x00040000
    12 Mbps -  0x00000040       MCS_6 - 0x00080000
    18 Mbps -  0x00000080       MCS_7 - 0x00100000
    24 Mbps -  0x00000200
    36 Mbps -  0x00000400
    48 Mbps -  0x00000800
    54 Mbps -  0x00001000
  Size of data field in MPDU (in bytes, 0 - 2284)
  Amount - number of packets in case of using series mode
  Power - output power in dBm*1000
  Seed - value for the scrambler
  Packet mode - 0-single, 1-multipile, 3-continuous, 4-FCC
  DC on/off - activate DCF
  gi - guard interval on/off for 11n rates
  Preamble
    1 Mbps - long preamble mode=0
    2, 5.5, 11 Mbps - long preamble mode=0, short preamble mode=1
    6, 9, 12, 18, 24, 36, 48, 54 Mbps - ofdm preamble mode=4
    from MCS_0 to MCS_7 - n mixed mode preamble mode=6, greenfield preamble mode=7
  Type is 0-data packet, 1-ack, 2-probe req, 3-random data, 4-user data
  Scrambler - on/off
  CLPC
    range 0-100 is disable calibration
  Sequence number mode (incremented or fixed)
  Destination Mac address

	RxStat procedure

There are 2 ways to do it - short where all parameters has default values and
full where you have to set all parameters manually.

Short way:
calibrator plt rx_statistics

In the short way each time the statistics will be reseted.

Full way:
calibrator wlan0 plt power_mode on
calibrator wlan0 plt start_rx_statcs
calibrator wlan0 plt get_rx_statcs
calibrator wlan0 plt stop_rx_statcs
calibrator wlan0 plt power_mode off

While willing to reset the statistic run:
calibrator wlan0 plt reset_rx_statcs

	Update NVS file procedure

This is procedure changes ini part of NVS file. It helps when there is need
to change ini part of NVS which already calibrated. 

calibrator set upd_nvs  [ []]

If NVS filename parameter not provided the current NVS file will be used from
destination directory (usually /lib/firmware).


--- Miscellaneous procedures

	Read MAC address from NVS file

calibrator get nvs_mac []

	Set NVS to use auto FEM detection 

calibrator set autofem 1


	Set FEM manufacturer

calibrator set fem_manuf 0|1


	Tone transmission testing
Get in PLT mode
calibrator wlan0 plt power_mode on

Run TxTone transmission
calibrator wlan0 plt tx_tone  
Tone type
    1 - Carrier FeedThrough
    2 - Single tone
Power [0 - 10000] mdB

Stop transmission
calibrator wlan0 plt tx_stop

Get out from PLT mode
calibrator wlan0 plt power_mode off

-------------------------------------------------------------------------------

The project can be accessed from git repository:
	git clone git://github.com/TI-OpenLink/ti-utils.git
",configuration calibrator utilities for Texas Instruments wireless solutions,"['C', 'Python', 'Shell']","[211496, 3781, 367]"
jittat/cafe-grader-judge-scripts,Judge scripts for Cafe grader.,,Judge scripts for the github project Caf Grader,"['Ruby', 'C', 'C++', 'Shell', 'Makefile', 'HTML']","[119876, 106198, 31482, 18143, 2099, 692]"
zopefoundation/zope.app.publication,Zope Application Publication and Traversal Components,Publication and traversal components.,publication and traversal components for Zope,['Python'],[136923]
metabrainz/libdiscid,C library for creating MusicBrainz DiscIDs from audio CDs,"A Library for creating MusicBrainz DiscIDs
------------------------------------------

libdiscid is a C library for creating MusicBrainz DiscIDs from audio CDs.
It reads a CD's table of contents (TOC) and generates an identifier which
can be used to lookup the CD at MusicBrainz (http://musicbrainz.org).
Additionally, it provides a submission URL for adding the DiscID to the
database.

The library also provides FreeDB disc IDs, and MCN + ISRCs, if available.

The interface of this library is new, but the DiscID algorithm and the
operating system dependent CD-ROM/DVD-ROM access code have been ported
from libmusicbrainz version 2. 


Please report all bugs you find via the MusicBrainz bug tracker.
Don't forget to state which OS and what version you are using:

    http://tickets.musicbrainz.org/browse/LIB

Questions about this package may be posted to the MusicBrainz
development mailing list (mb-devel):

    http://musicbrainz.org/doc/Communication/Mailing_Lists

More information can be found at the package's official homepage:

    http://musicbrainz.org/doc/libdiscid

",creating DiscIDs from audio CDs for MusicBrainz collaborative music database,"['C', 'CMake', 'Makefile', 'M4', 'Shell']","[105826, 11097, 3815, 3496, 1435]"
cemagg/sucem-fem,A finite element code for modelling electromagnetic phenomena by the Computational Electromagnetics Group at Stellenbosch University. Please see the wiki for more info.,"Welcome to the development site for SUCEM:FEM, a finite element modelling 
package from the Computational Electromagnetics Group at Stellenbosch University, South Africa.

# Requirements

# Usage 

# Getting started

See [http://github.com/cemagg/sucem-fem/wiki/Getting-Started]
",finite element modelling for modelling electromagnetic phenomena,"['Python', 'Shell']","[354700, 125]"
zfsrogue/spl-crypto,Solaris Porting Layer with crypto functions,"The Solaris Porting Layer (SPL) is a Linux kernel module which provides
many of the Solaris kernel APIs.  This shim layer makes it possible to
run Solaris kernel code in the Linux kernel with relatively minimal
modification.  This can be particularly useful when you want to track
upstream Solaris development closely and do not want the overhead of
maintaining a large patch which converts Solaris primitives to Linux
primitives.

To build packages for your distribution:

    $ ./configure
    $ make pkg

If you are building directly from the git tree and not an officially
released tarball you will need to generate the configure script.
This can be done by executing the autogen.sh script after installing
the GNU autotools for your distribution.

To copy the kernel code inside your kernel source tree for builtin
compilation:

    $ ./configure --enable-linux-builtin --with-linux=/usr/src/linux-...
    $ ./copy-builtin /usr/src/linux-...

The SPL comes with an automated test suite called SPLAT.  The test suite
is implemented in two parts.  There is a kernel module which contains
the tests and a user space utility which controls which tests are run.
To run the full test suite:

    $ sudo insmod ./module/splat/splat.ko
    $ sudo ./cmd/splat --all

Full documentation for building, configuring, testing, and using the
SPL can be found at: 
",Solaris unix operating system to run as a Linux Kernel module with cryptographic functions,"['C', 'Shell', 'C++', 'Awk']","[954585, 26049, 14930, 334]"
packfire/concrete,Concrete: Rock solid PHAR compiler for PHP,"#Concrete

[![Build Status](https://travis-ci.org/mauris/concrete.svg?branch=master)](https://travis-ci.org/mauris/concrete) [![Latest Stable Version](https://poser.pugx.org/mauris/concrete/v/stable.svg)](https://packagist.org/packages/mauris/concrete) [![Total Downloads](https://poser.pugx.org/mauris/concrete/downloads.svg)](https://packagist.org/packages/mauris/concrete)

>Rock-solid PHAR compiler for PHP

##What is Concrete?

Concrete is a simple CLI tool that helps you to compile your PHP application into PHAR binary for distribution.

##Usage

There are three ways of using Concrete. Through [Composer](https://getcomposer.org/), you can install Concrete to your system hassle-free or include it as a library on your project. Alternatively, you can build Concrete into a PHAR archive and use the binary.

###Usage - Composer CLI Application

With great thanks to Composer, you can install Concrete on your machine. You must ensure that Composer is installed on your system. Run the following command to install Concrete.

    composer global require mauris/concrete

After installation is complete, you will be able to use concrete:

    concrete build

###Usage - Composer Library

You can use Concrete by adding the following [`mauris/concrete`](https://packagist.org/packages/mauris/concrete) into your `composer.json` file like this:

    {
        ""require"":{
            ""mauris/concrete"": ""1.2.*""
        },
    }

Then install your dependencies by following the [Composer installation](http://getcomposer.org/doc/00-intro.md) command:

    $ php composer.phar install

After which, you may create a `Compiler` class that extends from `\Concrete\Compiler` and you may write your build script within the protected overriding method `compile()`. To build your PHAR binary, write a script to run the `build()` method of your `Compiler` class.

###Usage - PHAR Binary

You can use Concrete as a standalone PHAR binary. Either

1) Download from the [Release](https://github.com/mauris/concrete/releases) page; or,
2) Follow the instructions found in ""Building Concrete"" to build the `concrete.phar`.

##Building Concrete

Concrete uses `concrete.json` to compile itself into `concrete.phar` by running:

    $ php bin/concrete

Concrete will use the information found in the `concrete.json` file of the project directory to build itself.

Afterwhich, you will find the `concrete.phar` generated in the current working directory.

##License

Concrete is released open source under the *BSD 3-Clause License*. See the `LICENSE` file in repository for details.
",compiling a PHP application into PHAR binary file package format for distribution,['PHP'],[24116]
Elive/emodule-productivity,Pomodoro-like emodule application that helps you to fight procrastination by having control of your active windows that gives constant distractions,"Emodule-Productivity
====================

This emodule helps you to fight against working distractions (procrastination), it consist in a similar technique to pomodoro with breaktimes but a lot more powerful, because it gives or deny's you access to the distracting windows. 

 * Define the windows that you consider important in the _Work tools_ section, for example: Terminal, Meld, Gvim, Calculator, Inkscape, xpad, etc...
 * Set the minutes that you want to have for _breaktime_ after every _x_ minutes of work
 * Start working and enjoy :)


Features
========

When you are working, it hides all the windows that you have not defined as _tools for work_, when the breaktime is reached it shows you them back allowing you some minutes of access to distracting things like chats or web, then finally hides them again by controlling for you the time that you lose on them (notifying you with a sound 5 seconds before to hide them)

There's an option for allow you to satisfy the urgent windows if they appears, only one time


Development
===========

The next development (TODO) of this emodule is for now frozen because it does his job, it will be maintained for future fixes and updates if needed, but we are not going to include more features for now on the near future, not because we don't have interest but because we don't have time.

If you found it interesting and you want to *collaborate by making it better*, you are more than welcome to do it and we are interested in adding more features.

After to study a bit the psycology behind procrastination, we leave here one of the most useful articles found: http://blog.iqmatrix.com/overcome-procrastination


TODO and Ideas
==============

 * *Profiles*: There's different types of tasks in our lifes, for example _boring_ and _creative_, the creative ones requires the less distractions possible and calm music, the boring tasks requires active music and fast breaktimes. By having different profiles we can have different types of works, selectable on the main window, that makes our desktop act differently. Also remember that different types of _jobs_ requires different configurations (more exactly different applications to use).

 * *Statistcis*: One of the best ways to overcome procrastination is by *knowing* on what exactly _we lose time with_, we have already functions in the code to know how much times we have passed in every window, so we can make easly a way to show them to the user in *percentage bars* or a pie chart, we can also include the number of *keystrokes* used in every chart.

 * *Grant usage*: Sometimes we need a fast usage of a not-allowed application (like web), because we need for example a fast research in google, but at the same time is a focus of distraction, will be nice if by a _keyboard shortcut_ we call the module for grant us access to it, so it will ask us how much minutes we need for it and he will hide back the window after these minutes _(we are rational and sincere about the amount of time that we need, but we easly lose the control of the time passed, which is the task of this emodule)_, this feature will not allow to use again the application after passed a specific amount of time _'you already used it!'_

 * *Prohibited web access*: To have a list of comma-separated websites or keywords to pass to iptables for block us from access to the selected websites, this can give us the opportunity to use the web for our work but to not lose time in social websites (in fact we can easly lose time with any kind of website, specially new ones).

 * *Full Internet disconnection*: Internet is a focus of distractions, the optimal solution is of course to entire block it, by other side is possible that we still require some access to internet (apt-get, cronjobs of backups, etc), could be nice if is triggered some iptables rules that *blocks everything* except what we have listed to be allowed, just like the list of applications to use, but for internet keywords.

 * *Password-protected*: We are our own victims of our distractions, so is very easy for us to unload the module, click on stop-working, etc... these things should be not allowed/possible, maybe using a special password set by someone else

 * *Focus watcher*: That was one of the original ideas, to control _'which is the focused window'_ (the one you are working on), by giving features like re-focus on the correct window after an amount of time or to punish you if you lose too much time in the non-working window, this feature was replaced by the simply control of hiding windows, but some good ideas can be still recycled from this concept.

 * *Reclaim*: If the user has not typed or moved the mouse by a specific amount of time, reclaim his attention.

 * *Progress bar*: A progress-bar similar to the places emodule which turns from green to red per every cycle (breaktimes), this raises the reponsability of the user to focus on the work.

 * *Assessment and Monitoring*: In the same way that a personal trainer follows your progress, in every breaktime an entry box can ask you _'what you have reached on this cycle?'_, this data is very useful for a personal monitoring of progress since you can be conscious about your lacks and worth steps, this data can be also useful for show in a _daily statistics of progress and lacks_, other useful things can be asked like having a progress bar to set the value of efficiency, how much worth is, etc. _(see the article about overcome procrastination in the previous section)_.

 * *Goals*: This feature may change a bit the original behaviour of the emodule but is important to remember that we need to have a control/assesment of the time required for reach the goals, for example if we need 5 hours for finish something, we need to force us to use only 5 hours, instant rewards are a good incentive (to watch some funny video series), and punishing us in some way if we need more time than the decided one (blocking with iptables youporn for one week :))

 * *Postponement*: In the case that we have an assessment feature that knows which task are we doing at every moment, if we reach the double of time originally requested for finish a task, block the desktop for some minutes asking the user to reconsider whether it is worth what is trying to reach.



Tips
====

A free account on mindomo or using a slate is a very good way to _note_ everything that comes to your head and that you cannot do right now, this helps you freeing the pointers in your mind by moving away the ideas but not losing track of them

If you want to *try this application* but you can't compile it, an alternative and fast way is to directly try it from the live mode of Elive http://www.elivecd.org (from a bigger version than _2.0 - Topaz_)





",control active windows and fite procrastination,"['C', 'Shell']","[81065, 516]"
ckw-mod/ckw-mod,ckw is yet another command prompt.,"/////////////////////////////////////////////
// 概要

  コマンドプロンプトウィンドウを使い易くしたソフトです。

  コマンドプロンプトを隠しておいて、画面表示や キー操作など
  ユーザーインターフェース部分だけを置き換えています。

  Windows2000/XP/Vista/7で動作する Win32アプリケーションです。

  オリジナルの配布元は以下になります。（リンク切れ）
  http://www.geocities.jp/cygwin_ck/

  改変版はGithubにて公開しています。
  https://github.com/ckw-mod/ckw-mod

  このソフトウェアのライセンスは GNU General Public License v2です。

////////////////////////////////////////////
// コンパイル

  ・Microsoft Visual C++ 2008
  ・Microsoft Windows SDK v7.0
  という組み合わせでコンパイルを確認しています。

  Visual C++ 2008向けのソリューションファイルを同梱しています。

////////////////////////////////////////////
// その他

  「Console」を参考にさせて頂きました。
  http://sourceforge.net/projects/console/

  改変版を作るにあたって多数の方のパッチを取り込ませていただきました。
  この場をお借りしてお礼申し上げます。
",command prompt,"['C++', 'C', 'Shell']","[105898, 3318, 689]"
AgencyPMG/PMG-WP-Core,Useful utilities to make managing big WordPress sites easier.,"# PMG Core

PMG Core is a collection of utilities that PMG uses on its own and its clients
sites.  It's a library, with a tiny bit of functionality baked in.

**Note: this plugin requires PHP 5.3+**

Some things it does:

* Seeks to automate the creation of admin area fields.
* Makes adding meta boxes, user profile fields, and term fields really easy
* Automates the creation of post types and taxonomies.

## Whirlwind Tour

The central entry point is the `pmgcore` function.  You use this to create your
own ""projects"".

    create_type(
        'the_post_type',
        __('Singular Name', 'your-textdomain'),
        __('Plural Names', 'your-textdomain'),
        array(
            'public'             => true,
            'show_in_nav_menues' => false,
        )
    );

### Adding Taxonomies

    create_taxonomy(
        'the_taxonomy',
        __('Singular Name', 'your-textdomain'),
        __('Plural Names', 'your-textdomain'),
        array(
            'show_ui'   => true,
        ),
        array('page') // post type you want (optional, defaults to post)
    );

### Adding Settings (and Other) Fields

Adding settings fields/sections to already existing pages (General Options
here).

    setting('my_setting', 'general');

    $f->add_field('field_key, array(
        'label'     => __('Some Label', 'your-textdomain'),
        'type'      => 'text_input', // this is the default
        'cleaners'  => array('esc_url_raw'), // array of callable to run the field through on validation
        'section'   => 'default', // what section this belongs in (optional)
    ));

    // add a new section
    $f->add_section('section_key', array(
        'title'     => __('Section Title', 'your-textdomain'),
        'help'      => __('Section help text', 'your-textdomain'),
    ));

    // put fields in the new section
    $f->add_field('another_field_key', array(
        'label'     => __('Another Field', 'your-textdomain'),
        'type'      => 'textarea',
    ));

You can also add fields to a custom page by calling `pmgcore()->setting` without
the second argument.

All field creation has the same API: `add_section` and `add_field`. As you want
to create fields for a meta box:

    box_fields('my_metabox');

    // use $f as above

Or to create fields that use any of WordPress' various meta tables.

    meta_fields('my_metafields');

    // use $f as above

Additionally all fields have a `render` method which spits out the fields
themselves.  This behaves differently depending the type of fields created. See
[PMG\Core\Fields](https://github.com/AgencyPMG/PMG-WP-Core/tree/master/inc/Fields)
for more information.

### Creating Admin Pages.

First step: create fields like above.

    settings('my_setting');

    // do stuff with $f

Then you can use the `admin_page` method.

    admin_page('page_key', $f, array(
        'title'     => __('Page Title', 'your-textdomain'),
        'menu_name' => __('Menu Name', 'your-textdomain'),
        'parent'    => 'options-general.php', // optional -- default is none, a top level menu page
        'slug'      => 'your-page-slug',
    ));

### Adding Meta Boxes

Create a `MetaBoxFields` object like above.

    box_fields('my_metabox');

    // do stuff with $f

The use the `meta_box` method.

    meta_box('box_key', $f, array(
        'title'     => __('Box Title', 'your-textdomain'),
        'priority'  => 'high', // optional, default is 'high'
        'context'   => 'normal', // optional, default is 'normal'
    ));

The above will add a meta box to all public post types. You can specify post
types with an optional last argument:

    meta_box('box_key', $f, array(
        'title'     => __('Box Title', 'your-textdomain'),
        'priority'  => 'high', // optional, default is 'high'
        'context'   => 'normal', // optional, default is 'normal'
    ), array('page'));

### Adding User & Term Fields

Like pretty much everything else: create a fields object (using `meta_fields`).

    meta_fields('myterm_fields');

    // do stuff with $f

To add user fields use the `user_box` method.

    user_box('box_key', $f);

Or to put fields on user pages, use the `term_box` method.

    term_box('box_key', $f);

The above will put fields on all taxonomies with `show_ui` set to `true`. To
specify taxonomies, use the optional last argument.

    term_box('box_key', $f, array('category'));

### Meta Objects

PMG Core contains some wrappers of the WordPress metadata API to make things
easier to fetch and save.  Namely, the library will prefix things for you so you
don't have to worry about naming collisions.

There are four properties that contain these wrappers: `postmeta`, `usermeta`,
`commentmeta`, `termmeta`.

They all have the same API:

    postmeta;

    // put 'a value' in with the key '_my_project_some_key'
    $m->save($some_post_id, 'some_key', 'a value');

    // fetch the value in 'some_key'
    $m->get($some_post_id, 'some_key', 'default value');

    // delete a key
    $m->delete($some_post_id, 'some_key');

    // delete all values with the key 'some_key'
    $m->delete_all($some_post_id, 'some_key');

`termmeta` is not really termmeta.  It fakes term meta using the options table.
However, if `$wpdb->termmeta` is set (eg. someone has added a termmeta table) it
will use that.

The limitation here is that the library deals with only single meta items. This
may change in the future.

## Adding Rewrites

Use the `PMG\Core\Project::$router` property.

### Adding a rewrite rule

    router->add_rule(
        '^some-route/(\d+)/?$', // just regex
        'index.php?some_var=$matches[1]',
        'top' // this is option, defaults to top
    );

### Adding Rewrite Endpoints

    router->add_endpoint('ep', EP_ALL);

The second argument is option, defaults to `EP_ALL`.  Learn more about
endpoints
[here](http://make.wordpress.org/plugins/2012/06/07/rewrite-endpoints-api/).

### Adding Query Vars

    router->add_var('some_var');

    // add more than one
    pmgcore('your_project')->router->add_var(array('some_var', 'some_other_var'));

### Using the Router

The above doesn't gain you much more than a bit of convenience.  Use the
`add_route` property to take some shortcuts.

`add_route` only takes one argument: a route.  The route is just a string with
several variables built in.  The variables, in this case, take the form of
`<(int|str):some_key>`.  `add_route` will translate those into a rewrite.

So this:

    router->add_route('route//');

Is a shortcut for this:

    router;

    // add a rule
    $r->add_rule(
        '^route/(\d+)/([^/]+)/?$',
        'index.php?some_var=$matches[1]&other_var=$matches[2]'
    );

    // add the query vars
    $r->add_var(array('some_var', 'other_var'));

The downside, of course, is less fined grained control.  If you need any sort of
complex regex for your rewrite rules, it's better to just use strait regex and
`add_rule`.

### ""Catching"" Query Variables.

Sometimes you want to ""catch"" query variables on the front end and do certain
things if they hapen to be set.  `PMG\Core\Router::catch_var` let's you do that.

It takes two arguments: a query var to search and the callable to call when it's
found.  There's an optional third argument, `$exit`, which, if true, will cause
the execution to stop after the callable has been called. `$exit` defaults to
`true`.

    router->catch_var('some_var', function($v) {
        echo $v; // $v is the query var that was caught
    });

## Pluggable

PMG Core uses dependency injection to prevent loading a bunch of crap you don't
need.  In short, you can use the `pmgcore` entry point and only objects that you
use explicitely will be created.

To give an example, the first time you use the `postmeta` property, the object
that wraps the metadata API for post meta is created.

You can also mix and match classes as you see fit. I tried to keep everything
loosely coupled.

## Functionality

There is a little bit of
[functionality](https://github.com/AgencyPMG/PMG-WP-Core/tree/master/inc/Functionality) 
baked into this plugin.

### Cleanup

* Remove the meta generator tag from the `` section
* Allow users how can post `unfiltered_html` to put whatever they like in term
descriptions
* Set the default pingback flag to off
* Set the default ping status to off
* Set the default comment status to off
* Enable comment moderation
* Disable XML RPC
* Disabled WP-App (for WordPress 3.4 and lower)
* Remove all but the ""Right Now"" dashboard meta boxes

### Uploads

* Set the upload path to `{$_SERVER['DOCUMENT_ROOT']}/uploads`
* Set the upload url to `//{WP_HOME}/uploads`

### Headers

* Remove the shortlink header
* Add `X-Frame-Options: SAMEORIGIN` as a header on all WP rendered pages.
* Add `X-UA-Compatible: IE=edge,chrome=1` as a header on all WP rendered pages.
* Remove the `X-Pingback` header.
* Set an `X-Powered-By` header

### Enqueues

A single CSS and JS enqueue for the admin area -- this to make some pretty tabs
on metaboxes with multiple field sections.
",provide utilities for easy management of WordPress sites,"['PHP', 'JavaScript', 'CSS']","[76496, 2851, 1125]"
vivid-planet/kwf-newsletter-demo,Demo of a simple Newsletter System implemented using Koala Framework,,Newsletter System for web applications and websites,"['PHP', 'CSS']","[2638, 1071]"
liu-chong/micropolis,"Open Source Micropolis, based on the original SimCity. sync from http://micropolis.googlecode.com/svn",,Micropolis program,"['C', 'C++', 'Tcl', 'Python', 'JavaScript', 'C#', 'Shell', 'PHP']","[3971740, 1285777, 869087, 842059, 222845, 13782, 5883, 1879]"
CollectorsQuest/magnify-sdk,magnify.net API custom implementation,,video sharing platform,['PHP'],[37510]
NESCent/Taxonomy-Ontology-Tool,"Tool for constructing taxonomic ontologies from different authoritative sources for hierarchy, currently valid names, and synonyms.","This project is a tool for constructing taxonomic ontologies in OBO format (OWL format using taxa as individuals is in development).  In addition to the source and unit test code, the project contains an output ontology and several sample xml configuration files.",,"['Java', 'Shell']","[260543, 338]"
mteodori/jira-git-plugin,,,,['Java'],[129271]
sanguinariojoe/ocland,OpenCL Land is the OpenCL cloud computing interface,"ocland
======

OpenCL Land is the OpenCL cloud computing interface.

With ocland you can use devices in remote computers as OpenCL platforms installed in the local one, doing unnecessary the usage of other interfaces like MPI, resulting in a significant codes simplification. Also ocland can be used by all the applications that are accelerated with OpenCL without modifing nothing! No source code modifications or compilations are needed to have your OpenCL applications ready to use ocland!

Since ocland is based in a server-client structure ligthly lower performance can be expected compared with MPI based applications.

For the moment ocland is in a heavy development stage, but you can start testing it. But remember that is in alpha stage so is not recommended for production purposes.

ocland is composed by 3 subprojects (all of them provided with this package):

1) ocland server
2) ocland installable client driver (ICD)
3) ocland examples

All the ocland components compilation and install are aided with CMake.

ocland server
=============

Ocland server must be installed in all the computers that will serve computation resources remotely.

Suposing that you have downloaded ocland in a ocland.tar.gz compressed file you can install ocland server with the following commands (executed in the place where you downloaded the file):

tar -xvzf ocland.tar.gz
cd ocland
cmake -DOCLAND_SERVER:BOOL=ON -DOCLAND_SERVER_DAEMON:BOOL=ON -DOCLAND_CLIENT:BOOL=OFF -DOCLAND_CLIENT_ICD:BOOL=OFF OCLAND_EXAMPLES:BOOL=OFF .
make
make install

Note that the last command must be executed as superuser. Depending on your operative system, consider set the flag -DCMAKE_INSTALL_PREFIX:PATH=/usr too.

The daemon (set with the flag OCLAND_SERVER_DAEMON) is used in order to launch the ocland server at the operative system start, but if you don't want to launch automatically the ocland server you can disable the daemon (-DOCLAND_SERVER_DAEMON:BOOL=OFF).

When the server has been installed you can launch it typing:

ocland_server

In order to clients can access to ocland server resources several ports starting in 51000 must be opened. In ocland the port 51000 is used to stablish the connection between the client and server, but later more ports starting in 51001 will be opened to can perform asynchronously data transfers without interfere the main communication channel.

ocland ICD
==========

The ocland ICD must be installed in the computer where the OpenCL applications that you want to use remote resources will be launched.

Suposing that you have downloaded ocland in a ocland.tar.gz compressed file you can install ocland server with the following commands (executed in the place where you downloaded the file):

tar -xvzf ocland.tar.gz
cd ocland
cmake -DOCLAND_SERVER:BOOL=OFF -DOCLAND_SERVER_DAEMON:BOOL=OFF -DOCLAND_CLIENT:BOOL=ON -DOCLAND_CLIENT_ICD:BOOL=ON OCLAND_EXAMPLES:BOOL=OFF .
make
make install

Note that the last command must be executed as superuser. Depending on your operative system, consider set the flag -DCMAKE_INSTALL_PREFIX:PATH=/usr too.

The ocland driver will be installed, and a referency will be generated in the file /etc/OpenCL/vendors/ocland.icd in order to report to the ICD loader that must query ocland for available platforms.

In order to use remote resources you must create a plain text file called ocland in the folder where you will launch the OpenCL application, with the servers IP addresses (one per line). When application query for OpenCL platforms ocland will automatically connect to ocland servers specified in the ocland named file. If the file is not present, is blank, or the server are not available, simply no ocland platforms will offered, but you ever still have available the local platforms.

ocland examples
===============

The ocland examples are small tests designed to probe the ocland installation.

Suposing that you have downloaded ocland in a ocland.tar.gz compressed file you can install ocland server with the following commands (executed in the place where you downloaded the file):

tar -xvzf ocland.tar.gz
cd ocland
cmake -DOCLAND_SERVER:BOOL=OFF -DOCLAND_SERVER_DAEMON:BOOL=OFF -DOCLAND_CLIENT:BOOL=OFF -DOCLAND_CLIENT_ICD:BOOL=OFF OCLAND_EXAMPLES:BOOL=ON .
make
make install

Note that the last command must be executed as superuser. Depending on your operative system, consider set the flag -DCMAKE_INSTALL_PREFIX:PATH=/usr too.

Since the examples must be launched in the computers that has the ocland ICD installed, probably you want to install the ICD an the examples at the same time (-DOCLAND_CLIENT:BOOL=ON -DOCLAND_CLIENT_ICD:BOOL=ON OCLAND_EXAMPLES:BOOL=ON)

Supposing that you have installed the server and the client in the same computer, you can launch the first test executing (take into account the CMAKE_INSTALL_PREFIX selected flag):

echo 127.0.0.1 > ocland
/usr/local/share/ocland/test/test

And you will see how a small OpenCL application is executed in the local devices availables in two ways, the usual one and through the network managed by ocland (The platform name vendor and suffix have an ""ocland(127.0.0.1)"" prefix identifier)

It's magic!
",,"['C', 'CMake', 'C++', 'Shell']","[787894, 31932, 11518, 1072]"
grate-driver/grate,Grate: Open source Tegra2+ 2D/3D user-space,"# grate
Grate is an anagram of ""tegra"", actually it is a collection of open source reverse-engineering tools aiming at the NVIDIA Tegra2+ 3D engine.

## Documentation

We have several useful documents of the Tegra architecture on our [wiki](https://github.com/grate-driver/grate/wiki). In particular:
* [Command Stream](https://github.com/grate-driver/grate/wiki/Command-stream)
* [MMIO Registers](https://github.com/grate-driver/grate/wiki/MMIO-Registers)
* [Vertex Shader ISA](https://github.com/grate-driver/grate/wiki/Vertex-Shader-ISA)
* [Fragment Shader ISA](https://github.com/grate-driver/grate/wiki/Fragment-Shader-ISA)
* [Shader Linking](https://github.com/grate-driver/grate/wiki/Shader-Linking)
* [Geometry Submission](https://github.com/grate-driver/grate/wiki/Geometry-Submission)
",,"['C', 'Yacc', 'Lex', 'Makefile', 'Meson', 'GLSL', 'M4', 'Shell']","[1007340, 59123, 12237, 7546, 6884, 3828, 2478, 2321]"
singuerinc/puppet-font_explorer_pro,,"# Font Explorer Pro Puppet Module for Boxen

Experience an ultra-intuitive interface, a reliable architecture and a powerful feature set that provides quick access and control over your fonts.

[![Build Status](https://travis-ci.org/singuerinc/puppet-font_explorer_pro.png?branch=master)](https://travis-ci.org/singuerinc/puppet-font_explorer_pro)

## Usage

```puppet
include font_explorer_pro
```

## Required Puppet Modules

* `boxen`

## Development

Set `GITHUB_API_TOKEN` in your shell with a [Github oAuth Token](https://help.github.com/articles/creating-an-oauth-token-for-command-line-use) to raise your API rate limit. You can get some work done without it, but you're less likely to encounter errors like `Unable to find module 'boxen/puppet-boxen' on https://github.com`.

Then write some code. Run `script/cibuild` to test it. Check the `script`
directory for other useful tools.
",,"['Ruby', 'Puppet', 'Shell']","[200826, 3119, 416]"
kzoll/ztlogger,PHP website traffic logger script,"# README for ZT Logger #

----------

Version : 0.1.13
Date    : 2013/11/23

Copyright (C) 2011-2013 Kevin J. Zoll (kzoll@zolltech.com)
Copyright (C) 2011-2013 Zoll Technologies ()

This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with this program.  If not, see .

This product includes GeoLite data created by MaxMind, available from 

**Download:** [http://www.malwareteks.com/download.php?view.17](http://www.malwareteks.com/download.php?view.17 ""ZT Logger v0.1.10"")

**Bug Tracker:** [http://www.malwareteks.com/mteks_plugins/bug_tracker/bugs.php?0.view.3.0.0](http://www.malwareteks.com/mteks_plugins/bug_tracker/bugs.php?0.view.3.0.0 ""ZT Logger Bug Tracker"")

**Include Files:**
/ztlogger
Disclaimer.txt
GeoIP_License.txt
index.html
install.php
install2.php
License.txt
README.txt
ztlogger.php
/logs
.htaccess
index.html
/vault
.htaccess
geoip.inc
geoipcity.inc
geoipregionvars.php
GeoLiteCity.dat
index.html
ztlogger.ini

**Installation:**
Unpack ztlogger.zip and copy the ztlogger folder and its entire contents to your server. Ensure that all folders are chmod 0755 and all files are chmod 0644. Run install.php from your web browser.  Click the Install button.

The following files are created in the ZT Logger Vault directory during install:
counter.dat
directory.inc
hook.txt
ipwldb.csv

The IP Address from which the ZT Logger installation routine is ran will automatically be whitelisted.  All pages being accessed from that IP Address will not be logged by ZT Logger.

Check the ZT Logger Vault for hook.txt.  You will need to add the include statement at the beginning of the appropriate file of your CMS, Forum, or Blogging software, to start logging site activity.

bbPress 1.x - bb-load.php
e107 - class2.php
Drupal - index.php
Flatpress - defaults.php
IPB  - index.php (root and admin)
Joomla - index.php (root and current template)
phpBB - common.php
SMF - settings.php
vBulletin - global.php
Wordpress - wp-load.php

Make sure you delete install.php and install2.php from the ztlogger directory on your server. The installer is intentionally designed to not overwrite counter.dat, directory.inc, hook.txt, and ipwldb.csv if they are already present.  However, do not take any chances and make sure you delete install.php and install2.php.

**Upgrade:**
Copy ztlogger/ztlogger.php > ztlogger/ztlogger.php
Copy ztlogger/vault/GeoLiteCity.dat > ztlogger/vault/GeoLiteCity.dat
Copy ztlogger/vault/ztlogger.ini > ztlogger/vault/ztlogger.ini

**Whitelisting IPs:**
To add an IP address to the whitelist, you will need to manually edit ipwldb.csv.  This is a comma-separated values (comma-delimited) file.  Meaning all values in the csv database are separated by a comma.  So, don't forget to add a space at the beginning and a comma at the end of each and every IP address you enter. Each and every entry must have a space before and a comma after each and every IP address.  I can't stress that enough.

**Forthcoming:**
Whitelisting form to alleviate the need to manually edit ipwldb.csv.  Whitelisting will be password protected.  The Whitelist password will be stored in ztlogger.ini.

**What has Changed:**
0.1.13 (2013/12/23) - Added ability to prune ztlogger logs older than 30 days.                                   Updated GeoLiteCity database to latest edition available from maxmind.
0.1.12 (2013/11/05) - Updated to GeoIP PHP API v1.14
0.1.11 (2103/09/24) - Changed geoip.inc line #31 from define(""GEOIP_COUNTRY_EDITION"", 106); to define(""GEOIP_COUNTRY_EDITION"", 1);
0.1.10 (2012/08/03) - Improved error handling                                   Fixed Bug, introduced in last update, in determining $GEOIP_REGION_NAME
0.1.9 (2012/07/31) - Suppress PHP Notice: Undefined variable: proxyip in ztlogger.php on line 188                                 Suppress PHP Notice: Undefined index: in ztlogger.php on line 199                                 Suppress PHP Notice: Undefined index: proxyip in ztlogger.php on line 228
0.1.8 (2012/07/12) - Added Detection for TorExitNode
0.1.7 (2012/06/07) - Added Detection of Incapsula and nginx Reverse Proxies
0.1.6 (2012/03/16) - Better error handling                                 Refined ReservedIP function
0.1.5 (2012/03/07) - Fixed XSS vulnerability in ZT Logger <=0.1.4                                 Refinement in Geo Location format & output
0.1.4 (2012/02/18) - Further improvements in Client IP, Proxy IP and Real IP detection.                                 Minor fix in log formatting
0.1.3 (2012/02/13) - Improvements to Proxy detection
0.1.2 (2011/08/08) - GeoIP GeoLocation Data: now displays City, Region, Country.                                 Attempts to identify ProxyIP and Real IP (may need further work).
0.1.1 (2011/07/02) - Changes in variables names to eliminate conflicts with variable names in other packages.                                 Changes in log format.                                 Now includes version of ztlogger that is running.",,['PHP'],[124389]
couchbase/couchbase-net-client,The official Couchbase SDK for .NET Core and Full Frameworks,"The Official Couchbase .NET SDK ![](http://sdk.jenkins.couchbase.com/job/dotnet/job/couchbase-net-client-scripted-build-pipeline/badge/icon)
[![Join the chat at https://gitter.im/couchbase/discuss](https://badges.gitter.im/couchbase/discuss.svg)](https://gitter.im/couchbase/discuss?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

* master is 3.0 development branch
* release27 is 2.7.X development branch
* release13 is 1.3.X development branch

## Getting Started

To get up and running with the SDK, please visit the [online documentation](http://developer.couchbase.com/documentation/server/4.5/sdk/dotnet/start-using-sdk.html).

## Running Tests

We maintain a collection of both unit and integration test projects.

### Unit Tests

Couchbase.UnitTests contains environment independent tests and do not require a local cluster to run.

### Running the Integration Tests ##

Couchbase.IntegrationTests contains tests that are run against a real Couchbase Server and has different requirements depending on what server version you are running them against:

Couchbase Server 4.0+
1. The ""beer-sample"" and ""travel-sample"" sample buckets installed. They can be installed by logging into the Couchbase Console and then Settings->Sample Buckets.
2. Create the following buckets:
	1. ""default"" - a Couchbase bucket with no password
	2. ""authenticated"" - a Couchbase bucket with a password of ""secret""
	3. ""memcached"" - a Memcached bucket with no password
3. Install an SSL certificate (copied from the Couchbase console Security->Root Certificate)
4. A default primary index configured on the following buckets: `default`, `authenticated`, `beer-sample` and `travel-sample` (eg create primary index on `default`)
5. Add an FTS index to the `travel-sample` bucket called `idx-travel`
6. Update config.json with the hostname of your Couchbase Server IP and set enhancedAuth to `false`
7. Update app.config's hostname and *basic* Couchbase client section with the Couchbase Server IP

Couchbase Server 5.0+ - In addition to the steps above:
1. ""ephemeral"" - an Ephemeral bucket
2. Update config.json enhancedAuth to `true`
3. A user called `authenticated` with a password of `secret`

NOTE: Couchbase Server 5.0+ uses Role-Based Access Control (RBAC) for authentication. This supersedes configuring bucket passwords with discrete users with their own passwords and offers much more granular control.

## Pull Requests and Submissions ##
Being an Open Source project, the Couchbase SDK depends upon feedback and submissions from the community. If you feel as if you want to submit a bug fix or a feature, please post a Pull Request. The Pull Request will go through a formal code review process and merged after being +2'd by a Couchbase Engineer. In order to accept a submission, Couchbase requires that all contributors sign the Contributor License Agreement (CLA). You can do this by creating an account in [Gerrit](http://review.couchbase.org), our official Code Review system. After you have created your account, login and check the CLA checkbox.

Once the CLA is signed, a Couchbase engineer will push the pull request to Gerrit and one or more Couchbase engineers will review the submission. If it looks good they will then +2 the changeset and merge it with master. In addition, if the submission needs more work, you will need to amend the Changeset with another Patchset. Note that is strongly encouraged to submit a Unit Test with each submission and also include a description of the submission, what changed and what the result is.


",,"['C#', 'JavaScript']","[1581575, 42807]"
colszowka/phantomjs-gem,Phantomjs via Rubygems: Auto-install phantomjs on demand for current platform. Comes with poltergeist integration.,"# PhantomJS as a RubyGem

[![Build Status](https://travis-ci.org/colszowka/phantomjs-gem.png?branch=master)](https://travis-ci.org/colszowka/phantomjs-gem)

**DISCLAIMER: Alpha status, YMMV!**

I am lazy as hell, and wanted to be able to install [PhantomJS](http://phantomjs.org) via Rubygems/Bundler when using [poltergeist](https://github.com/jonleighton/poltergeist).

It keeps installations of phantomjs in `$HOME/.phantomjs/VERSION/PLATFORM`. When you call `Phantomjs.path`, it will return the path to the phantomjs executable in there. If that is not present, it will first fetch and
install the prebuilt packages suitable for the current plattform (currently Linux 32/64 or OS X supported).

If there is a phantomjs executable in your `$PATH` that matches the version number packaged in this gem, this one will be used instead of installing one in your `$HOME/.phantomjs`.

You will need `cURL` or `wget` on your system. For extraction, `bunzip2` and `tar` are required on Linux, and `unzip` on OS X. They should be installed already.

**TL;DR:** Instead of manually installing phantomjs on your machines, use this gem. It will take care of it.

## Example

```ruby
require 'phantomjs'
Phantomjs.path # => path to a phantom js executable suitable to your current platform. Will install before return when not installed yet.

# Or run phantomjs with the passed arguments:
Phantomjs.run('./path/to/script.js') # => returns stdout

# Also takes a block to receive each line of output:
Phantomjs.run('./path/to/script.js') { |line| puts line }
```

## Usage with Poltergeist/Capybara

Add this to your `Gemfile`:

```ruby
group :test do
    gem 'poltergeist'
    gem 'phantomjs', :require => 'phantomjs/poltergeist'
end
```

This will automatically require (and install) phantomjs and configure Capybara in the same way as noted below for manual setup.

Note that you need to add poltergeist as a dependency explicitly since it is not a dependency of this gem in order to avoid forcing users to install poltergeist if the just want to use phantomjs itself.

### Manual setup

Add `gem 'phantomjs', :group => :test` to your `Gemfile` and run `bundle`. In your test/spec helper, re-configure the Poltergeist capybara driver to use the phantomjs package from this gem:

```ruby
require 'phantomjs' # <-- Not required if your app does Bundler.require automatically (e.g. when using Rails)
Capybara.register_driver :poltergeist do |app|
    Capybara::Poltergeist::Driver.new(app, :phantomjs => Phantomjs.path)
end
```

Check out [the poltergeist docs](https://www.ruby-toolbox.com/gems/phantomjs) for all the options you can pass in there.

## A note about versions.

The gem version consists of 4 digits: The first 3 indicate the phantomjs release installed via this gem, the last one is the internal version of this gem, in case I screw things up and need to push another release in the interim.

## Contributing

**Warning**: The `spec_helper` calls `Phantomjs.implode` when it is loaded, which purges the `~/.phantomjs` directory. This is no bad thing, it just means every time you run the specs you'll download and install all three packages over, so tread with caution please. :)

1. Fork it
2. Create your feature branch (`git checkout -b my-new-feature`)
3. Commit your changes (`git commit -am 'Added some feature'`)
4. Push to the branch (`git push origin my-new-feature`)
5. Create new Pull Request

## Copyright

(c) 2013-2014 Christoph Olszowka

Note that this project merely simplifies the installation of the entirely separate PhantomJS project
via a Ruby gem. You can find the license information for PhantomJS at http://phantomjs.org/
",,"['Ruby', 'JavaScript']","[16071, 125]"
djblets/djblets,A collection of useful extensions for Django.,"Djblets
=======

Djblets is a large collection of general and special-purpose building blocks
designed to help with the development of web applications written using
Django_ and Python.

The following modules are available. These contain classes, functions,
template tags, templates, etc. that can be used by your own codebase.

* djblets.auth_ -
  Authentication-related utilities for registration, login rate limiting, and
  other auth-related uses

* djblets.avatars_ -
  Avatar rendering with flexible backends (supporting Gravatars, custom URLs,
  file uploads, or custom options)

* djblets.cache_ -
  Helpers for working with client-side and server-side caching needs

* djblets.conditions_ -
  User-defined condition rules under which actions should be performed

* djblets.configforms_ -
  Category-based, multi-form configuration pages

* djblets.datagrid_ -
  Customizable grids for displaying information, with custom columns

* djblets.db_ -
  Specialized fields, validation, and query operations for databases

* djblets.extensions_ -
  Extension framework, allowing third-party developers to extend your product
  or service

* djblets.features_ -
  Feature flags for enabling/disabling functionality based on any criteria

* djblets.feedview_ -
  Inline RSS feed reader for news posts and other data

* djblets.forms_ -
  Specialized fields and widgets, enhanced form rendering, and
  dictionary-backed form data

* djblets.gravatars_ -
  Low-level functions and template tags for injecting Gravatars_ into pages

* djblets.http_ -
  Utilities for working with HTTP requests and responses.

* djblets.integrations_ -
  Framework for integrating with third-party services and offering unlimited
  numbers of user-defined configurations

* djblets.log_ -
  Enhanced logging capabilities and log viewing

* djblets.mail_ -
  Enhanced Mail sending with DMARC checks and send-on-behalf-of-user
  functionality

* djblets.markdown_ -
  Markdown rendering for pages and e-mails, with WYSIWYG editing/rendering
  support

* djblets.pipeline_ -
  Pipeline_ compilers for ES6 JavaScript and optimized LessCSS support

* djblets.privacy_ -
  Privacy-by-design support, allowing consent to be requested and tracked
  and personal information redacted

* djblets.recaptcha_ -
  Mixins and form widgets for reCAPTCHA_ integration

* djblets.registries_ -
  Base support for defining in-code registries, which tracks and allows lookup
  of custom-registered objects

* djblets.siteconfig_ -
  In-database site configuration and settings, with Django settings mappings

* djblets.template_ -
  Loaders for intelligent template caching and utilities for working with
  template caches and state

* djblets.testing_ -
  Utilities for enhancing unit tests and defining smarter test runners

* djblets.urls_ -
  Flexible root-level URL handlers, dynamic URL patterns that can be changed
  at runtime, and more

* djblets.util_ -
  An assortment of misc. utility functions and template tags

* djblets.views_ -
  Class-based View mixins for controlling caching and more complex dispatch
  behavior

* djblets.webapi_ -
  Foundation for building fully-featured, consisent, maintainable REST APIs

We built and maintain Djblets as part of the `Review Board`_ code review
product and Splat_ bug tracker at Beanbag_.

See the documentation_ for guides and code references for working with
Djblets.


.. _Beanbag: https://www.beanbaginc.com/
.. _Django: https://www.djangoproject.com/
.. _GDPR: https://www.eugdpr.org/
.. _Gravatars: https://gravatars.com/
.. _Pipeline: http://django-pipeline.readthedocs.io/en/latest/
.. _reCAPTCHA: https://www.google.com/recaptcha/
.. _Review Board: https://www.reviewboard.org/
.. _Splat: https://www.hellosplat.com/
.. _documentation: https://www.reviewboard.org/docs/djblets/latest/

.. _djblets.auth:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-auth
.. _djblets.avatars:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-avatars
.. _djblets.cache:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-cache
.. _djblets.conditions:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-conditions
.. _djblets.configforms:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-configforms
.. _djblets.datagrid:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-datagrid
.. _djblets.db:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-db
.. _djblets.extensions:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-extensions
.. _djblets.features:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-features
.. _djblets.feedview:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-feedview
.. _djblets.forms:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-forms
.. _djblets.gravatars:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-gravatars
.. _djblets.http:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-http
.. _djblets.integrations:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-integrations
.. _djblets.log:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-log
.. _djblets.mail:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-mail
.. _djblets.markdown:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-markdown
.. _djblets.pipeline:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-pipeline
.. _djblets.privacy:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-privacy
.. _djblets.recaptcha:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-recaptcha
.. _djblets.registries:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-registries
.. _djblets.siteconfig:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-siteconfig
.. _djblets.template:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-template
.. _djblets.testing:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-testing
.. _djblets.urls:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-urls
.. _djblets.util:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-util
.. _djblets.views:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-views
.. _djblets.webapi:
   https://www.reviewboard.org/docs/djblets/latest/coderef/#coderef-djblets-webapi


Compatibility
=============

Djblets 0.9 (release-0.9.x_) supports Python 2.6 and 2.7, and Django 1.6.

Djblets 1.0 (release-1.0.x_) supports Python 2.7 and Django 1.6.

Djblets 2.0 (release-2.0.x_) supports Python 2.7, 3.5, and 3.6, and Django
1.6, 1.8, 1.0. 1,0. and 1.11.

See the `release notes`_ for information on the latest public releases.


.. _release-0.9.x: https://github.com/djblets/djblets/tree/release-0.9.x
.. _release-1.0.x: https://github.com/djblets/djblets/tree/release-1.0.x
.. _release-2.0.x: https://github.com/djblets/djblets/tree/release-2.0.x
.. _release notes: https://www.reviewboard.org/docs/releasenotes/djblets/


Installing Djblets
==================

We provide source builds, Wheels, and Eggs for Djblets. We recommend you use
Wheels unless you have a reason to use Eggs or source builds.

To install Wheels via pip::

    $ pip install Djblets

To install Eggs via easy_install::

    $ easy_install Djblets


Getting Support
===============

We can help you with Djblets-related development over on our `Review Board
development list`_.


.. _Review Board development list:
   https://http://groups.google.com/group/reviewboard-dev


Reporting Bugs
==============

Hit a bug? Let us know by
`filing a bug report `_.

You can also look through the
`existing bug reports `_ to see if anyone
else has already filed the bug.


Contributing
============

Are you a developer? Do you want to integrate Djblets into your project and
contribute back? Great! Let's help get you started.

First off, we have a few handy guides:

* `Review Board Contributor Guide`_ -
  This generally applies to Djblets as well.

We accept patches on `reviews.reviewboard.org
`_. (Please note that we *do not* accept pull
requests.)

.. _Review Board Contributor Guide:
   https://www.reviewboard.org/docs/codebase/dev://www.notion.so/reviewboard/Review-Board-45d228fb07a0459b84fee509ac054cec


Related Projects
================

* `Review Board`_ -
  Our dedicated open source code review product for teams of all sizes.
",,"['Python', 'JavaScript', 'CSS', 'HTML']","[2342798, 403483, 71473, 36104]"
hhru/tornado,"Tornado is a Python web framework and asynchronous networking library, originally developed at FriendFeed.","Tornado Web Server
==================

.. image:: https://badges.gitter.im/Join%20Chat.svg
   :alt: Join the chat at https://gitter.im/tornadoweb/tornado
   :target: https://gitter.im/tornadoweb/tornado?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

`Tornado `_ is a Python web framework and
asynchronous networking library, originally developed at `FriendFeed
`_.  By using non-blocking network I/O, Tornado
can scale to tens of thousands of open connections, making it ideal for
`long polling `_,
`WebSockets `_, and other
applications that require a long-lived connection to each user.

Hello, world
------------

Here is a simple ""Hello, world"" example web app for Tornado:

.. code-block:: python

    import tornado.ioloop
    import tornado.web

    class MainHandler(tornado.web.RequestHandler):
        def get(self):
            self.write(""Hello, world"")

    def make_app():
        return tornado.web.Application([
            (r""/"", MainHandler),
        ])

    if __name__ == ""__main__"":
        app = make_app()
        app.listen(8888)
        tornado.ioloop.IOLoop.current().start()

This example does not use any of Tornado's asynchronous features; for
that see this `simple chat room
`_.

Documentation
-------------

Documentation and links to additional resources are available at
http://www.tornadoweb.org
",,"['Python', 'Shell', 'C', 'Ruby', 'HTML']","[1625160, 4070, 1664, 1428, 25]"
proycon/pynlpl,"PyNLPl, pronounced as 'pineapple', is a Python library for Natural Language Processing. It contains various modules useful for common, and less common, NLP tasks. PyNLPl can be used for basic tasks such as the extraction of n-grams and frequency lists, and to build simple language model. There are also more complex data types and algorithms. Moreover, there are parsers for file formats common in NLP (e.g. FoLiA/Giza/Moses/ARPA/Timbl/CQL). There are also clients to interface with various NLP specific servers. PyNLPl most notably features a very extensive library for working with FoLiA XML (Format for Linguistic Annotation).","PyNLPl - Python Natural Language Processing Library
=====================================================

.. image:: https://travis-ci.org/proycon/pynlpl.svg?branch=master
    :target: https://travis-ci.org/proycon/pynlpl

.. image:: http://readthedocs.org/projects/pynlpl/badge/?version=latest
	:target: http://pynlpl.readthedocs.io/en/latest/?badge=latest
	:alt: Documentation Status

.. image:: http://applejack.science.ru.nl/lamabadge.php/pynlpl
   :target: http://applejack.science.ru.nl/languagemachines/

.. image:: https://zenodo.org/badge/759484.svg
   :target: https://zenodo.org/badge/latestdoi/759484

PyNLPl, pronounced as 'pineapple', is a Python library for Natural Language
Processing. It contains various modules useful for common, and less common, NLP
tasks. PyNLPl can be used for basic tasks such as the extraction of n-grams and
frequency lists, and to build simple language model. There are also more
complex data types and algorithms. Moreover, there are parsers for file formats
common in NLP (e.g. FoLiA/Giza/Moses/ARPA/Timbl/CQL). There are also clients to
interface with various NLP specific servers. PyNLPl most notably features a
very extensive library for working with FoLiA XML (Format for Linguistic
Annotatation).

The library is a divided into several packages and modules. It works on Python
2.7, as well as Python 3.

The following modules are available:

- ``pynlpl.datatypes`` - Extra datatypes (priority queues, patterns, tries)
- ``pynlpl.evaluation`` - Evaluation & experiment classes (parameter search, wrapped
  progressive sampling, class evaluation (precision/recall/f-score/auc), sampler, confusion matrix, multithreaded experiment pool)
- ``pynlpl.formats.cgn`` - Module for parsing CGN (Corpus Gesproken Nederlands) part-of-speech tags
- ``pynlpl.formats.folia`` - Extensive library for reading and manipulating the
  documents in `FoLiA `_ format (Format for Linguistic Annotation).
- ``pynlpl.formats.fql`` - Extensive library for the FoLiA Query Language (FQL),
  built on top of ``pynlpl.formats.folia``. FQL is currently documented `here
  `__.
- ``pynlpl.formats.cql`` - Parser for the Corpus Query Language (CQL), as also used by
  Corpus Workbench and Sketch Engine. Contains a convertor to FQL.
- ``pynlpl.formats.giza`` - Module for reading GIZA++ word alignment data
- ``pynlpl.formats.moses`` - Module for reading Moses phrase-translation tables.
- ``pynlpl.formats.sonar`` - Largely obsolete module for pre-releases of the
  SoNaR corpus, use ``pynlpl.formats.folia`` instead.
- ``pynlpl.formats.timbl`` - Module for reading Timbl output (consider using
  `python-timbl `_ instead though)
- ``pynlpl.lm.lm`` - Module for simple language model and reader for ARPA
  language model data as well (used by SRILM).
- ``pynlpl.search`` - Various search algorithms (Breadth-first, depth-first,
  beam-search, hill climbing, A star, various variants of each)
- ``pynlpl.statistics`` - Frequency lists, Levenshtein, common statistics and
  information theory functions
- ``pynlpl.textprocessors`` - Simple tokeniser, n-gram extraction

Installation
--------------------

Download and install the latest stable version directly from the Python Package
Index with ``pip install pynlpl`` (or ``pip3`` for Python 3 on most
systems). For global installations prepend ``sudo``.

Alternatively, clone this repository and run ``python setup.py install`` (or
``python3 setup.py install`` for Python 3 on most system. Prepend ``sudo`` for
global installations.

This software may also be found in the certain Linux distributions, such as
the latest versions as Debian/Ubuntu, as ``python-pynlpl`` and ``python3-pynlpl``.
PyNLPL is also included in our `LaMachine `_ distribution.

Documentation
--------------------

API Documentation can be found `here `__.


",,"['Python', 'Shell', 'C++']","[1358116, 2333, 1452]"
sonatype/plexus-compiler,,"The cannonical git repository is located at https://github.com/codehaus-plexus/plexus-compiler


",,['Java'],[348738]
tgjones/ormongo,"Ormongo is a very lightweight implementation of the ActiveRecord pattern, built on top of the 10gen Mongo C# driver.",,,['C#'],[139570]
herumi/cybozulib,a tiny library for C++,"[![Build Status](https://travis-ci.org/herumi/cybozulib.png)](https://travis-ci.org/herumi/cybozulib)

# cybozulib

# Abstract
This is a tiny C++ library for Windows and Linux.

# How to use

directory position

```
/cybozulib
          /cybozulib_ext ; necessary for Windows if openssl, gmp are used
```

# License
[BSD 3-Clause License](http://opensource.org/licenses/BSD-3-Clause)

# Author

MITSUNARI Shigeo(herumi@nifty.com)

# Disclaimer
This OSS is my own personal work and does not have any relationship with Cybozu Labs, Inc.,
Cybozu Inc. or any other organization which I belong to.

# sais.hxx
sais.hxx is written by Yuta Mori.
",,"['C++', 'Python', 'Makefile', 'Batchfile', 'CMake']","[711221, 9737, 4446, 745, 150]"
mkraft/fides,Add referential integrity to polymorphic Rails models.,"# Fides

Enforces Rails polymorphic associations at the database level.

### Longer Description

Use this gem in Rails migrations to create SQL Triggers to enforce the data 
integrity of polymorphic associations at the database level.

Triggers are invoked by the database before inserts, updates, and deletes to 
prevent polymorphic associations from losing data integrity.

If an insert/update is attempted on a polymorphic table with a record that 
refers to a non-existent record in another table, a SQL error is raised. If a 
delete is attempted from a table that is referred to by a record in the 
polymorphic table, a SQL error is raised.

## Installation

Add this line to your application's Gemfile:

    gem 'fides'

And then execute:

    $ bundle

Or install it yourself as:

    $ gem install fides

## Usage

Fides has the following public methods:

#### `add_polymorphic_triggers`

Parameters:
    
- `:polymorphi_model` (required)
- `:associated_models` (required)

#### `remove_polymorphic_triggers`

Parameters:

- `:polymorphic_model` (required)

Following the [Rails Polymorphic Associations example](http://guides.rubyonrails.org/association_basics.html#polymorphic-associations),
you would do the following in a migration:

    class AddReferentialIntegrityToImageable < ActiveRecord::Migration

      def up
        add_polymorphic_triggers(polymorphic_model: ""Picture"", associated_models: [""Employee"", ""Product""])
      end

      def down
        remove_polymorphic_triggers(polymorphic_model: ""Picture"")
      end
    
    end

If you're using Rails < version 3.1, then use Fides in your migration like this:

    class AddReferentialIntegrityToImageable < ActiveRecord::Migration

      extend Fides

      def self.up
        add_polymorphic_triggers(polymorphic_model: ""Picture"", associated_models: [""Employee"", ""Product""])
      end

      def self.down
        remove_polymorphic_triggers(polymorphic_model: ""Picture"")
      end
    
    end

## Database Adapters

Fides currently functions with `postgresql` and `sqlite3` adapters. Feel free 
to contribute other adapters as desired (ex. `mysql2`), and ensure that the 
common database integration tests all pass prior to submitting the pull request.

## Tests

    rake test:unit
    rake test:integration:sqlite3
    rake test:integration:postgresql

To run the postgresql integration tests you must first copy 
test/config/database.yml.example to test/config/database.yml and customize the 
values for your local postgres install.
    
",,['Ruby'],[30831]
cerb-plugins/wgm.facebook,Cerb integration with Facebook's OAuth API,"Cerb Plugins - wgm.facebook
===========================================
Copyright (C) 2017 Webgroup Media, LLC.  
[http://cerb.ai/](http://cerb.ai/)  

What's this?
------------
This plugin provides integration with [Facebook](http://www.facebook.com/) via their REST API. This plugin is intended to be a bridge used by other plugins to communicate with the Facebook Graph API. In addition, it adds a new bot action with the ability to post status updates to Facebook.

Installation
------------
* Make a new [Facebook Application](https://developers.facebook.com/apps)
* Install the plugin from **Setup->Plugins->Library**.
* Click **Setup->Services->Facebook** and configure your Facebook app credentials.
* Authorize the app for the users you wish to be available to the plugin

Credits
-------
This plugin was developed by [Webgroup Media, LLC](https://cerb.ai/).

License
-------

[http://opensource.org/licenses/gpl-2.0.php](http://opensource.org/licenses/gpl-2.0.php)  

This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
",,"['PHP', 'Smarty']","[17491, 5999]"
alanbem/Josser,JSON-RPC client for PHP,"Josser - JSON-RPC client for PHP 5.3+
=====================================

[![Build Status](https://secure.travis-ci.org/alanbem/Josser.png?branch=master)](http://travis-ci.org/alanbem/Josser)

JSON-RPC? What is it?
---------------------

JSON-RPC is a stateless, light-weight remote procedure call protocol encoded in JSON. It is a very simple
protocol, defining only a handful of data types and commands.

Which specification of JSON-RPC does Josser support?
----------------------------------------------------

Josser supports [JSON-RPC 1.0] (http://json-rpc.org/wiki/specification) and
[revised JSON-RPC 2.0] (http://www.jsonrpc.org/specification). Unfortunately only client-server
connections are possible with Josser - albeit JSON-RPC 1.0 was designed as P2P - due to PHP limitations.

It is worth to mention that Josser's architecture allows to plug your own JSON-RPC flavours or implement existing
semi-standardized JSON-RPC protocols e.g. abandoned JSON-RPC 1.1WD.

Transport mechanism
-------------------

As specification (both 1.0 and 2.0) states, JSON-RPC is transport agnostic. Josser sticks to that and allows to use
http, sockets, tcp/ip or anything else your project requires (like post-it notes on a fridge :D).
Currently only http transport is implemented.

Documentation
=============

Usage
-----

Invoking remote methods is fairly simple:

```php
 'http://user:password@your-service.com/math:8888']); // http client
$transport = new HttpTransport($guzzle); // RPC over http
$protocol  = new JsonRpc1; // lets use JSON-RPC 1.0

$client = new Client($transport, $protocol);

// send a request
$sum = $client->request('sum', array(5, 4));

var_dump($sum); // int(9)
```

If remote method does not return anything, notifications are what you need:

```php
notify('logout');
```

Error handling
--------------

Josser informs about errors throughâ€‰ set of exceptions.
 
```php
request('method', array(1, ""param2""));
} catch (RpcFaultException $e) {
    echo 'Ups! Remote server sent an error.';
} catch (TransportFailureException $e) {
    echo 'Josser did not send remote call.';
} catch (RequestResponseMismatchException $e) {
    echo ""Response id does not match request id."";
} catch (InvalidResponseException $e) {
    echo ""Response object is invalid due to protocol constraints."";
} catch(InvalidRequestException $e) {
    echo ""Request is invalid due to protocol constraints."";
}              
```

For convenience catch-all exception exists.
  
```php
request('method', array(1, ""param2""));
} catch (JosserException $e) {
    echo 'Josser error occurred.';
}           
```

Request objects
---------------

When you call remote procedures, those calls are internally translated into generic request objects.

```php
request('math.divide', array(1, 2));
$client->notify('system.logout');

// code above is equivalent of

$request = new Request('math.divide', array(1, 2), 213123); // 213123 is a request identifier
$client->call($request);

$notification = new Notification('system.logout');
$client->call($notification);
```

As you can see, you can work with those low-level objects directly but downside of this approach is that you must provide your
own id for every request. By default Client::request() generates this id for you on its own.
Also remember that Client::call() does not return result directly - it return response object instead. To get to
underlying result data use Response::getResult() like this:

```php
call($request);

var_dump($response->getResult()); // int(4)
```

Notice the possibility of creating your own, project specific, request objects.

```php
= 5.3

Submitting bugs and feature requests
------------------------------------

Bugs and feature request are tracked on [Github](https://github.com/alanbem/josser/issues)

Author
------

Alan Gabriel Bem - 

License
-------

Josser is licensed under the MIT License - see the LICENSE file for details
",,['PHP'],[79295]
ramusus/kinopoiskpy,Python API to kinopoisk.ru,"# Kinopoiskpy

[![PyPI version](https://img.shields.io/pypi/v/kinopoiskpy.svg)](https://pypi.python.org/pypi/kinopoiskpy) [![Circle CI](https://circleci.com/gh/ramusus/kinopoiskpy/tree/master.svg?style=shield)](https://circleci.com/gh/ramusus/kinopoiskpy) [![Build Status](https://img.shields.io/travis/ramusus/kinopoiskpy.svg?branch=master)](https://travis-ci.org/ramusus/kinopoiskpy) [![Coverage Status](https://coveralls.io/repos/ramusus/kinopoiskpy/badge.svg?branch=master)](https://coveralls.io/r/ramusus/kinopoiskpy)

This package is pythonic API to kinopoisk.ru website.

## Installation

To install the latest version using pip:

    $ pip install kinopoiskpy

## Example usage

Search movies:

    >>> from kinopoisk.movie import Movie
    >>> movie_list = Movie.objects.search('Redacted')
    >>> len(movie_list)
    1
    >>> print movie_list[0].title
    Без цензуры
    >>> print movie_list[0].id
    278229

Get content of movie by ID:

    >>> from kinopoisk.movie import Movie
    >>> movie = Movie(id=278229)
    >>> movie.get_content('main_page')
    >>> movie.year
    2007
    >>> movie.title
    u'Без цензуры'
    >>> movie.title_en
    u'Redacted'
    >>> movie.plot
    u'В центре картины  -  небольшой отряд американских солдат на контрольно-пропускном пункте в Ираке. Причём восприятие их истории постоянно меняется. Мы видим события глазами самих солдат, представителей СМИ, иракцев и понимаем, как на каждого из них влияет происходящее, их встречи и столкновения друг с другом.'
    >>> movie.runtime
    90
    >>> movie.tagline
    u'""Фильм, запрещенный к прокату во многих странах""'
    >>> movie.rating
    8.5
    >>> movie.get_content('posters')
    >>> len(movie.posters) > 0
    True

Get content of person by ID:

    >>> from kinopoisk.person import Person
    >>> person = Person(id=6245)
    >>> person.get_content('main_page')
    >>> person.id
    6245
    >>> person.name
    u'Джонни Депп'
    >>> person.year_birth
    1963
    >>> person.name_en
    u'Johnny Depp'
    >>> len(person.information) > 50
    True
    >>> person.get_content('photos')
    >>> len(person.photos) > 10
    True

## Contributors

[Alex Rembish](http://github.com/rembish)
",,['Python'],[63278]
FrankHB/yslib,"The YSLib Project (main hg-git mirror) (NOTE: The README document in the repository is missing by design. Please go to the main repository for documentation, issues and pull requests.)",,,"['C++', 'C', 'Makefile', 'Shell', 'Objective-C']","[3467580, 819459, 59986, 30725, 534]"
