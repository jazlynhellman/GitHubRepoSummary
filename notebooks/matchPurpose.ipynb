{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import en_core_web_sm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from nltk import stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newDescriptions(descriptions):\n",
    "    newDescr=[]\n",
    "    for d in descriptions: \n",
    "        try:\n",
    "            new_d = stripPunct(d)\n",
    "            new_d = makeLower(new_d)\n",
    "            if isVerb(d):\n",
    "                x = \"This repository \" + str(new_d)\n",
    "            else:\n",
    "                x = \"This repository contains \" + str(new_d)\n",
    "        except:\n",
    "            x = \"\"\n",
    "        newDescr.append(x)\n",
    "    return newDescr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isVerb(description):\n",
    "    doc = nlp(description)\n",
    "    \n",
    "    if doc[0].pos_ == 'VERB':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTree(text):\n",
    "\n",
    "    df_tree = pd.DataFrame(columns=['Text','Dep','Head Text','Head Pos','Children', 'Stop','Lemma/Stem'])\n",
    "    stemmer=stem.PorterStemmer()\n",
    "    if type(text) is str:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "\n",
    "            df_tree = df_tree.append({'Text':token.text, 'Dep':token.dep_, 'Head Text':token.head.text,\n",
    "                           'Head Pos':token.head.pos_, 'Children':[child for child in token.children], 'Stop':token.is_stop, 'Lemma/Stem':stemmer.stem(token.lemma_)}, ignore_index=True)\n",
    "\n",
    "    return df_tree\n",
    "\n",
    "\n",
    "def getAllTrees(texts):\n",
    "    trees = []\n",
    "    for text in texts:\n",
    "        trees.append(getTree(text))\n",
    "        \n",
    "    return trees\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define punctuation\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "def stripPunct(my_str):\n",
    "\n",
    "\n",
    "    # To take input from the user\n",
    "    # my_str = input(\"Enter a string: \")\n",
    "\n",
    "    # remove punctuation from the string\n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "        else:\n",
    "            no_punct = no_punct + \" \"\n",
    "    return no_punct\n",
    "\n",
    "def stripAllPunct(strings):\n",
    "    to_return=[]\n",
    "    for s in strings:\n",
    "        try:\n",
    "            to_return.append(stripPunct(s))\n",
    "        except:\n",
    "            to_return.append(stripPunct(s))\n",
    "    return to_return\n",
    "\n",
    "def makeLower(my_str):\n",
    "    return my_str.lower()\n",
    "\n",
    "def makeAllLower(strings):\n",
    "    to_return=[]\n",
    "    for s in strings:\n",
    "        try:\n",
    "            to_return.append(makeLower(s))\n",
    "        except:\n",
    "            to_return.append(\"\")\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incrementOccurance(index, occurances):\n",
    "    occurances[index]+=1\n",
    "    \n",
    "def singleTreeOccurance(lst, list_of_universal_dependencies, occurances):\n",
    "    for d in lst:\n",
    "        try:\n",
    "            index=list_of_universal_dependencies.index(str(d))\n",
    "        except:\n",
    "            list_of_universal_dependencies.append(str(d))\n",
    "            index = list_of_universal_dependencies.index(str(d))\n",
    "            \n",
    "       \n",
    "        incrementOccurance(index, occurances)\n",
    "        \n",
    "        \n",
    "def allTreeOccurances(lists, list_of_universal_dependencies, occurances):\n",
    "    for l in lists:\n",
    "        singleTreeOccurance(l, list_of_universal_dependencies, occurances)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for validation\n",
    "\n",
    "# occurances_v=np.zeros((36,1))\n",
    "# list_of_universal_dependencies_v=[]\n",
    "# example_dependencies_v=['None']*36\n",
    "# tree_occurance_v = allTreeOccurances(trees_v, list_of_universal_dependencies_v, occurances_v)\n",
    "\n",
    "# # for utility\n",
    "\n",
    "# occurances_u=np.zeros((37,1))\n",
    "# list_of_universal_dependencies_u=[]\n",
    "# example_dependencies_u=['None']*37\n",
    "# tree_occurance_u = allTreeOccurances(trees_u, list_of_universal_dependencies_u, occurances_u)\n",
    "\n",
    "# # for organization\n",
    "\n",
    "# occurances_o=np.zeros((36,1))\n",
    "# list_of_universal_dependencies_o=[]\n",
    "# example_dependencies_o=['None']*36\n",
    "# tree_occurance_o = allTreeOccurances(trees_o, list_of_universal_dependencies_o, occurances_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it_v = 0\n",
    "# for i in occurances_v:\n",
    "    \n",
    "#     #print(i, list_of_universal_dependencies[it], spacy.explain(list_of_universal_dependencies[it]))\n",
    "#     it_v+=1\n",
    "    \n",
    "# o_v = []    \n",
    "# for i in occurances_v:\n",
    "#     o_v.append(int(i))\n",
    "    \n",
    "# print(len(list_of_universal_dependencies_v))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #######################\n",
    "# it_u = 0\n",
    "# for i in occurances_u:\n",
    "    \n",
    "#     #print(i, list_of_universal_dependencies[it], spacy.explain(list_of_universal_dependencies[it]))\n",
    "#     it_u+=1\n",
    "    \n",
    "# o_u = []    \n",
    "# for i in occurances_u:\n",
    "#     o_u.append(int(i))\n",
    "    \n",
    "# print(len(list_of_universal_dependencies_u))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #######################\n",
    "# it_o = 0\n",
    "# for i in occurances_o:\n",
    "    \n",
    "#     #print(i, list_of_universal_dependencies[it], spacy.explain(list_of_universal_dependencies[it]))\n",
    "#     it_o+=1\n",
    "    \n",
    "# o_o = []    \n",
    "# for i in occurances_o:\n",
    "#     o_o.append(int(i))\n",
    "    \n",
    "# print(len(list_of_universal_dependencies_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take first 80 descriptions\n",
    "# remove stop words\n",
    "# stem words (or lematize ... figure out which is better)\n",
    "# import the purpose from the correct sheet\n",
    "# stem or lematize\n",
    "# match individual words in description to purpose and save dep, head text, pos for properly matched words\n",
    "\n",
    "# next look at intro paragraph of readme to do the same.\n",
    "\n",
    "# alternatively, for the properly id'd words ---> get first sentence from wikipedia for background (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmaAndStem(tree):\n",
    "    stemmer=stem.PorterStemmer()\n",
    "    string = \"\"\n",
    "    count=0\n",
    "    lemma = tree['Lemma/Stem']\n",
    "    for l in lemma:\n",
    "        if count==0:\n",
    "            string = string+stemmer.stem(l)\n",
    "            count+=1\n",
    "        else:\n",
    "            string = string + \" \" +  stemmer.stem(l)\n",
    "    return string\n",
    "    \n",
    "\n",
    "def allLemmaStem(trees):\n",
    "    lems=[]\n",
    "    for t in trees:\n",
    "        try:\n",
    "            l=(lemmaAndStem(t))\n",
    "        except:\n",
    "            l=\"\"\n",
    "        lems.append(l)\n",
    "    return lems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all matching sequences using difflib library\n",
    "\n",
    "def matchTwoStrings(string1, string2):\n",
    "    to_return=[]\n",
    "    matches = difflib.SequenceMatcher(lambda x: x == \" \",string1,string2).get_matching_blocks()\n",
    "    for match in matches:\n",
    "        x=(string1[match.a:match.a+match.size])\n",
    "        if len(x)<2:\n",
    "            continue\n",
    "        else:\n",
    "            to_return.append(x)\n",
    "    return to_return\n",
    "\n",
    "\n",
    "def findAllMatches(list1,list2):\n",
    "    to_return=[]\n",
    "    for i in range(len(list1)):\n",
    "        matches=matchTwoStrings(list1[i],list2[i])\n",
    "        to_return.append(matches)\n",
    "        \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matching sequences, get dependency structure\n",
    "\n",
    "def matchTwoStrings(string1, string2):\n",
    "    to_return=[]\n",
    "    matches = difflib.SequenceMatcher(lambda x: x == \" \",string1,string2).get_matching_blocks()\n",
    "    for match in matches:\n",
    "        x=(string1[match.a:match.a+match.size])\n",
    "        if len(x)<3:\n",
    "            continue\n",
    "        else:\n",
    "            to_return.append(x)\n",
    "    return to_return\n",
    "\n",
    "\n",
    "def findAllMatches(list1,list2):\n",
    "    to_return=[]\n",
    "    for i in range(len(list1)):\n",
    "        matches=matchTwoStrings(list1[i],list2[i])\n",
    "        to_return.append(matches)\n",
    "        \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def findIndexInSeries(series,value):\n",
    "    index=0\n",
    "    for i in range(len(series)):\n",
    "        if str(value)!=series[i]:\n",
    "            index+=1\n",
    "        else:\n",
    "            return index\n",
    "\n",
    "\n",
    "def findCorrelatingDep(dataframe,index, token):\n",
    "    #print(index)\n",
    "    try:\n",
    "        df_temp=dataframe.iloc[[index]]\n",
    "        dep = df_temp[\"Dep\"]\n",
    "        for i in dep:\n",
    "            return(i)\n",
    "    except:\n",
    "        return(\"*\"+token+\"*\")\n",
    "\n",
    "def setSeriesPurposeTrue(series, idx):\n",
    "#     tree.at[idx,'Purpose']=1\n",
    "#\n",
    "\n",
    "#      To finish. \n",
    "#      goal: create new series of 0's and 1's representing which index of initial dataframe is purpose and which isnt\n",
    "\n",
    "\n",
    "#\n",
    "    pass\n",
    "    \n",
    "        \n",
    "    \n",
    "def matchesPerTree(matches, tree):\n",
    "    dependencies=[]\n",
    "    for match in matches:\n",
    "        doc=nlp(match)\n",
    "        for token in doc:\n",
    "            if str(token) == \" \":\n",
    "                continue\n",
    "            else:\n",
    "                idx = findIndexInSeries(tree[\"Lemma/Stem\"], str(token))\n",
    "                dependency = findCorrelatingDep(tree,idx, str(token))\n",
    "                #setPurposeTrue(tree, idx)\n",
    "                #print(tree)\n",
    "                dependencies.append(dependency)\n",
    "    return dependencies\n",
    "\n",
    "def allTreeMatches(listOfMatches,listOfTrees):\n",
    "    listOfDependencies=[]\n",
    "    for i in range(len(listOfMatches)):\n",
    "        deps = matchesPerTree(listOfMatches[i],listOfTrees[i])\n",
    "        listOfDependencies.append(deps)\n",
    "    return listOfDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file to update\n",
    "file_v = \"validation dataset/validation_info_init.csv\"\n",
    "file_o = \"organization dataset/organization_info.csv\"\n",
    "file_u = \"utility dataset/utility_info.csv\"\n",
    "sheet_v = pd.read_csv(file_v, encoding = \"ISO-8859-1\")\n",
    "sheet_o = pd.read_csv(file_o)\n",
    "sheet_u = pd.read_csv(file_u)\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "descriptions_v = sheet_v['Description']\n",
    "descriptions_o = sheet_o['Description']\n",
    "descriptions_u = sheet_u['Description']\n",
    "\n",
    "\n",
    "#clean purpose (lower case and remove punctuation)\n",
    "purpose_v = sheet_v['Purpose']\n",
    "purpose_v = makeAllLower(purpose_v)\n",
    "purpose_v = stripAllPunct(purpose_v)\n",
    "\n",
    "\n",
    "#clean description (lower case and remove punctuation) and make a sentence\n",
    "newD_v = newDescriptions(descriptions_v)\n",
    "newD_o = newDescriptions(descriptions_o)\n",
    "newD_u = newDescriptions(descriptions_u)\n",
    "\n",
    "# make into dataframe\n",
    "df_newDescr_v = pd.DataFrame(newD_v)\n",
    "df_newDescr_o = pd.DataFrame(newD_o)\n",
    "df_newDescr_u = pd.DataFrame(newD_u)\n",
    "\n",
    "df_purpose_v=pd.DataFrame(purpose_v)\n",
    "\n",
    "\n",
    "# get the tree structure for each sentence/purpose\n",
    "trees_v = getAllTrees(newD_v)\n",
    "trees_o = getAllTrees(newD_o)\n",
    "trees_u = getAllTrees(newD_u)\n",
    "trees_purpose = getAllTrees(purpose_v)\n",
    "\n",
    "\n",
    "# take first 80 from validation of both sentence and purpose\n",
    "trees_match = trees_v[0:81]\n",
    "trees_match_purpose = trees_purpose[0:81]\n",
    "\n",
    "descr_match=newD_v[0:81]\n",
    "\n",
    "lems_descr = allLemmaStem(trees_match)\n",
    "\n",
    "lems_purpose = allLemmaStem(trees_match_purpose)\n",
    "\n",
    "\n",
    "\n",
    "# find all string matches between initial description and manually labeled purposes\n",
    "\n",
    "matches=findAllMatches(lems_descr,lems_purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each description's list of matched strings, find the corresponding dependency type\n",
    "# trees_match\n",
    "dependenciesMatched=allTreeMatches(matches, trees_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['compound', 'dobj', 'prep', 'compound', 'pobj', 'compound'],\n",
       " ['amod', 'pobj'],\n",
       " ['nsubj', 'ROOT', 'conj'],\n",
       " [],\n",
       " ['xcomp', 'dobj'],\n",
       " ['*cont*'],\n",
       " ['compound', 'dobj'],\n",
       " ['compound', 'nsubj', 'amod', 'pobj'],\n",
       " ['amod', 'amod', 'ROOT'],\n",
       " [],\n",
       " ['relcl', 'compound', '*protoc*'],\n",
       " ['prep', 'compound', 'pobj'],\n",
       " ['compound', 'compound', 'dobj'],\n",
       " ['amod', 'pobj', 'prep', '*quickba*'],\n",
       " [],\n",
       " [],\n",
       " ['nsubj'],\n",
       " ['compound', '*pro*'],\n",
       " [],\n",
       " ['compound'],\n",
       " ['compound'],\n",
       " ['*re*', 'amod'],\n",
       " ['dobj', 'cc', 'compound', 'compound'],\n",
       " ['*co*'],\n",
       " [],\n",
       " ['compound', 'ROOT'],\n",
       " ['xcomp', 'pobj'],\n",
       " ['dobj', 'compound', 'pobj'],\n",
       " ['xcomp', 'dobj'],\n",
       " ['acl', 'compound', 'dobj'],\n",
       " [],\n",
       " [],\n",
       " ['compound', 'prep', 'pobj'],\n",
       " ['amod', 'compound', 'prep', 'pobj'],\n",
       " [],\n",
       " ['dobj', 'prep', 'pobj', 'compound', 'dobj'],\n",
       " ['attr', 'prep', 'amod', 'compound'],\n",
       " ['*l*', 'prep', 'amod', 'pobj'],\n",
       " ['ROOT', 'compound', 'dobj', 'prep', 'compound', 'pobj'],\n",
       " [],\n",
       " ['compound'],\n",
       " ['compound', 'compound', 'compound', 'dobj'],\n",
       " [],\n",
       " ['*impl*',\n",
       "  '*ment*',\n",
       "  'prep',\n",
       "  'pobj',\n",
       "  'aux',\n",
       "  'xcomp',\n",
       "  'det',\n",
       "  'amod',\n",
       "  'compound',\n",
       "  'dobj'],\n",
       " ['advmod', 'ROOT'],\n",
       " ['compound', 'compound', 'pobj'],\n",
       " ['compound', 'dobj', '*gener*'],\n",
       " ['amod', 'dobj'],\n",
       " [],\n",
       " ['*imag*'],\n",
       " [],\n",
       " ['*co*'],\n",
       " [],\n",
       " ['amod', 'amod', 'dobj'],\n",
       " ['amod', 'prep', 'compound', 'pobj'],\n",
       " ['*repo*', 'prep', 'amod', 'pobj'],\n",
       " ['compound', 'dobj'],\n",
       " ['xcomp', 'dobj', 'prep', 'pobj'],\n",
       " ['ccomp', 'amod', 'dobj', '*f*'],\n",
       " ['det', 'compound', '*s*', 'pobj'],\n",
       " ['amod', 'npadvmod', '*par*'],\n",
       " ['pobj', 'nummod', 'compound', '*charact*'],\n",
       " ['*video*'],\n",
       " ['xcomp'],\n",
       " ['xcomp'],\n",
       " ['det', 'amod', 'amod', 'compound', 'pobj'],\n",
       " ['compound'],\n",
       " ['prep', 'det', 'amod', 'amod', 'pobj'],\n",
       " ['*con*', 'dobj'],\n",
       " ['compound', 'dobj', 'prep', '*caf*', 'pobj'],\n",
       " ['nmod', 'cc', '*traver*', 'dobj'],\n",
       " ['pcomp', 'compound'],\n",
       " ['amod', 'amod', 'pcomp', 'amod', 'dobj'],\n",
       " ['compound', 'prep', 'amod', 'pobj'],\n",
       " ['dobj', 'prep'],\n",
       " ['*cont*', '*fi*', 'dobj'],\n",
       " ['compound', 'attr'],\n",
       " ['dobj', 'ccomp', 'compound', 'dobj'],\n",
       " ['compound', 'pobj'],\n",
       " ['dobj'],\n",
       " []]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(len(dependenciesMatched)):\n",
    "#     print(i, \" : \", dependenciesMatched[i], \"\\n\\t\", matches[i],\"\\n\\t\",trees_match[i], trees_match_purpose[i])\n",
    "\n",
    "# # #dataframe_matched = pd.DataFrame(columns=['Number'])\n",
    "\n",
    "dependenciesMatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lems_purpose' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-02fb6c9083f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlems_purpose\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m49\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlems_descr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m49\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m49\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lems_purpose' is not defined"
     ]
    }
   ],
   "source": [
    "print(lems_purpose[49])\n",
    "print(lems_descr[49])\n",
    "print(matches[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurances_purpose=np.zeros((42,1))\n",
    "list_of_universal_dependencies_purpose=[]\n",
    "example_dependencies_purpose=['None']*42\n",
    "tree_occurance_purpose = allTreeOccurances(dependenciesMatched, list_of_universal_dependencies_purpose, occurances_purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  [43.] compound  =  compound\n",
      "1  :  [28.] dobj  =  direct object\n",
      "2  :  [17.] prep  =  prepositional modifier\n",
      "3  :  [24.] pobj  =  object of preposition\n",
      "4  :  [25.] amod  =  adjectival modifier\n",
      "5  :  [3.] nsubj  =  nominal subject\n",
      "7  :  [1.] conj  =  conjunct\n",
      "8  :  [7.] xcomp  =  open clausal complement\n",
      "10  :  [1.] relcl  =  relative clause modifier\n",
      "15  :  [2.] cc  =  coordinating conjunction\n",
      "17  :  [1.] acl  =  clausal modifier of noun (adjectival clause)\n",
      "18  :  [2.] attr  =  attribute\n",
      "22  :  [1.] aux  =  auxiliary\n",
      "23  :  [4.] det  =  determiner\n",
      "24  :  [1.] advmod  =  adverbial modifier\n",
      "28  :  [2.] ccomp  =  clausal complement\n",
      "31  :  [1.] npadvmod  =  noun phrase as adverbial modifier\n",
      "33  :  [1.] nummod  =  numeric modifier\n",
      "38  :  [1.] nmod  =  modifier of nominal\n",
      "40  :  [2.] pcomp  =  complement of preposition\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(occurances_purpose)):\n",
    "    if(spacy.explain(list_of_universal_dependencies_purpose[i])is not None):\n",
    "        print(i, \" : \" , occurances_purpose[i], (list_of_universal_dependencies_purpose[i]), \" = \" ,spacy.explain(list_of_universal_dependencies_purpose[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
